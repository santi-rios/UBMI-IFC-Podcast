---
title: "UBMI-IFC Podcast Generator"
subtitle: "An Automated Biomedical Research Podcast Generation System"
author: "UBMI-IFC Team"
date: today
format:
  html:
    theme: cosmo
    toc: true
    toc-depth: 3
    code-fold: show
    code-tools: true
    embed-resources: true
execute:
  warning: false
  error: false
---

# üéß UBMI-IFC Podcast Generator

This document is an interactive demonstration of an intelligent system that automatically generates biomedical research podcasts. The workflow proceeds in four main stages:
1.  **Data Acquisition**: Scrapes recent publications from the IFC-UNAM website and finds related articles on PubMed.
2.  **AI Content Generation**: Uses Google Gemini to create an engaging podcast script from the collected research.
3.  **AI Voice Synthesis**: Converts the generated script into a high-quality audio file using Google's Text-to-Speech technology.
4.  **Analysis & Export**: Provides quality metrics and saves all generated assets.

---

## ‚öôÔ∏è Step 1: Setup and Configuration

This first step sets up the environment, imports all necessary modules, and loads the configuration. It includes robust checks to ensure that even if a module is missing, the notebook can proceed with a mock implementation.

```{python}
#| label: setup-and-imports
#| code-summary: "Setup environment, check and import all modules"

import sys
import os
from pathlib import Path
import asyncio
import pandas as pd
from datetime import datetime, timedelta
import json
import nest_asyncio

# Apply nest_asyncio to allow running async functions in the notebook
nest_asyncio.apply()

# --- 1. PATH SETUP ---
notebook_dir = Path().resolve()
# Handle running from root or /notebooks directory
src_dir = notebook_dir / "src" if (notebook_dir / "src").exists() else notebook_dir.parent / "src"
sys.path.insert(0, str(src_dir))

print(f"üìÅ Project directory: {src_dir.parent}")
print(f"üìÅ Source directory: {src_dir}")
print(f"‚úÖ Source exists: {src_dir.exists()}")

# --- 2. DYNAMIC MODULE CHECKING ---
def check_actual_modules():
    """Check what classes/functions are actually available in each module."""
    modules_to_check = [
        ("scrapers.ifc_scraper", "scrapers/ifc_scraper.py"),
        ("pubmed.searcher", "pubmed/searcher.py"),
        ("llm.script_generator", "llm/script_generator.py"),
        ("utils.config", "utils/config.py"),
        ("utils.logger", "utils/logger.py")
    ]
    available = {}
    print("\nüîç Checking module contents...")
    for name, path in modules_to_check:
        if (src_dir / path).exists():
            print(f"  ‚úÖ Found module: {name}")
            available[name] = True
        else:
            print(f"  ‚ùå Missing module: {name}")
            available[name] = False
    return available

module_info = check_actual_modules()

# --- 3. ROBUST IMPORTS WITH FALLBACKS ---
try:
    if module_info.get('utils.config'):
        from utils.config import load_config
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock `load_config`.")
    def load_config(): return {'llm': {}, 'api_keys': {}}

try:
    if module_info.get('utils.logger'):
        from utils.logger import setup_logger, get_logger
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock logger.")
    def setup_logger(level="INFO"): pass
    def get_logger(name): return logging.getLogger(name)

try:
    if module_info.get('scrapers.ifc_scraper'):
        from scrapers.ifc_scraper import IFCPublicationScraper
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock `IFCPublicationScraper`.")
    class IFCPublicationScraper:
        def __init__(self, c): pass
        async def scrape_publications_by_year(self, y): return []

try:
    if module_info.get('pubmed.searcher'):
        from pubmed.searcher import PubMedSearcher, PubMedArticle
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock `PubMedSearcher`.")
    class PubMedArticle: pass
    class PubMedSearcher:
        def __init__(self, c): pass
        async def search_recent_articles(self, q, d, m): return []
        async def fetch_article_details(self, p): return []

try:
    if module_info.get('llm.script_generator'):
        from llm.script_generator import PodcastScriptGenerator
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock `PodcastScriptGenerator`.")
    class PodcastScriptGenerator:
        def __init__(self, c): pass
        async def generate_podcast_script(self, d): return "Mock script."
        async def generate_episode_metadata(self, d): return {'title': 'Mock', 'tags': []}

# --- 4. NOTEBOOK-ONLY VOICE GENERATOR ---
# This class is defined here to use notebook-specific context like API clients.
class VoiceGenerator:
    """Generates audio using Google TTS, available only in this notebook context."""
    def __init__(self, config):
        self.config = config
        self.api_key = config.get('api_keys', {}).get('google_tts') or config.get('api_keys', {}).get('google')
        self.client = None
        self.available = False
        if self.api_key:
            try:
                from google import genai
                self.client = genai.Client(api_key=self.api_key)
                self.available = True
                print("‚úÖ VoiceGenerator initialized with Google TTS client.")
            except Exception as e:
                print(f"‚ùå VoiceGenerator failed to initialize: {e}")
        else:
            print("‚ö†Ô∏è VoiceGenerator: Missing Google TTS API key.")

    async def text_to_speech(self, text: str, output_path: Path):
        if not self.available:
            print("‚ùå Voice generation skipped: client not available.")
            return None
        try:
            print("üéôÔ∏è Generating audio with Google TTS...")
            response = self.client.models.generate_content(
                model=self.config.get('audio', {}).get('model', 'gemini-2.5-flash-preview-tts'),
                contents=f"Read this in a professional podcast host voice: {text}",
                config={"response_modalities": ['Audio']},
            )
            audio_data = response.candidates[0].content.parts[0].inline_data.data
            
            import wave
            with wave.open(str(output_path), "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(24000)
                wf.writeframes(audio_data)
            print(f"‚úÖ Audio saved to {output_path}")
            return output_path
        except Exception as e:
            print(f"‚ùå Voice generation failed: {e}")
            return None

# --- 5. INITIALIZE ---
setup_logger(level="INFO")
logger = get_logger("podcast_workflow")
config = load_config()
google_api_key = config.get('api_keys', {}).get('google', '')

print("\nüéâ Setup complete!")
if google_api_key:
    print(f"‚úÖ Google API key found: {google_api_key[:8]}...")
else:
    print("‚ùå Google API key not found in config!")

```

## üìö Step 2: Data Acquisition

We gather the raw material for our podcast by scraping IFC-UNAM for institutional research and searching PubMed for related, broader context.

```{python}
#| label: data-acquisition
#| code-summary: "Scrape IFC and search PubMed for articles"

async def acquire_data():
    """Scrape IFC and search PubMed for recent articles."""
    
    # --- IFC Scraping ---
    print("\nüï∑Ô∏è Scraping IFC-UNAM for recent publications...")
    ifc_scraper = IFCPublicationScraper(config)
    current_year = datetime.now().year
    try:
        ifc_publications = await ifc_scraper.scrape_publications_by_year(current_year)
        if not ifc_publications: # If no pubs in current year, try last year
            ifc_publications = await ifc_scraper.scrape_publications_by_year(current_year - 1)
        print(f"  ‚úÖ Found {len(ifc_publications)} publications from IFC.")
    except Exception as e:
        print(f"  ‚ùå IFC scraping failed: {e}. Proceeding without IFC data.")
        ifc_publications = []

    # --- PubMed Search ---
    print("\nüî¨ Searching PubMed for related articles...")
    pubmed_searcher = PubMedSearcher(config)
    # Use keywords from IFC pubs or default to a broad search
    search_terms = list(set(kw for pub in ifc_publications for kw in getattr(pub, 'keywords', [])))
    if not search_terms:
        search_terms = ['biomedical research', 'molecular biology', 'neuroscience']
        print(f"  ‚ö†Ô∏è No keywords from IFC, using default terms: {search_terms}")
    
    try:
        pmids = await pubmed_searcher.search_recent_articles(
            query_terms=search_terms, days_back=90, max_results=20
        )
        print(f"  ‚úÖ Found {len(pmids)} relevant article IDs from PubMed.")
        pubmed_articles = await pubmed_searcher.fetch_article_details(pmids[:10]) # Fetch details for top 10
        print(f"  ‚úÖ Fetched details for {len(pubmed_articles)} articles.")
    except Exception as e:
        print(f"  ‚ùå PubMed search failed: {e}. Proceeding without PubMed data.")
        pubmed_articles = []
        
    return ifc_publications, pubmed_articles

# Run data acquisition
ifc_publications, pubmed_articles = await acquire_data()
```

## ü§ñ Step 3: AI Content Generation

With our source material collected, we now use Google's Gemini model to analyze the articles and generate a structured, engaging podcast script and relevant metadata.

```{python}
#| label: ai-content-generation
#| code-summary: "Generate podcast script and metadata with AI"

async def generate_content():
    """Generate podcast script and metadata using an AI content generator."""
    print("\nü§ñ Generating podcast content with AI...")
    
    if not ifc_publications and not pubmed_articles:
        print("  ‚ùå Cannot generate content: No articles found.")
        return {'script': '', 'metadata': {}}

    content_data = {
        'ifc_publications': [p.__dict__ for p in ifc_publications[:2]],
        'pubmed_articles': [p.__dict__ for p in pubmed_articles[:3]],
        'topic': 'Recent Advances in Biomedical Science',
    }
    
    try:
        content_generator = PodcastScriptGenerator(config)
        script = await content_generator.generate_podcast_script(content_data)
        metadata = await content_generator.generate_episode_metadata(content_data)
        
        print("  ‚úÖ AI content generation successful.")
        print(f"  üìÑ Script preview: '{script[:200]}...'")
        print(f"  üè∑Ô∏è Episode Title: {metadata.get('title', 'N/A')}")
        return {'script': script, 'metadata': metadata}
        
    except Exception as e:
        print(f"  ‚ùå AI content generation failed: {e}")
        return {'script': 'Error generating script.', 'metadata': {'title': 'Error'}}

# Generate the content
podcast_content = await generate_content()
```

## üé§ Step 4: AI Voice Synthesis

The generated script is now converted into a natural-sounding audio file. This step uses the notebook-defined `VoiceGenerator` which leverages Google's TTS API.

```{python}
#| label: voice-synthesis
#| code-summary: "Convert script to audio using AI voice"

async def synthesize_voice():
    """Convert the generated script to an audio file."""
    print("\nüé§ Synthesizing voice...")
    
    if not podcast_content or not podcast_content.get('script') or "Error" in podcast_content.get('script'):
        print("  ‚ùå Cannot generate audio: Invalid script.")
        return None

    output_dir = Path("outputs/audio")
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    audio_file_path = output_dir / f"podcast_episode_{timestamp}.wav"
    
    voice_generator = VoiceGenerator(config)
    generated_file = await voice_generator.text_to_speech(podcast_content['script'], audio_file_path)
    return generated_file

# Generate the audio file
audio_file = await synthesize_voice()
```

## üìà Step 5: Analysis and Export

Finally, we analyze the results of our pipeline, display quality metrics, and save all the generated assets‚Äîthe script, metadata, and audio file‚Äîto the `outputs` directory.

```{python}
#| label: analysis-and-export
#| code-summary: "Analyze results and save all generated files"

def analyze_and_export():
    """Analyze pipeline results and save all assets."""
    print("\nüìä Analyzing results and exporting assets...")
    
    # --- Create Output Directory ---
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # --- Data Quality Analysis ---
    print("\n--- Data Quality ---")
    print(f"  IFC Publications Found: {len(ifc_publications)}")
    print(f"  PubMed Articles Found: {len(pubmed_articles)}")
    
    # --- Content Analysis ---
    print("\n--- Generated Content ---")
    script_len = len(podcast_content.get('script', ''))
    word_count = len(podcast_content.get('script', '').split())
    print(f"  Script Length: {script_len} characters (~{word_count} words)")
    print(f"  Episode Title: {podcast_content.get('metadata', {}).get('title', 'N/A')}")

    # --- Export JSON Content ---
    content_path = output_dir / f"podcast_{timestamp}.json"
    with open(content_path, 'w', encoding='utf-8') as f:
        json.dump(podcast_content, f, indent=2, ensure_ascii=False)
    print(f"\nüíæ Script and metadata saved to: {content_path}")

    # --- Final Status ---
    print("\n--- Pipeline Status ---")
    status = {
        "IFC Scraping": len(ifc_publications) > 0,
        "PubMed Search": len(pubmed_articles) > 0,
        "AI Script Generation": script_len > 100,
        "AI Voice Synthesis": audio_file is not None and audio_file.exists()
    }
    for component, success in status.items():
        print(f"  {'‚úÖ' if success else '‚ùå'} {component}")
    
    success_rate = sum(status.values()) / len(status) * 100
    print(f"  ----------------------")
    print(f"  üéØ Overall Success Rate: {success_rate:.0f}%")

# Run the final step
analyze_and_export()
```

---

## üõ†Ô∏è Technical Implementation & Usage

### Architecture

The system is built on a modular architecture:
-   **Scrapers**: Asynchronous clients for fetching data from web sources.
-   **LLM Providers**: A unified interface for interacting with different Large Language Models (e.g., Google Gemini).
-   **Workflow**: The main notebook orchestrates the flow of data between these components.

### Quick Start

1.  **Clone the repository.**
2.  **Install dependencies**: `pip install -r requirements.txt`
3.  **Configure APIs**: Copy `config/config.example.yaml` to `config/config.yaml` and add your API keys.
4.  **Run this file**: Use `quarto render index.qmd` to generate a full HTML report or run the cells interactively in a compatible IDE like VS Code.
```// filepath: /home/santi/Projects/UBMI-IFC-Podcast/index.qmd
---
title: "UBMI-IFC Podcast Generator"
subtitle: "An Automated Biomedical Research Podcast Generation System"
author: "UBMI-IFC Team"
date: today
format:
  html:
    theme: cosmo
    toc: true
    toc-depth: 3
    code-fold: show
    code-tools: true
    embed-resources: true
execute:
  warning: false
  error: false
---

# üéß UBMI-IFC Podcast Generator

This document is an interactive demonstration of an intelligent system that automatically generates biomedical research podcasts. The workflow proceeds in four main stages:
1.  **Data Acquisition**: Scrapes recent publications from the IFC-UNAM website and finds related articles on PubMed.
2.  **AI Content Generation**: Uses Google Gemini to create an engaging podcast script from the collected research.
3.  **AI Voice Synthesis**: Converts the generated script into a high-quality audio file using Google's Text-to-Speech technology.
4.  **Analysis & Export**: Provides quality metrics and saves all generated assets.

---

## ‚öôÔ∏è Step 1: Setup and Configuration

This first step sets up the environment, imports all necessary modules, and loads the configuration. It includes robust checks to ensure that even if a module is missing, the notebook can proceed with a mock implementation.

```{python}
#| label: setup-and-imports
#| code-summary: "Setup environment, check and import all modules"

import sys
import os
from pathlib import Path
import asyncio
import pandas as pd
from datetime import datetime, timedelta
import json
import nest_asyncio

# Apply nest_asyncio to allow running async functions in the notebook
nest_asyncio.apply()

# --- 1. PATH SETUP ---
notebook_dir = Path().resolve()
# Handle running from root or /notebooks directory
src_dir = notebook_dir / "src" if (notebook_dir / "src").exists() else notebook_dir.parent / "src"
sys.path.insert(0, str(src_dir))

print(f"üìÅ Project directory: {src_dir.parent}")
print(f"üìÅ Source directory: {src_dir}")
print(f"‚úÖ Source exists: {src_dir.exists()}")

# --- 2. DYNAMIC MODULE CHECKING ---
def check_actual_modules():
    """Check what classes/functions are actually available in each module."""
    modules_to_check = [
        ("scrapers.ifc_scraper", "scrapers/ifc_scraper.py"),
        ("pubmed.searcher", "pubmed/searcher.py"),
        ("llm.script_generator", "llm/script_generator.py"),
        ("utils.config", "utils/config.py"),
        ("utils.logger", "utils/logger.py")
    ]
    available = {}
    print("\nüîç Checking module contents...")
    for name, path in modules_to_check:
        if (src_dir / path).exists():
            print(f"  ‚úÖ Found module: {name}")
            available[name] = True
        else:
            print(f"  ‚ùå Missing module: {name}")
            available[name] = False
    return available

module_info = check_actual_modules()

# --- 3. ROBUST IMPORTS WITH FALLBACKS ---
try:
    if module_info.get('utils.config'):
        from utils.config import load_config
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock `load_config`.")
    def load_config(): return {'llm': {}, 'api_keys': {}}

try:
    if module_info.get('utils.logger'):
        from utils.logger import setup_logger, get_logger
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock logger.")
    def setup_logger(level="INFO"): pass
    def get_logger(name): return logging.getLogger(name)

try:
    if module_info.get('scrapers.ifc_scraper'):
        from scrapers.ifc_scraper import IFCPublicationScraper
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock `IFCPublicationScraper`.")
    class IFCPublicationScraper:
        def __init__(self, c): pass
        async def scrape_publications_by_year(self, y): return []

try:
    if module_info.get('pubmed.searcher'):
        from pubmed.searcher import PubMedSearcher, PubMedArticle
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock `PubMedSearcher`.")
    class PubMedArticle: pass
    class PubMedSearcher:
        def __init__(self, c): pass
        async def search_recent_articles(self, q, d, m): return []
        async def fetch_article_details(self, p): return []

try:
    if module_info.get('llm.script_generator'):
        from llm.script_generator import PodcastScriptGenerator
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock `PodcastScriptGenerator`.")
    class PodcastScriptGenerator:
        def __init__(self, c): pass
        async def generate_podcast_script(self, d): return "Mock script."
        async def generate_episode_metadata(self, d): return {'title': 'Mock', 'tags': []}

# --- 4. NOTEBOOK-ONLY VOICE GENERATOR ---
# This class is defined here to use notebook-specific context like API clients.
class VoiceGenerator:
    """Generates audio using Google TTS, available only in this notebook context."""
    def __init__(self, config):
        self.config = config
        self.api_key = config.get('api_keys', {}).get('google_tts') or config.get('api_keys', {}).get('google')
        self.client = None
        self.available = False
        if self.api_key:
            try:
                from google import genai
                self.client = genai.Client(api_key=self.api_key)
                self.available = True
                print("‚úÖ VoiceGenerator initialized with Google TTS client.")
            except Exception as e:
                print(f"‚ùå VoiceGenerator failed to initialize: {e}")
        else:
            print("‚ö†Ô∏è VoiceGenerator: Missing Google TTS API key.")

    async def text_to_speech(self, text: str, output_path: Path):
        if not self.available:
            print("‚ùå Voice generation skipped: client not available.")
            return None
        try:
            print("üéôÔ∏è Generating audio with Google TTS...")
            response = self.client.models.generate_content(
                model=self.config.get('audio', {}).get('model', 'gemini-2.5-flash-preview-tts'),
                contents=f"Read this in a professional podcast host voice: {text}",
                config={"response_modalities": ['Audio']},
            )
            audio_data = response.candidates[0].content.parts[0].inline_data.data
            
            import wave
            with wave.open(str(output_path), "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(24000)
                wf.writeframes(audio_data)
            print(f"‚úÖ Audio saved to {output_path}")
            return output_path
        except Exception as e:
            print(f"‚ùå Voice generation failed: {e}")
            return None

# --- 5. INITIALIZE ---
setup_logger(level="INFO")
logger = get_logger("podcast_workflow")
config = load_config()
google_api_key = config.get('api_keys', {}).get('google', '')

print("\nüéâ Setup complete!")
if google_api_key:
    print(f"‚úÖ Google API key found: {google_api_key[:8]}...")
else:
    print("‚ùå Google API key not found in config!")

```

## üìö Step 2: Data Acquisition

We gather the raw material for our podcast by scraping IFC-UNAM for institutional research and searching PubMed for related, broader context.

```{python}
#| label: data-acquisition
#| code-summary: "Scrape IFC and search PubMed for articles"

async def acquire_data():
    """Scrape IFC and search PubMed for recent articles."""
    
    # --- IFC Scraping ---
    print("\nüï∑Ô∏è Scraping IFC-UNAM for recent publications...")
    ifc_scraper = IFCPublicationScraper(config)
    current_year = datetime.now().year
    try:
        ifc_publications = await ifc_scraper.scrape_publications_by_year(current_year)
        if not ifc_publications: # If no pubs in current year, try last year
            ifc_publications = await ifc_scraper.scrape_publications_by_year(current_year - 1)
        print(f"  ‚úÖ Found {len(ifc_publications)} publications from IFC.")
    except Exception as e:
        print(f"  ‚ùå IFC scraping failed: {e}. Proceeding without IFC data.")
        ifc_publications = []

    # --- PubMed Search ---
    print("\nüî¨ Searching PubMed for related articles...")
    pubmed_searcher = PubMedSearcher(config)
    # Use keywords from IFC pubs or default to a broad search
    search_terms = list(set(kw for pub in ifc_publications for kw in getattr(pub, 'keywords', [])))
    if not search_terms:
        search_terms = ['biomedical research', 'molecular biology', 'neuroscience']
        print(f"  ‚ö†Ô∏è No keywords from IFC, using default terms: {search_terms}")
    
    try:
        pmids = await pubmed_searcher.search_recent_articles(
            query_terms=search_terms, days_back=90, max_results=20
        )
        print(f"  ‚úÖ Found {len(pmids)} relevant article IDs from PubMed.")
        pubmed_articles = await pubmed_searcher.fetch_article_details(pmids[:10]) # Fetch details for top 10
        print(f"  ‚úÖ Fetched details for {len(pubmed_articles)} articles.")
    except Exception as e:
        print(f"  ‚ùå PubMed search failed: {e}. Proceeding without PubMed data.")
        pubmed_articles = []
        
    return ifc_publications, pubmed_articles

# Run data acquisition
ifc_publications, pubmed_articles = await acquire_data()
```

## ü§ñ Step 3: AI Content Generation

With our source material collected, we now use Google's Gemini model to analyze the articles and generate a structured, engaging podcast script and relevant metadata.

```{python}
#| label: ai-content-generation
#| code-summary: "Generate podcast script and metadata with AI"

async def generate_content():
    """Generate podcast script and metadata using an AI content generator."""
    print("\nü§ñ Generating podcast content with AI...")
    
    if not ifc_publications and not pubmed_articles:
        print("  ‚ùå Cannot generate content: No articles found.")
        return {'script': '', 'metadata': {}}

    content_data = {
        'ifc_publications': [p.__dict__ for p in ifc_publications[:2]],
        'pubmed_articles': [p.__dict__ for p in pubmed_articles[:3]],
        'topic': 'Recent Advances in Biomedical Science',
    }
    
    try:
        content_generator = PodcastScriptGenerator(config)
        script = await content_generator.generate_podcast_script(content_data)
        metadata = await content_generator.generate_episode_metadata(content_data)
        
        print("  ‚úÖ AI content generation successful.")
        print(f"  üìÑ Script preview: '{script[:200]}...'")
        print(f"  üè∑Ô∏è Episode Title: {metadata.get('title', 'N/A')}")
        return {'script': script, 'metadata': metadata}
        
    except Exception as e:
        print(f"  ‚ùå AI content generation failed: {e}")
        return {'script': 'Error generating script.', 'metadata': {'title': 'Error'}}

# Generate the content
podcast_content = await generate_content()
```

## üé§ Step 4: AI Voice Synthesis

The generated script is now converted into a natural-sounding audio file. This step uses the notebook-defined `VoiceGenerator` which leverages Google's TTS API.

```{python}
#| label: voice-synthesis
#| code-summary: "Convert script to audio using AI voice"

async def synthesize_voice():
    """Convert the generated script to an audio file."""
    print("\nüé§ Synthesizing voice...")
    
    if not podcast_content or not podcast_content.get('script') or "Error" in podcast_content.get('script'):
        print("  ‚ùå Cannot generate audio: Invalid script.")
        return None

    output_dir = Path("outputs/audio")
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    audio_file_path = output_dir / f"podcast_episode_{timestamp}.wav"
    
    voice_generator = VoiceGenerator(config)
    generated_file = await voice_generator.text_to_speech(podcast_content['script'], audio_file_path)
    return generated_file

# Generate the audio file
audio_file = await synthesize_voice()
```

## üìà Step 5: Analysis and Export

Finally, we analyze the results of our pipeline, display quality metrics, and save all the generated assets‚Äîthe script, metadata, and audio file‚Äîto the `outputs` directory.

```{python}
#| label: analysis-and-export
#| code-summary: "Analyze results and save all generated files"

def analyze_and_export():
    """Analyze pipeline results and save all assets."""
    print("\nüìä Analyzing results and exporting assets...")
    
    # --- Create Output Directory ---
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # --- Data Quality Analysis ---
    print("\n--- Data Quality ---")
    print(f"  IFC Publications Found: {len(ifc_publications)}")
    print(f"  PubMed Articles Found: {len(pubmed_articles)}")
    
    # --- Content Analysis ---
    print("\n--- Generated Content ---")
    script_len = len(podcast_content.get('script', ''))
    word_count = len(podcast_content.get('script', '').split())
    print(f"  Script Length: {script_len} characters (~{word_count} words)")
    print(f"  Episode Title: {podcast_content.get('metadata', {}).get('title', 'N/A')}")

    # --- Export JSON Content ---
    content_path = output_dir / f"podcast_{timestamp}.json"
    with open(content_path, 'w', encoding='utf-8') as f:
        json.dump(podcast_content, f, indent=2, ensure_ascii=False)
    print(f"\nüíæ Script and metadata saved to: {content_path}")

    # --- Final Status ---
    print("\n--- Pipeline Status ---")
    status = {
        "IFC Scraping": len(ifc_publications) > 0,
        "PubMed Search": len(pubmed_articles) > 0,
        "AI Script Generation": script_len > 100,
        "AI Voice Synthesis": audio_file is not None and audio_file.exists()
    }
    for component, success in status.items():
        print(f"  {'‚úÖ' if success else '‚ùå'} {component}")
    
    success_rate = sum(status.values()) / len(status) * 100
    print(f"  ----------------------")
    print(f"  üéØ Overall Success Rate: {success_rate:.0f}%")

# Run the final step
analyze_and_export()
```

---

## üõ†Ô∏è Technical Implementation & Usage

### Architecture

The system is built on a modular architecture:
-   **Scrapers**: Asynchronous clients for fetching data from web sources.
-   **LLM Providers**: A unified interface for interacting with different Large Language Models (e.g., Google Gemini).
-   **Workflow**: The main notebook orchestrates the flow of data between these components.

### Quick Start

1.  **Clone the repository.**
2.  **Install dependencies**: `pip install -r requirements.txt`
3.  **Configure APIs**: Copy `config/config.example.yaml` to `config/config.yaml` and add your API keys.
4.  **Run this file**: Use `quarto render index.qmd` to generate a full HTML report or run the cells interactively in a compatible IDE like VS Code.