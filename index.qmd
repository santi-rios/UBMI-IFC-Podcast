---
title: "UBMI-IFC Podcast Generator"
subtitle: "An Automated Biomedical Research Podcast Generation System"
author: "UBMI-IFC Team"
date: today
date-modified: today
format:
  html:
    theme: darkly
    embed-resources: true
toc: true
toc-depth: 3
execute:
  warning: false
  error: false
code-fold: true
code-copy: true
code-tools:
    source: repo
code-line-numbers: true
highlight-style: atom-one
lightbox: true
footnotes-hover: true
reference-location: block
---

# üéß UBMI-IFC Podcast Generator

This document is an interactive demonstration of an intelligent system that automatically generates biomedical research podcasts. The workflow proceeds in four main stages:
1.  **Data Acquisition**: Scrapes recent publications from the IFC-UNAM website and finds related articles on PubMed.
2.  **AI Content Generation**: Uses Google Gemini to create an engaging podcast script from the collected research.
3.  **AI Voice Synthesis**: Converts the generated script into a high-quality audio file using Google's Text-to-Speech technology.
4.  **Analysis & Export**: Provides quality metrics and saves all generated assets.

---

## ‚öôÔ∏è Step 1: Setup and Configuration

This first step sets up the environment, imports all necessary modules, and loads the configuration. It includes robust checks to ensure that even if a module is missing, the notebook can proceed with a mock implementation.

```bash
source venv/bin/activate
```

```{python}
#| label: setup-and-imports
#| code-summary: "Setup environment, check and import all modules"

import sys
import os
from pathlib import Path
import asyncio
import pandas as pd
from datetime import datetime, timedelta
import json
import nest_asyncio
import aiohttp
from bs4 import BeautifulSoup

# Apply nest_asyncio to allow running async functions in the notebook
nest_asyncio.apply()

# --- 1. PATH SETUP ---
notebook_dir = Path().resolve()
# Handle running from root or /notebooks directory
src_dir = notebook_dir / "src" if (notebook_dir / "src").exists() else notebook_dir.parent / "src"
sys.path.insert(0, str(src_dir))

print(f"üìÅ Project directory: {src_dir.parent}")
print(f"üìÅ Source directory: {src_dir}")
print(f"‚úÖ Source exists: {src_dir.exists()}")
```


```{python}
#| code-summary: "MODULE CHECKING"

# --- 2. DYNAMIC MODULE CHECKING ---
def check_actual_modules():
    """Check what classes/functions are actually available in each module."""
    modules_to_check = [
        ("scrapers.ifc_scraper", "scrapers/ifc_scraper.py"),
        ("pubmed.searcher", "pubmed/searcher.py"),
        ("llm.script_generator", "llm/script_generator.py"),
        ("utils.config", "utils/config.py"),
        ("utils.logger", "utils/logger.py")
    ]
    available = {}
    print("\nüîç Checking module contents...")
    for name, path in modules_to_check:
        if (src_dir / path).exists():
            print(f"  ‚úÖ Found module: {name}")
            available[name] = True
        else:
            print(f"  ‚ùå Missing module: {name}")
            available[name] = False
    return available

module_info = check_actual_modules()
```


```{python}
#| code-summary: "Imports"

# --- 3. IMPORTS WITH FALLBACKS ---
try:
    if module_info.get('utils.config'):
        from utils.config import load_config
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock `load_config`.")
    def load_config(): return {'llm': {}, 'api_keys': {}}

try:
    if module_info.get('utils.logger'):
        from utils.logger import setup_logger, get_logger
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock logger.")
    def setup_logger(level="INFO"): pass
    def get_logger(name): return logging.getLogger(name)

try:
    if module_info.get('scrapers.ifc_scraper'):
        from scrapers.ifc_scraper import IFCPublicationScraper
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock `IFCPublicationScraper`.")
    class IFCPublicationScraper:
        def __init__(self, c): pass
        async def scrape_publications_by_year(self, y): return []

try:
    if module_info.get('pubmed.searcher'):
        from pubmed.searcher import PubMedSearcher, PubMedArticle
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock `PubMedSearcher`.")
    class PubMedArticle: pass
    class PubMedSearcher:
        def __init__(self, c): pass
        async def search_recent_articles(self, q, d, m): return []
        async def fetch_article_details(self, p): return []

try:
    if module_info.get('llm.script_generator'):
        from llm.script_generator import PodcastScriptGenerator
    else: raise ImportError()
except ImportError:
    print("‚ö†Ô∏è Using mock `PodcastScriptGenerator`.")
    class PodcastScriptGenerator:
        def __init__(self, c): pass
        async def generate_podcast_script(self, d): return "Mock script."
        async def generate_episode_metadata(self, d): return {'title': 'Mock', 'tags': []}

```


```{python}
#| code-summary: "VOICE GENERATOR"

# --- 4. NOTEBOOK-ONLY VOICE GENERATOR ---
# This class is defined here to use notebook-specific context like API clients.
class VoiceGenerator:
    """Generates audio using Google TTS."""
    def __init__(self, config):
        self.config = config
        self.api_key = config.get('api_keys', {}).get('google_tts') or config.get('api_keys', {}).get('google')
        self.client = None
        self.available = False
        if self.api_key:
            try:
                from google import genai
                self.client = genai.Client(api_key=self.api_key)
                self.available = True
                print("‚úÖ VoiceGenerator initialized with Google TTS client.")
            except Exception as e:
                print(f"‚ùå VoiceGenerator failed to initialize: {e}")
        else:
            print("‚ö†Ô∏è VoiceGenerator: Missing Google TTS API key.")

    async def text_to_speech(self, text: str, output_path: Path):
        if not self.available:
            print("‚ùå Voice generation skipped: client not available.")
            return None
        try:
            print("üéôÔ∏è Generating audio with Google TTS...")
            response = self.client.models.generate_content(
                model=self.config.get('audio', {}).get('model', 'gemini-2.5-flash-preview-tts'),
                contents=f"Read this in a professional podcast host voice: {text}",
                config={"response_modalities": ['Audio']},
            )
            audio_data = response.candidates[0].content.parts[0].inline_data.data
            
            import wave
            with wave.open(str(output_path), "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(24000)
                wf.writeframes(audio_data)
            print(f"‚úÖ Audio saved to {output_path}")
            return output_path
        except Exception as e:
            print(f"‚ùå Voice generation failed: {e}")
            return None
```


```{python}
#| code-summary: "INITIALIZE"

# --- 5. INITIALIZE ---
setup_logger(level="INFO")
logger = get_logger("podcast_workflow")
config = load_config()
google_api_key = config.get('api_keys', {}).get('google', '')

print("\nüéâ Setup complete!")
if google_api_key:
    print(f"‚úÖ Google API key found: {google_api_key[:8]}...")
else:
    print("‚ùå Google API key not found in config!")

```


## üìö Step 2: Data Acquisition

We gather the raw material for our podcast by scraping IFC-UNAM for institutional research and searching PubMed for related, broader context.

(I think we should expect ~123 publicaations for 2024)

```{python}
#| label: data-acquisition
#| code-summary: "Scrape IFC 2024"

import aiohttp
from bs4 import BeautifulSoup
import re
import pandas as pd

# 1. IMPROVED SCRAPER
# This version is more flexible and should extract more publications.
async def scrape_ifc_publications_revised(year=2024):
    """
    Revised scraper with more robust parsing and cleaner output preparation.
    """
    url = f"https://www.ifc.unam.mx/publicaciones.php?year={year}"
    
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            print(f"üîç Scraping IFC publications for {year}...")
            
            # This selector is good, it finds all the publication containers.
            publication_links = soup.find_all('a', class_=['opensans400', 'd-flexy'])
            print(f"üìö Found {len(publication_links)} potential publication links. Attempting to parse all...")
            
            publications = []
            
            for link in publication_links:
                pub_text = link.get_text().strip().replace('\n', ' ')
                
                # Default values
                authors, pub_year, title, journal, doi = None, year, None, None, None

                try:
                    # More robust extraction using combined regex and string splitting
                    
                    # Extract Authors and Year: Captures everything before "(YYYY)"
                    author_year_match = re.search(r'^(.*?)\s*\((\d{4})\)', pub_text)
                    if author_year_match:
                        authors = author_year_match.group(1).strip().rstrip('.')
                        pub_year = int(author_year_match.group(2))

                    # Extract DOI: Standard DOI pattern
                    doi_match = re.search(r'(10\.\d+/[^\s<>"]+)', pub_text)
                    if doi_match:
                        doi = doi_match.group(1).strip()

                    # Extract Title and Journal from the text between year and DOI
                    if author_year_match:
                        # Get the text after the year "(YYYY). "
                        remaining_text = pub_text[author_year_match.end(0):].strip()
                        if remaining_text.startswith('.'):
                            remaining_text = remaining_text[1:].strip()
                        
                        # Remove the DOI part if it exists, to not confuse the parser
                        if doi:
                            remaining_text = remaining_text.replace(doi, '')

                        # Split the remainder by periods. Usually: "Title. Journal. Other info."
                        parts = [part.strip() for part in remaining_text.split('.') if part]
                        if len(parts) > 0:
                            title = parts[0]
                        if len(parts) > 1:
                            journal = parts[1]

                    # Get the IFC-specific URL
                    ifc_url = link.get('href')
                    if ifc_url and not ifc_url.startswith('http'):
                        ifc_url = f"https://www.ifc.unam.mx/{ifc_url}"
                    
                    publications.append({
                        'title': title,
                        'authors': authors,
                        'journal': journal,
                        'year': pub_year,
                        'doi': doi,
                        'ifc_url': ifc_url,
                    })
                
                except Exception as e:
                    # This will catch any unexpected errors for a specific entry
                    print(f"Could not parse entry: {pub_text[:60]}... | Error: {e}")
                    continue

            print(f"üéâ Successfully extracted information for {len(publications)} publications!")
            return publications

# --- Main execution block ---
publications_data = await scrape_ifc_publications_revised(2024)

# 2. TIDY TABLE OUTPUT
# Convert the list of dictionaries into a Pandas DataFrame
if publications_data:
    df_publications = pd.DataFrame(publications_data)
    
    # Set display options for better viewing within notebooks/Quarto
    pd.set_option('display.max_rows', 10)
    pd.set_option('display.max_colwidth', 60)

    # Displaying the DataFrame as the last line in the cell
    # Quarto will automatically render this as a paginated table.
    df_publications


```
