---
title: "UBMI-IFC Podcast Generator"
subtitle: "An Automated Biomedical Research Podcast Generation System"
author: "UBMI-IFC Team"
date: today
date-modified: today
format:
  html:
    theme: darkly
    embed-resources: true
toc: true
toc-depth: 3
execute:
  warning: false
  error: false
code-fold: true
code-copy: true
code-tools:
    source: repo
code-line-numbers: true
highlight-style: atom-one
lightbox: true
footnotes-hover: true
reference-location: block
---


## Scraper

- Obtener todas las publicaciones (2021-2025) de la [p√°gina del IFC](https://www.ifc.unam.mx/publicaciones.php?year=2025).


```{python}
import json
import pandas as pd
import itables
from IPython.display import HTML, display

# Enable interactive mode for all DataFrames in the notebook
itables.init_notebook_mode(all_interactive=True)

# Silence itables typing warnings about undocumented 'options' argument
itables.options.warn_on_undocumented_option = False

# Cargar JSON
try:
    with open('./data/raw/all_ifc_publications.json', 'r', encoding='utf-8') as file:
        publications_scraper = json.load(file)
    print(f"Resultado de scraper: {len(publications_scraper)} publicaciones")
except FileNotFoundError:
    print("Error: './data/raw/all_ifc_publications.json' no existe.")
    publications_scraper = []
except json.JSONDecodeError:
    print("Error: formato inv√°lido de JSON.")
    publications_scraper = []

# Si hay datos, convertir a DataFrame y mostrar con itables (si est√° disponible)
if publications_scraper:
    # Normalizar estructuras JSON anidadas a columnas planas
    df = pd.json_normalize(publications_scraper)

    # Mostrar en orden aleatorio
    df = df.sample(frac=1).reset_index(drop=True)

    # Inject CSS to truncate long text with ellipsis in the rendered DataTable
    css = """
    <style>
    /* Target DataTables cells rendered by itables */
    table.dataTable td, table.dataTable th {
      max-width: 180px;
      white-space: nowrap;
      overflow: hidden;
      text-overflow: ellipsis;
    }
    /* Allow horizontal scrolling for very wide tables */
    div.dataTables_wrapper {
      overflow-x: auto;
    }
    </style>
    """
    display(HTML(css))

    # DataTables options: fix column widths and enable horizontal scroll
    dt_options = {
        'autoWidth': False,
        'columnDefs': [{'targets': '_all', 'width': '180px'}],
        'scrollX': True,
        'pageLength': 25
    }

    itables.show(df, classes='stripe hover order-column', options=dt_options)

```

## Expandir base de datos

Queremos expandir la base de datos buscando en PUBMED las publicaciones relacioneadas a la afiliaci√≥n "IFC, UNAM". PAra obtener los nombres de las variaciones del instituto, vamos a ver c√≥mo se escribe en los art√≠√≥culos para mejorar la busqueda.

### Obtener PDFs

Para esto, podemos usar una de las 3 variaciones reportadas en 'notebooks/02_expand_database.ipynb'. Se utiliz√≥ la opci√≥n B (exportar las publicaciones a bib y de ah√≠ importar a zotero y utilizar una extensi√≥n que descargue los PDFs).

```{python}
# Let's also check the actual BibTeX content
publicaciones_bibtex = './data/processed/all_ifc_publications.bib'

print("\nüìÑ Ejemplo de art√≠culos en BibTeX:")
with open(publicaciones_bibtex, 'r', encoding='utf-8') as f:
    content = f.read()
    # Show first entry
    first_entry_end = content.find('\n}\n') + 3
    print(content[:first_entry_end])
```

PDF obtenidos: 

```{python}
from pathlib import Path

dir_path = Path('papers/downloaded/zotero')
if not dir_path.exists():
    print(0)
else:
    count = sum(1 for p in dir_path.rglob('*') if p.is_file() and p.suffix.lower() == '.pdf')
    print(count)
```


### Minado de afiliaciones

Ahora vamos a extraer a partir del texto de los PDFs (utilizando 'PyMuPDF') en conjunto con 'spaCy'.

- Utiliza **regex** y [NLP](https://en.wikipedia.org/wiki/Natural_language_processing) para detectar las afiliaciones
- Consideramos ingl√©s y espa√±ol ('en_core_web_sm' y 'es_core_web_sm')
- A partir de las afiliaciones, expandiremos la b√∫squeda de Pubmed

Resultados:

```{python}
# Load the pre-filtered affiliation results
import json

def load_filtered_affiliations(min_score=15.0):
    """
    Load pre-filtered affiliation clusters for PubMed searches.
    
    Args:
        min_score: Minimum relevance score to include (default: 15.0 for high quality)
        
    Returns:
        List of affiliation terms optimized for PubMed searches
    """
    
    filtered_file = './data/processed/filtered_affiliations.json'
    
    print(f"üìÅ Cargando afiliaciones: {filtered_file}")
    
    with open(filtered_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    clusters = data['relevant_affiliation_clusters']
    
    # Filter by score and extract search terms
    affiliation_terms = []
    
    for cluster in clusters:
        if cluster['relevance_score'] >= min_score:
            
            # Add representative term
            representative = clean_affiliation_for_search(cluster['representative'])
            if representative:
                affiliation_terms.append(f'"{representative}"[Affiliation]')
            
            # Add top variations (limit to avoid too many terms)
            for variation in cluster['variations'][:3]:  # Top 3 variations per cluster
                cleaned = clean_affiliation_for_search(variation)
                if cleaned and len(cleaned) > 10:  # Only substantial terms
                    search_term = f'"{cleaned}"[Affiliation]'
                    if search_term not in affiliation_terms:  # Avoid duplicates
                        affiliation_terms.append(search_term)
    
    print(f"‚úÖ Cargando {len(affiliation_terms)} t√©rminos de b√∫squeda de {len([c for c in clusters if c['relevance_score'] >= min_score])} clusters")
    
    return affiliation_terms

def clean_affiliation_for_search(term):
    """Clean an affiliation term for PubMed search."""
    import re
    
    if not term:
        return ""
    
    # Remove common noise patterns
    cleaned = re.sub(r'[‚Ä¢\d]+\s*', '', term)  # Remove bullets and leading numbers
    cleaned = re.sub(r'[^\w\s\-,.]', ' ', cleaned)  # Remove special chars except basic punctuation
    cleaned = re.sub(r'\s+', ' ', cleaned)  # Normalize whitespace
    cleaned = cleaned.strip()
    
    # Remove very generic prefixes
    prefixes_to_remove = ['the ', 'a ', 'an ', 'at the ', 'from the ']
    for prefix in prefixes_to_remove:
        if cleaned.lower().startswith(prefix):
            cleaned = cleaned[len(prefix):]
    
    # Skip very short or generic terms
    if len(cleaned) < 8:
        return ""
    
    generic_terms = ['university', 'institute', 'department', 'school', 'college']
    if cleaned.lower() in generic_terms:
        return ""
    
    return cleaned

# Load the filtered affiliations
print("üîç Afiliaciones para b√∫squeda de PUBMED")
print("=" * 60)

filtered_affiliations = load_filtered_affiliations(min_score=15.0)

print(f"\nüìã TOP 10 t√©rminos:")
for i, term in enumerate(filtered_affiliations[:10], 1):
    print(f"{i:2d}. {term}")

```

Despu√©s de revisar y limpiar manualmente los t√©rminos de b√∫squeda encontrados, hacemos la b√∫squeda en Pubmed y expandimos la base de datos:

```{python}
# Compare counts between two JSON files and show a table with the raw JSON contents for exploration

from pathlib import Path
import json
import pandas as pd
import itables
from IPython.display import HTML, display

itables.init_notebook_mode(all_interactive=True)
itables.options.warn_on_undocumented_option = False

def safe_load_json(path):
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"‚ö†Ô∏è Archivo no encontrado: {path}")
        return None
    except json.JSONDecodeError:
        print(f"‚ö†Ô∏è JSON inv√°lido: {path}")
        return None

def count_records(obj):
    """Heur√≠stica simple para contar entradas en un JSON cargado."""
    if obj is None:
        return 0
    if isinstance(obj, list):
        return len(obj)
    if isinstance(obj, dict):
        # If dict contains an obvious list of records, pick the largest list
        list_lengths = [len(v) for v in obj.values() if isinstance(v, list)]
        if list_lengths:
            return max(list_lengths)
        # fallback: count top-level keys as 1 record (or 0)
        return 1 if obj else 0
    return 0

# Paths to compare (if the "processed" file does not exist, we'll compare the same raw file)
path_a = Path('./data/raw/all_ifc_publications.json')
path_b = Path('./data/processed/pubmed_filtered_search_results.json')

if not path_b.exists():
    # Fall back to the same file if processed version is not present
    path_b = path_a

json_a = safe_load_json(path_a) or []
json_b = safe_load_json(path_b) or []

count_a = count_records(json_a)
count_b = count_records(json_b)
diff = count_b - count_a

# Print a short comparison summary
print(f"Archivo A: {path_a} ‚Üí {count_a} entradas")
print(f"Archivo B: {path_b} ‚Üí {count_b} entradas")
if path_a.samefile(path_b):
    print("Nota: ambos paths apuntan al mismo archivo.")
print(f"Diferencia (B - A): {diff}")

# Mostrar una peque√±a tabla resumen con pandas + itables
summary_df = pd.DataFrame({
    'archivo': [str(path_a), str(path_b)],
    'entradas': [count_a, count_b]
})
display(HTML("<h4>Resumen de conteo</h4>"))
itables.show(summary_df, classes='stripe hover order-column', options={'paging': False, 'searching': False})

# Finalmente, mostrar el contenido del JSON A como DataFrame para exploraci√≥n
display(HTML("<h4>Exploraci√≥n: contenido de ./data/processed/pubmed_filtered_search_results.json</h4>"))

def extract_record_list(obj):
    """Return a list of records from common JSON shapes."""
    if obj is None:
        return []
    if isinstance(obj, list):
        return obj
    if isinstance(obj, dict):
        # Pick the largest list value if any (e.g. 'articles', 'items', 'results')
        list_values = [v for v in obj.values() if isinstance(v, list)]
        if list_values:
            return max(list_values, key=len)
        # Otherwise treat the dict itself as a single record
        return [obj]
    # Fallback: wrap scalar into list
    return [obj]

# Replace the previous normalization attempt with:
records = extract_record_list(json_b)

if not records:
    print("No records found in JSON to normalize.")
else:
    try:
        df_raw = pd.json_normalize(records)
    except Exception:
        # Last-resort: wrap the whole object
        df_raw = pd.DataFrame(records)

    # Keep display compact: randomize order and limit columns width (CSS)
    df_raw = df_raw.sample(frac=1).reset_index(drop=True)
    css = """
    <style>
    table.dataTable td, table.dataTable th { max-width: 180px; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }
    div.dataTables_wrapper { overflow-x: auto; }
    </style>
    """
    display(HTML(css))
    itables.show(df_raw, classes='stripe hover order-column',
                 options={'autoWidth': False,
                          'columnDefs': [{'targets': '_all', 'width': '180px'}],
                          'scrollX': True, 'pageLength': 25})

```


## Embeddings

Probado con:

üî• PyTorch: 2.8.0+cu128
üñ•Ô∏è CUDA
üéÆ GPU: NVIDIA GeForce RTX 3050 Laptop GPU
üíæ GPU: 4.0 GB

### EmbeddingGemma

Par√°metros:

üîß cuda: True
üì• Modelo EmbeddingGemma: 'google/embeddinggemma-300M'
üìä Par√°metros del modelo: '307,581,696'


```{python}
from pathlib import Path
from IPython.display import HTML, IFrame, display

viz_path = Path('./notebooks/notebooks/data/processed/embeddinggemma_visualization_a.html')

if viz_path.exists():
    try:
        # Try embedding raw HTML so interactive JS/CSS stays inline
        display(HTML(viz_path.read_text(encoding='utf-8')))
    except Exception:
        # Fallback to an iframe if direct embedding fails
        display(IFrame(src=str(viz_path), width='100%', height=700))
else:
    print(f"Visualization not found: {viz_path}")
```

### Clusters

Se utiliz√≥ la t√©cnica de TF-IDF (Term Frequency-Inverse Document Frequency), utilizada en procesamiento de lenguaje natural (NLP),para identificar palabras clave o t√©rminos representativos.

::: {.callout-note}

1. **Frecuencia de t√©rmino (TF)**: Mide cu√°ntas veces aparece un t√©rmino en un documento en relaci√≥n con la longitud total del documento. Esto ayuda a identificar t√©rminos frecuentes dentro de un documento.

$$
TF(t) = \frac{\text{N√∫mero de veces que aparece el t√©rmino } t}{\text{N√∫mero total de t√©rminos en el documento}}
$$

2. **Frecuencia inversa de documento (IDF)**: Mide qu√© tan √∫nico es un t√©rmino en el corpus. Si un t√©rmino aparece en muchos documentos, su IDF ser√° bajo, ya que no es distintivo.
$$
IDF(t) = \log\left(\frac{\text{N√∫mero total de documentos}}{\text{N√∫mero de documentos que contienen el t√©rmino } t}\right)
$$

3. **TF-IDF**: Es el producto de TF e IDF. Los t√©rminos con un TF-IDF alto son aquellos que son frecuentes en un documento pero raros en el resto del corpus, lo que los hace representativos.

$$
TF-IDF(t) = TF(t) \times IDF(t)
$$

:::

```{python}
from pathlib import Path
from IPython.display import HTML, IFrame, display
import re

viz_path = Path('notebooks/notebooks/data/processed/embeddinggemma_visualization_cluster.html')

if not viz_path.exists():
    print(f"Visualization not found: {viz_path}")
else:
    html_text = viz_path.read_text(encoding='utf-8')

    # Try to embed the full HTML inline (best for preserving JS/CSS)
    try:
        display(HTML(html_text))
    except Exception:
        # If inline embedding fails (often due to heavy JS), show a static table (if present)
        table_match = re.search(r'(<table[\s\S]*?>[\s\S]*?</table>)', html_text, flags=re.IGNORECASE)
        if table_match:
            display(HTML(table_match.group(1)))
        # Always provide an iframe fallback for the interactive plot/visualization
        display(IFrame(src=str(viz_path), width='100%', height=800))
```


```{python}
from pathlib import Path
from IPython.display import HTML, IFrame, display
import re

html_path = Path('notebooks/notebooks/data/processed/embeddinggemma_cluster_summary.html')

if not html_path.exists():
    print(f"Visualization not found: {html_path}")
else:
    html_text = html_path.read_text(encoding='utf-8')
    table_html = None

    # Prefer BeautifulSoup if available for robust extraction
    try:
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_text, 'html.parser')
        tables = soup.find_all('table')
        if tables:
            table_html = ''.join(str(t) for t in tables)
    except Exception:
        # Fallback to regex if BeautifulSoup is not installed
        m = re.search(r'(<table[\s\S]*?>[\s\S]*?</table>)', html_text, flags=re.IGNORECASE)
        if m:
            table_html = m.group(1)

    if table_html:
        # Wrap table in a responsive container to allow horizontal scrolling
        display(HTML(f'<div style="overflow-x:auto">{table_html}</div>'))
    else:
        # If no table was found, embed the full HTML as an iframe
        display(HTML("<p>No &lt;table&gt; found in the visualization file. Showing full HTML below.</p>"))
        display(IFrame(src=str(html_path), width='100%', height=800))

```