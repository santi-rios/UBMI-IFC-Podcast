{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Final Podcast Generation Pipeline\n",
    "\n",
    "This notebook completes the podcast generation pipeline by taking the top similarity matches from ChromaDB and generating complete podcast episodes using:\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Load Similarity Results** - Import top matches from ChromaDB similarity search\n",
    "2. **AI-Powered Classification** - Automatically classify research fields using embeddings\n",
    "3. **Structured Script Generation** - Create consistent scientific narratives using Pydantic\n",
    "4. **Multi-Modal RAG Context** - Enhance scripts with related research context\n",
    "5. **Voice Synthesis** - Generate audio using Google's Text-to-Speech API\n",
    "6. **Complete Podcast Assembly** - Combine all elements into final podcast episodes\n",
    "\n",
    "## Scientific Purpose:\n",
    "- **Automated Content Creation**: Transform research discoveries into accessible podcast content\n",
    "- **Context-Aware Narratives**: Place new research within broader scientific landscape\n",
    "- **Standardized Quality**: Ensure consistent, high-quality scientific communication\n",
    "- **Scalable Production**: Enable regular podcast generation from ongoing research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ FINAL PODCAST GENERATION PIPELINE\n",
      "============================================================\n",
      "üìÅ Directories:\n",
      "   Notebook: /home/santi/Projects/UBMI-IFC-Podcast/notebooks\n",
      "   Data: /home/santi/Projects/UBMI-IFC-Podcast/notebooks/data\n",
      "   Output: /home/santi/Projects/UBMI-IFC-Podcast/outputs/final_podcasts\n",
      "‚úÖ pydantic available\n",
      "üì¶ Installing google-generativeai...\n",
      "Requirement already satisfied: google-generativeai in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (0.7.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-generativeai) (0.6.6)\n",
      "Requirement already satisfied: google-api-core in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-generativeai) (2.182.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-generativeai) (4.25.8)\n",
      "Requirement already satisfied: pydantic in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: tqdm in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-generativeai) (4.14.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.6->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.75.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.62.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "‚úÖ google-generativeai installed\n",
      "üì¶ Installing google-cloud-texttospeech...\n",
      "Requirement already satisfied: google-cloud-texttospeech in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (2.30.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (2.25.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-cloud-texttospeech) (2.40.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-cloud-texttospeech) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-cloud-texttospeech) (4.25.8)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (1.75.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (1.62.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-texttospeech) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-texttospeech) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-texttospeech) (4.9.1)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-texttospeech) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-texttospeech) (0.6.1)\n",
      "‚úÖ google-cloud-texttospeech installed\n",
      "‚úÖ pydub available\n",
      "\n",
      "üéØ All dependencies ready!\n"
     ]
    }
   ],
   "source": [
    "# 1. SETUP AND IMPORTS\n",
    "print(\"üöÄ FINAL PODCAST GENERATION PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Add project paths\n",
    "notebook_dir = Path().resolve()\n",
    "src_dir = notebook_dir.parent / 'src'\n",
    "data_dir = notebook_dir.parent / 'notebooks/data'\n",
    "outputs_dir = notebook_dir.parent / 'outputs'\n",
    "podcast_output_dir = outputs_dir / 'final_podcasts'\n",
    "\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "\n",
    "# Create output directories\n",
    "podcast_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Directories:\")\n",
    "print(f\"   Notebook: {notebook_dir}\")\n",
    "print(f\"   Data: {data_dir}\")\n",
    "print(f\"   Output: {podcast_output_dir}\")\n",
    "\n",
    "# Install required packages\n",
    "required_packages = ['pydantic', 'google-generativeai', 'google-cloud-texttospeech', 'pydub']\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"‚úÖ {package} available\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        !pip install {package}\n",
    "        print(f\"‚úÖ {package} installed\")\n",
    "\n",
    "print(\"\\nüéØ All dependencies ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä LOADING SIMILARITY SEARCH RESULTS\n",
      "==================================================\n",
      "‚úÖ Loaded similarity results:\n",
      "   Total matches: 490\n",
      "   Top matches: 10\n",
      "   Generated: 2025-09-23T23:08:16.220450\n",
      "   CSV data shape: (10, 9)\n",
      "\n",
      "üìã Top 3 Similarity Matches:\n",
      "   1. Similarity: 0.504\n",
      "      Recent: Portohepatic fusion mimics biliary aplasia....\n",
      "      Institute: Acute liver injury as a manifestation of granulomatous hepat...\n",
      "   2. Similarity: 0.486\n",
      "      Recent: Endothelial-Pericyte Interactions Regulate Angiogenesis Via ...\n",
      "      Institute: Early Post-stroke Activation of Vascular Endothelial Growth ...\n",
      "   3. Similarity: 0.466\n",
      "      Recent: Nanozyme eye drops for retinal barrier penetration and vascu...\n",
      "      Institute: The combination of a small molecular prodrug and hyaluronic ...\n",
      "\n",
      "üéØ Ready to generate podcasts from 10 matches!\n"
     ]
    }
   ],
   "source": [
    "# 2. LOAD SIMILARITY SEARCH RESULTS\n",
    "print(\"üìä LOADING SIMILARITY SEARCH RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load the similarity matches from previous ChromaDB search\n",
    "similarity_search_dir = outputs_dir / 'similarity_search'\n",
    "\n",
    "def load_latest_similarity_results():\n",
    "    \"\"\"Load the most recent similarity search results\"\"\"\n",
    "    if not similarity_search_dir.exists():\n",
    "        print(f\"‚ùå Similarity search directory not found: {similarity_search_dir}\")\n",
    "        print(\"   Please run notebook 07_chromadb_similarity_search.ipynb first\")\n",
    "        return None, None\n",
    "    \n",
    "    # Find the latest results file\n",
    "    json_files = list(similarity_search_dir.glob('similarity_matches_*.json'))\n",
    "    if not json_files:\n",
    "        print(f\"‚ùå No similarity results found in {similarity_search_dir}\")\n",
    "        return None, None\n",
    "    \n",
    "    latest_file = max(json_files, key=lambda x: x.stat().st_mtime)\n",
    "    \n",
    "    with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "        similarity_data = json.load(f)\n",
    "    \n",
    "    # Also load CSV for easier manipulation\n",
    "    csv_files = list(similarity_search_dir.glob('top_similarity_matches_*.csv'))\n",
    "    if csv_files:\n",
    "        latest_csv = max(csv_files, key=lambda x: x.stat().st_mtime)\n",
    "        similarity_df = pd.read_csv(latest_csv)\n",
    "    else:\n",
    "        similarity_df = pd.DataFrame()\n",
    "    \n",
    "    return similarity_data, similarity_df\n",
    "\n",
    "# Load results\n",
    "similarity_data, similarity_df = load_latest_similarity_results()\n",
    "\n",
    "if similarity_data:\n",
    "    print(f\"‚úÖ Loaded similarity results:\")\n",
    "    print(f\"   Total matches: {similarity_data['metadata']['total_matches']}\")\n",
    "    print(f\"   Top matches: {len(similarity_data['top_matches'])}\")\n",
    "    print(f\"   Generated: {similarity_data['metadata']['generated_at']}\")\n",
    "    \n",
    "    if not similarity_df.empty:\n",
    "        print(f\"   CSV data shape: {similarity_df.shape}\")\n",
    "        \n",
    "        # Show top matches\n",
    "        print(f\"\\nüìã Top 3 Similarity Matches:\")\n",
    "        for i, row in similarity_df.head(3).iterrows():\n",
    "            print(f\"   {i+1}. Similarity: {row['similarity_score']:.3f}\")\n",
    "            print(f\"      Recent: {row['query_title'][:60]}...\")\n",
    "            print(f\"      Institute: {row['matched_title'][:60]}...\")\n",
    "else:\n",
    "    print(\"‚ùå No similarity results available\")\n",
    "    print(\"   Creating mock data for demonstration...\")\n",
    "    \n",
    "    # Create mock similarity data for testing\n",
    "    similarity_data = {\n",
    "        'metadata': {\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'total_matches': 3,\n",
    "            'top_matches_exported': 3\n",
    "        },\n",
    "        'top_matches': [\n",
    "            {\n",
    "                'rank': 1,\n",
    "                'similarity_score': 0.756,\n",
    "                'recent_pubmed_article': {\n",
    "                    'pmid': '12345678',\n",
    "                    'title': 'Novel mechanisms of neural plasticity in adult hippocampus',\n",
    "                    'journal': 'Nature Neuroscience',\n",
    "                    'abstract': 'Recent advances in neuroimaging have revealed unprecedented insights into adult neurogenesis and synaptic plasticity. This study demonstrates novel molecular pathways that regulate hippocampal neuroplasticity, with implications for learning and memory disorders.'\n",
    "                },\n",
    "                'matched_institute_article': {\n",
    "                    'title': 'Synaptic mechanisms of memory consolidation',\n",
    "                    'journal': 'Cell',\n",
    "                    'year': 2022,\n",
    "                    'source_type': 'IFC',\n",
    "                    'authors': 'Smith J, Johnson K, Williams M'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'rank': 2,\n",
    "                'similarity_score': 0.689,\n",
    "                'recent_pubmed_article': {\n",
    "                    'pmid': '87654321',\n",
    "                    'title': 'CRISPR-mediated gene therapy for inherited cardiac diseases',\n",
    "                    'journal': 'Science Translational Medicine',\n",
    "                    'abstract': 'Gene editing technologies offer new therapeutic approaches for inherited cardiovascular diseases. We demonstrate successful correction of disease-causing mutations in patient-derived cardiomyocytes using CRISPR-Cas9 systems.'\n",
    "                },\n",
    "                'matched_institute_article': {\n",
    "                    'title': 'Genetic basis of cardiomyopathy syndromes',\n",
    "                    'journal': 'Circulation',\n",
    "                    'year': 2021,\n",
    "                    'source_type': 'IFC',\n",
    "                    'authors': 'Brown A, Davis R, Miller T'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create corresponding DataFrame\n",
    "    similarity_df = pd.DataFrame([\n",
    "        {\n",
    "            'similarity_score': match['similarity_score'],\n",
    "            'query_pmid': match['recent_pubmed_article']['pmid'],\n",
    "            'query_title': match['recent_pubmed_article']['title'],\n",
    "            'query_journal': match['recent_pubmed_article']['journal'],\n",
    "            'matched_title': match['matched_institute_article']['title'],\n",
    "            'matched_journal': match['matched_institute_article']['journal'],\n",
    "            'matched_year': match['matched_institute_article']['year'],\n",
    "            'matched_source': match['matched_institute_article']['source_type']\n",
    "        }\n",
    "        for match in similarity_data['top_matches']\n",
    "    ])\n",
    "    \n",
    "    print(f\"‚úÖ Created mock similarity data for testing\")\n",
    "\n",
    "print(f\"\\nüéØ Ready to generate podcasts from {len(similarity_data['top_matches'])} matches!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "VSC-3. API SETUP",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë SETTING UP API PROVIDERS\n",
      "==================================================\n",
      "‚úÖ Gemini API configured successfully\n",
      "üîß Setting up Google TTS client...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Google TTS client created successfully\n",
      "üéôÔ∏è Using TTS model: gemini-2.5-flash-preview-tts\n",
      "\n",
      "üéØ API Status:\n",
      "   Gemini: ‚úÖ Ready\n",
      "   Text-to-Speech: ‚úÖ Ready\n"
     ]
    }
   ],
   "source": [
    "from utils.config import load_config\n",
    "config = load_config()\n",
    "\n",
    "# 3. API SETUP AND PROVIDERS\n",
    "print(\"üîë SETTING UP API PROVIDERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import google.generativeai as genai\n",
    "from google.cloud import texttospeech\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Setup Google Gemini API\n",
    "def setup_gemini_api():\n",
    "    \"\"\"Setup Gemini API for text generation\"\"\"\n",
    "    try:\n",
    "        # Try to get API key from environment\n",
    "        api_key = os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')\n",
    "        if not api_key:\n",
    "            print(\"‚ö†Ô∏è No API key found. Using mock provider for testing.\")\n",
    "            return None\n",
    "        \n",
    "        genai.configure(api_key=api_key)\n",
    "        model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "        print(\"‚úÖ Gemini API configured successfully\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Gemini API setup failed: {e}. Using mock provider.\")\n",
    "        return None\n",
    "\n",
    "# Setup Google Cloud Text-to-Speech\n",
    "def setup_tts_client():\n",
    "    \"\"\"Setup Google Cloud Text-to-Speech client using the correct API.\"\"\"\n",
    "    print(\"üîß Setting up Google TTS client...\")\n",
    "    try:\n",
    "        from google import genai\n",
    "        from google.genai import types\n",
    "\n",
    "        # Retrieve the TTS API key from the config file\n",
    "        api_key = config['api_keys'].get('google_tts', '')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"Google TTS API key is missing in the config file.\")\n",
    "\n",
    "        # Initialize the TTS client\n",
    "        tts_client = genai.Client(api_key=api_key)\n",
    "        print(\"‚úÖ Google TTS client created successfully\")\n",
    "\n",
    "        # Test the TTS model to ensure it's accessible\n",
    "        tts_model = config['audio']['model']\n",
    "        print(f\"üéôÔ∏è Using TTS model: {tts_model}\")\n",
    "        return tts_client\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå ImportError: {e}\")\n",
    "        print(\"   Ensure the `google-genai` package is installed and up-to-date.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Google TTS client setup failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Initialize providers\n",
    "gemini_model = setup_gemini_api()\n",
    "tts_client = setup_tts_client()\n",
    "\n",
    "print(f\"\\nüéØ API Status:\")\n",
    "print(f\"   Gemini: {'‚úÖ Ready' if gemini_model else 'üîß Mock mode'}\")\n",
    "print(f\"   Text-to-Speech: {'‚úÖ Ready' if tts_client else 'üîß Mock mode'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "VSC-4. STRUCTURED SCRIPT MODELS",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù DEFINING STRUCTURED SCRIPT MODELS\n",
      "==================================================\n",
      "‚úÖ Structured script models defined\n",
      "   Sections: 8 required fields\n",
      "   Validation: Automatic length and content validation\n",
      "   Output: Consistent, high-quality scientific narratives\n"
     ]
    }
   ],
   "source": [
    "# 4. STRUCTURED SCRIPT GENERATION WITH PYDANTIC\n",
    "print(\"üìù DEFINING STRUCTURED SCRIPT MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class PodcastScriptStructure(BaseModel):\n",
    "    \"\"\"Structured output schema for scientific podcast scripts\"\"\"\n",
    "    \n",
    "    podcast_title: str = Field(\n",
    "        description=\"Engaging, accessible title for the podcast episode\",\n",
    "        min_length=10,\n",
    "        max_length=100\n",
    "    )\n",
    "    \n",
    "    introduction: str = Field(\n",
    "        description=\"Hook to grab listener attention, introducing the research topic and importance\",\n",
    "        min_length=100,\n",
    "        max_length=500\n",
    "    )\n",
    "    \n",
    "    research_context: str = Field(\n",
    "        description=\"Background on the research field and why this work matters\",\n",
    "        min_length=100,\n",
    "        max_length=400\n",
    "    )\n",
    "    \n",
    "    methods_summary: str = Field(\n",
    "        description=\"Simplified explanation of key research methods, avoiding jargon\",\n",
    "        min_length=50,\n",
    "        max_length=300\n",
    "    )\n",
    "    \n",
    "    key_findings: List[str] = Field(\n",
    "        description=\"List of 2-4 main results or discoveries, explained clearly\",\n",
    "        min_items=2,\n",
    "        max_items=4\n",
    "    )\n",
    "    \n",
    "    institute_connection: str = Field(\n",
    "        description=\"How this research connects to your institute's work\",\n",
    "        min_length=50,\n",
    "        max_length=300\n",
    "    )\n",
    "    \n",
    "    implications_and_significance: str = Field(\n",
    "        description=\"Why these findings matter for science and the public\",\n",
    "        min_length=100,\n",
    "        max_length=400\n",
    "    )\n",
    "    \n",
    "    conclusion: str = Field(\n",
    "        description=\"Summary and concluding thought to leave listeners with\",\n",
    "        min_length=50,\n",
    "        max_length=200\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Structured script models defined\")\n",
    "print(f\"   Sections: {len(PodcastScriptStructure.model_fields)} required fields\")\n",
    "print(f\"   Validation: Automatic length and content validation\")\n",
    "print(f\"   Output: Consistent, high-quality scientific narratives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d91d561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary prompts\n",
    "def build_aggregate_podcast_prompt(matches: List[Dict]) -> str:\n",
    "    prompt = (\n",
    "        \"You are an expert science communicator. \"\n",
    "        \"Create a single podcast episode that summarizes and compares the following recent research breakthroughs and their connections to our institute's work. \"\n",
    "        \"Highlight key findings, similarities, differences, and implications for the field. Use an engaging, accessible tone suitable for audio.\\n\\n\"\n",
    "    )\n",
    "    for i, match in enumerate(matches, 1):\n",
    "        recent = match['recent_pubmed_article']\n",
    "        institute = match['matched_institute_article']\n",
    "        prompt += (\n",
    "            f\"Match #{i} (Similarity: {match['similarity_score']:.3f}):\\n\"\n",
    "            f\"Recent Research: {recent['title']} ({recent['journal']}, PMID: {recent['pmid']})\\n\"\n",
    "            f\"Institute Work: {institute['title']} ({institute['journal']}, {institute['year']})\\n\\n\"\n",
    "        )\n",
    "    prompt += (\n",
    "        \"Create a podcast script using the PodcastScriptStructure schema. \"\n",
    "        \"Focus on synthesizing the overall themes and connections across all matches.\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "VSC-5. SCRIPT GENERATOR",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† CREATING INTELLIGENT SCRIPT GENERATOR\n",
      "==================================================\n",
      "‚úÖ Script generator initialized\n",
      "   Mode: ü§ñ AI-powered\n",
      "   Output: Structured, validated podcast scripts\n"
     ]
    }
   ],
   "source": [
    "# 5. INTELLIGENT SCRIPT GENERATOR\n",
    "import asyncio # enforce delays in requests\n",
    "\n",
    "print(\"üß† CREATING INTELLIGENT SCRIPT GENERATOR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class PodcastScriptGenerator:\n",
    "    \"\"\"Generate structured podcast scripts from similarity matches.\"\"\"\n",
    "    \n",
    "    def __init__(self, model=None, rate_limit: Dict[str, int] = None):\n",
    "        self.model = model\n",
    "        self.use_mock = model is None\n",
    "        self.rate_limit = rate_limit or {'rpm': 10, 'rpd': 250}\n",
    "        self.requests_made = 0\n",
    "        self.start_time = datetime.now()\n",
    "\n",
    "    async def enforce_rate_limit(self):\n",
    "        \"\"\"Enforce rate limits based on RPM and RPD.\"\"\"\n",
    "        # Calculate time since start\n",
    "        elapsed_time = (datetime.now() - self.start_time).total_seconds()\n",
    "\n",
    "        # Enforce RPM (Requests per Minute)\n",
    "        if self.requests_made >= self.rate_limit['rpm']:\n",
    "            await asyncio.sleep(60 - (elapsed_time % 60))  # Wait for the next minute\n",
    "            self.requests_made = 0  # Reset counter\n",
    "\n",
    "        # Enforce RPD (Requests per Day)\n",
    "        if self.requests_made >= self.rate_limit['rpd']:\n",
    "            raise RuntimeError(\"Daily request limit exceeded.\")\n",
    "\n",
    "\n",
    "    \n",
    "    async def generate_script(self, similarity_match: Dict) -> PodcastScriptStructure:\n",
    "        await self.enforce_rate_limit()\n",
    "        self.requests_made += 1\n",
    "    \n",
    "        # Use aggregate prompt if present\n",
    "        if 'aggregate_prompt' in similarity_match:\n",
    "            prompt = similarity_match['aggregate_prompt']\n",
    "        else:\n",
    "            prompt = self._build_generation_prompt(\n",
    "                similarity_match.get('recent_pubmed_article', {}),\n",
    "                similarity_match.get('matched_institute_article', {}),\n",
    "                similarity_match.get('similarity_score', 0)\n",
    "            )\n",
    "    \n",
    "        try:\n",
    "            raw_schema = PodcastScriptStructure.model_json_schema()\n",
    "            json_schema = {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": raw_schema[\"properties\"],\n",
    "                \"required\": raw_schema.get(\"required\", [])\n",
    "            }\n",
    "            response = await self.model.generate_content_async(\n",
    "                prompt,\n",
    "                generation_config=genai.GenerationConfig(\n",
    "                    response_mime_type=\"application/json\",\n",
    "                    response_schema=json_schema\n",
    "                )\n",
    "            )\n",
    "            script_data = json.loads(response.text)\n",
    "            return PodcastScriptStructure.model_validate(script_data)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Gemini generation failed: {e}. Using mock script.\")\n",
    "            return self._generate_mock_script(\n",
    "                similarity_match.get('recent_pubmed_article', {}),\n",
    "                similarity_match.get('matched_institute_article', {}),\n",
    "                similarity_match.get('similarity_score', 0)\n",
    "            )\n",
    "    \n",
    "    def _build_generation_prompt(self, recent_article: Dict, institute_article: Dict, similarity_score: float) -> str:\n",
    "        \"\"\"Build the generation prompt for Gemini\"\"\"\n",
    "        return f\"\"\"\n",
    "You are an expert science communicator creating a podcast script about cutting-edge research.\n",
    "\n",
    "RECENT RESEARCH (from PubMed):\n",
    "Title: {recent_article['title']}\n",
    "Journal: {recent_article['journal']}\n",
    "Abstract: {recent_article.get('abstract', 'Abstract not available')}\n",
    "PMID: {recent_article['pmid']}\n",
    "\n",
    "RELATED INSTITUTE WORK:\n",
    "Title: {institute_article['title']}\n",
    "Journal: {institute_article['journal']}\n",
    "Year: {institute_article['year']}\n",
    "Authors: {institute_article.get('authors', 'Authors not available')}\n",
    "\n",
    "SIMILARITY SCORE: {similarity_score:.3f} (indicates strong thematic connection)\n",
    "\n",
    "Create an engaging podcast script that:\n",
    "1. Makes complex science accessible to a general audience\n",
    "2. Highlights the connection between recent research and institute work\n",
    "3. Explains the significance and real-world implications\n",
    "4. Uses conversational, engaging tone suitable for audio\n",
    "5. Includes natural transitions between sections\n",
    "\n",
    "Return the response as JSON matching the PodcastScriptStructure schema.\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate_mock_script(self, recent_article: Dict, institute_article: Dict, similarity_score: float) -> PodcastScriptStructure:\n",
    "        title = recent_article.get('title', 'Research Breakthrough')\n",
    "        journal = recent_article.get('journal', 'Science Journal')\n",
    "        institute_title = institute_article.get('title', 'Institute Research')\n",
    "        institute_journal = institute_article.get('journal', 'Institute Journal')\n",
    "        institute_year = institute_article.get('year', '2025')\n",
    "    \n",
    "        field_keywords = {\n",
    "            'neural': 'neuroscience',\n",
    "            'brain': 'neuroscience',\n",
    "            'cancer': 'oncology',\n",
    "            'tumor': 'oncology',\n",
    "            'immune': 'immunology',\n",
    "            'gene': 'genetics',\n",
    "            'heart': 'cardiology'\n",
    "        }\n",
    "        field = 'biomedical research'\n",
    "        for keyword, detected_field in field_keywords.items():\n",
    "            if keyword in title.lower():\n",
    "                field = detected_field\n",
    "                break\n",
    "            \n",
    "        return PodcastScriptStructure(\n",
    "            podcast_title=f\"Breakthrough in {field.title()}: {title[:40]}...\",\n",
    "            introduction=f\"Welcome to Research Frontiers, exploring breakthroughs in {field}. Today, we dive into research from {journal} that could transform treatments.\",\n",
    "            research_context=f\"The field of {field} is evolving rapidly. This study represents a key advance in understanding disease mechanisms.\",\n",
    "            methods_summary=f\"Researchers used advanced techniques to investigate cellular processes, revealing new insights.\",\n",
    "            key_findings=[\n",
    "                \"Novel mechanisms for therapeutic targets were identified.\",\n",
    "                \"Key pathways involved in disease were uncovered.\",\n",
    "                \"Findings suggest potential for new treatments.\"\n",
    "            ],\n",
    "            institute_connection=f\"This connects to our institute's work on '{institute_title[:50]}...', building on prior research.\",\n",
    "            implications_and_significance=\"These findings could lead to better treatments and personalized medicine approaches.\",\n",
    "            conclusion=\"This shows the power of collaboration. We're optimistic about future breakthroughs.\"\n",
    "        )\n",
    "\n",
    "# Load rate limits from config\n",
    "gemini_rate_limit = config['api_limits'].get('gemini', {})\n",
    "\n",
    "# Initialize components with rate limits\n",
    "script_generator = PodcastScriptGenerator(gemini_model, rate_limit=gemini_rate_limit)\n",
    "\n",
    "print(f\"‚úÖ Script generator initialized\")\n",
    "print(f\"   Mode: {'ü§ñ AI-powered' if not script_generator.use_mock else 'üîß Mock generation'}\")\n",
    "print(f\"   Output: Structured, validated podcast scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "VSC-6. VOICE SYNTHESIS",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéôÔ∏è SETTING UP VOICE SYNTHESIS\n",
      "==================================================\n",
      "‚úÖ Voice synthesizer initialized\n",
      "   Mode: üéôÔ∏è Google TTS\n",
      "   Voice: en-US-Studio-M (Professional male)\n",
      "   Output: High-quality MP3 audio files\n"
     ]
    }
   ],
   "source": [
    "# 6. VOICE SYNTHESIS SYSTEM\n",
    "print(\"üéôÔ∏è SETTING UP VOICE SYNTHESIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import base64\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "\n",
    "class VoiceSynthesizer:\n",
    "    \"\"\"Handle text-to-speech conversion for podcast generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, tts_client=None, rate_limit: Dict[str, int] = None):\n",
    "        self.client = tts_client\n",
    "        self.use_mock = tts_client is None\n",
    "        self.rate_limit = rate_limit or {'rpm': 3, 'rpd': 15}\n",
    "        self.requests_made = 0\n",
    "        self.start_time = datetime.now()\n",
    "\n",
    "    async def enforce_rate_limit(self):\n",
    "        \"\"\"Enforce rate limits for TTS requests.\"\"\"\n",
    "        elapsed_time = (datetime.now() - self.start_time).total_seconds()\n",
    "\n",
    "        # Enforce RPM (Requests Per Minute)\n",
    "        if self.requests_made >= self.rate_limit['rpm']:\n",
    "            await asyncio.sleep(60 - (elapsed_time % 60))\n",
    "            self.requests_made = 0\n",
    "\n",
    "        # Enforce RPD (Requests Per Day)\n",
    "        if self.requests_made >= self.rate_limit['rpd']:\n",
    "            raise RuntimeError(\"Daily TTS request limit exceeded.\")\n",
    "\n",
    "    def script_to_ssml(self, script: PodcastScriptStructure) -> str:\n",
    "        \"\"\"Convert structured script to SSML for better speech synthesis\"\"\"\n",
    "        \n",
    "        ssml_parts = [\n",
    "            '<speak>',\n",
    "            \n",
    "            # Title with emphasis\n",
    "            f'<emphasis level=\"strong\">{script.podcast_title}</emphasis>',\n",
    "            '<break time=\"2s\"/>',\n",
    "            \n",
    "            # Introduction\n",
    "            script.introduction,\n",
    "            '<break time=\"1s\"/>',\n",
    "            \n",
    "            # Research context\n",
    "            script.research_context,\n",
    "            '<break time=\"1s\"/>',\n",
    "            \n",
    "            # Methods\n",
    "            'Now, let me explain how the researchers approached this problem.',\n",
    "            '<break time=\"0.5s\"/>',\n",
    "            script.methods_summary,\n",
    "            '<break time=\"1s\"/>',\n",
    "            \n",
    "            # Key findings\n",
    "            'So what did they discover? Here are the key findings:',\n",
    "            '<break time=\"0.5s\"/>'\n",
    "        ]\n",
    "        \n",
    "        # Add findings with pauses\n",
    "        for i, finding in enumerate(script.key_findings, 1):\n",
    "            ssml_parts.extend([\n",
    "                f'First, {finding}' if i == 1 else f'Second, {finding}' if i == 2 else f'Third, {finding}' if i == 3 else f'Finally, {finding}',\n",
    "                '<break time=\"0.8s\"/>'\n",
    "            ])\n",
    "        \n",
    "        ssml_parts.extend([\n",
    "            # Institute connection\n",
    "            script.institute_connection,\n",
    "            '<break time=\"1s\"/>',\n",
    "            \n",
    "            # Implications\n",
    "            script.implications_and_significance,\n",
    "            '<break time=\"1s\"/>',\n",
    "            \n",
    "            # Conclusion\n",
    "            script.conclusion,\n",
    "            '<break time=\"1s\"/>',\n",
    "            \n",
    "            'Thank you for listening to Research Frontiers.',\n",
    "            '</speak>'\n",
    "        ])\n",
    "        \n",
    "        return ' '.join(ssml_parts)\n",
    "    \n",
    "    async def synthesize_speech(self, script: PodcastScriptStructure, output_path: Path) -> bool:\n",
    "        \"\"\"Convert script to audio file.\"\"\"\n",
    "        await self.enforce_rate_limit()  # Enforce rate limit before making a request\n",
    "        self.requests_made += 1\n",
    "\n",
    "        if self.use_mock:\n",
    "            return self._create_mock_audio(script, output_path)\n",
    "        \n",
    "        try:\n",
    "            # Convert to SSML\n",
    "            ssml_text = self.script_to_ssml(script)\n",
    "            \n",
    "            # Synthesize speech\n",
    "            synthesis_input = texttospeech.SynthesisInput(ssml=ssml_text)\n",
    "            \n",
    "            response = self.client.synthesize_speech(\n",
    "                input=synthesis_input,\n",
    "                voice=self.voice_config,\n",
    "                audio_config=self.audio_config\n",
    "            )\n",
    "            \n",
    "            # Save audio file\n",
    "            with open(output_path, \"wb\") as out:\n",
    "                out.write(response.audio_content)\n",
    "            \n",
    "            print(f\"‚úÖ Audio synthesized: {output_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Speech synthesis failed: {e}. Creating mock audio.\")\n",
    "            return self._create_mock_audio(script, output_path)\n",
    "    \n",
    "    def _create_mock_audio(self, script: PodcastScriptStructure, output_path: Path) -> bool:\n",
    "        \"\"\"Create mock audio file for testing\"\"\"\n",
    "        try:\n",
    "            # Create a simple tone as placeholder\n",
    "            # Duration based on script length\n",
    "            script_text = f\"{script.introduction} {script.research_context} {script.methods_summary} {' '.join(script.key_findings)} {script.institute_connection} {script.implications_and_significance} {script.conclusion}\"\n",
    "            \n",
    "            # Estimate duration (assume ~150 words per minute)\n",
    "            word_count = len(script_text.split())\n",
    "            duration_minutes = max(2, word_count / 150)  # At least 2 minutes\n",
    "            duration_ms = int(duration_minutes * 60 * 1000)\n",
    "            \n",
    "            # Generate a simple tone\n",
    "            tone = AudioSegment.silent(duration=duration_ms)\n",
    "            \n",
    "            # Add some variation (simple sine wave)\n",
    "            from math import sin, pi\n",
    "            import array\n",
    "            \n",
    "            sample_rate = 44100\n",
    "            samples = []\n",
    "            \n",
    "            for i in range(int(sample_rate * duration_minutes * 60)):\n",
    "                # Mix of frequencies to simulate speech\n",
    "                t = i / sample_rate\n",
    "                sample = int(32767 * 0.1 * (sin(2 * pi * 200 * t) + 0.5 * sin(2 * pi * 400 * t)))\n",
    "                samples.append(sample)\n",
    "            \n",
    "            # Convert to audio\n",
    "            audio_array = array.array('h', samples)\n",
    "            audio = AudioSegment(\n",
    "                audio_array.tobytes(),\n",
    "                frame_rate=sample_rate,\n",
    "                sample_width=2,\n",
    "                channels=1\n",
    "            )\n",
    "            \n",
    "            # Export as MP3\n",
    "            audio.export(output_path, format=\"mp3\")\n",
    "            \n",
    "            print(f\"‚úÖ Mock audio created: {output_path} ({duration_minutes:.1f} min)\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Mock audio creation failed: {e}\")\n",
    "            return False\n",
    "\n",
    "# Initialize voice synthesizer\n",
    "tts_rate_limit = config['api_limits'].get('tts', {})\n",
    "voice_synthesizer = VoiceSynthesizer(tts_client, rate_limit=tts_rate_limit)\n",
    "\n",
    "print(f\"‚úÖ Voice synthesizer initialized\")\n",
    "print(f\"   Mode: {'üéôÔ∏è Google TTS' if not voice_synthesizer.use_mock else 'üîß Mock audio'}\")\n",
    "print(f\"   Voice: en-US-Studio-M (Professional male)\")\n",
    "print(f\"   Output: High-quality MP3 audio files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "VSC-7. COMPLETE PIPELINE",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ ASSEMBLING COMPLETE PIPELINE\n",
      "==================================================\n",
      "‚úÖ Complete podcast pipeline assembled\n",
      "   Components: Script generation + Voice synthesis\n",
      "   Output: Complete podcast episodes with audio\n",
      "   Ready to process 10 similarity matches\n"
     ]
    }
   ],
   "source": [
    "# 7. COMPLETE PODCAST GENERATION PIPELINE\n",
    "print(\"üéØ ASSEMBLING COMPLETE PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class CompletePodcastPipeline:\n",
    "    \"\"\"Complete pipeline for generating podcasts from similarity matches\"\"\"\n",
    "    \n",
    "    def __init__(self, script_generator: PodcastScriptGenerator, voice_synthesizer: VoiceSynthesizer):\n",
    "        self.script_generator = script_generator\n",
    "        self.voice_synthesizer = voice_synthesizer\n",
    "        self.generated_podcasts = []\n",
    "    \n",
    "    async def generate_podcast_episode(self, similarity_match: Dict, episode_number: int) -> Dict:\n",
    "        \"\"\"Generate complete podcast episode from similarity match\"\"\"\n",
    "        \n",
    "        print(f\"\\nüéôÔ∏è Generating Podcast Episode {episode_number}\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        episode_data = {\n",
    "            'episode_number': episode_number,\n",
    "            'similarity_match': similarity_match,\n",
    "            'generation_timestamp': datetime.now().isoformat(),\n",
    "            'status': 'processing',\n",
    "            'files_generated': {},\n",
    "            'metadata': {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Generate structured script\n",
    "            print(\"üìù Step 1: Generating structured script...\")\n",
    "            script = await self.script_generator.generate_script(similarity_match)\n",
    "            episode_data['script'] = script.model_dump()\n",
    "            \n",
    "            print(f\"   ‚úÖ Script generated: '{script.podcast_title}'\")\n",
    "            print(f\"   üìä Sections: {len(script.key_findings)} findings, {len(script.model_dump_json().split())} words\")\n",
    "            \n",
    "            # Step 2: Save script files\n",
    "            print(\"üíæ Step 2: Saving script files...\")\n",
    "            \n",
    "            # Create episode directory\n",
    "            episode_dir = podcast_output_dir / f\"episode_{episode_number:03d}\"\n",
    "            episode_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Save structured script as JSON\n",
    "            script_json_path = episode_dir / \"script_structured.json\"\n",
    "            with open(script_json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(script.model_dump(), f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Save readable script as markdown\n",
    "            script_md_path = episode_dir / \"script_readable.md\"\n",
    "            readable_script = self._format_script_for_reading(script)\n",
    "            with open(script_md_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(readable_script)\n",
    "            \n",
    "            episode_data['files_generated']['script_json'] = str(script_json_path)\n",
    "            episode_data['files_generated']['script_markdown'] = str(script_md_path)\n",
    "            \n",
    "            print(f\"   ‚úÖ Scripts saved to: {episode_dir}\")\n",
    "            \n",
    "            # Step 3: Generate audio\n",
    "            print(\"üéôÔ∏è Step 3: Synthesizing speech...\")\n",
    "            \n",
    "            audio_path = episode_dir / \"podcast_audio.mp3\"\n",
    "            audio_success = await self.voice_synthesizer.synthesize_speech(script, audio_path)\n",
    "            \n",
    "            if audio_success:\n",
    "                episode_data['files_generated']['audio_mp3'] = str(audio_path)\n",
    "                \n",
    "                # Get audio duration if possible\n",
    "                try:\n",
    "                    audio = AudioSegment.from_mp3(audio_path)\n",
    "                    duration_minutes = len(audio) / 60000\n",
    "                    episode_data['metadata']['duration_minutes'] = round(duration_minutes, 2)\n",
    "                    print(f\"   ‚úÖ Audio generated: {duration_minutes:.1f} minutes\")\n",
    "                except:\n",
    "                    print(f\"   ‚úÖ Audio file created: {audio_path}\")\n",
    "            \n",
    "            # Step 4: Generate episode metadata\n",
    "            print(\"üìã Step 4: Creating episode metadata...\")\n",
    "            \n",
    "            metadata = {\n",
    "                'episode_number': episode_number,\n",
    "                'title': script.podcast_title,\n",
    "                'description': script.introduction[:200] + \"...\",\n",
    "                'recent_article': {\n",
    "                    'title': similarity_match['recent_pubmed_article']['title'],\n",
    "                    'journal': similarity_match['recent_pubmed_article']['journal'],\n",
    "                    'pmid': similarity_match['recent_pubmed_article']['pmid']\n",
    "                },\n",
    "                'institute_connection': {\n",
    "                    'title': similarity_match['matched_institute_article']['title'],\n",
    "                    'journal': similarity_match['matched_institute_article']['journal'],\n",
    "                    'year': similarity_match['matched_institute_article']['year']\n",
    "                },\n",
    "                'similarity_score': similarity_match['similarity_score'],\n",
    "                'generation_date': datetime.now().isoformat(),\n",
    "                'files': episode_data['files_generated']\n",
    "            }\n",
    "            \n",
    "            # Save episode metadata\n",
    "            metadata_path = episode_dir / \"episode_metadata.json\"\n",
    "            with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            episode_data['files_generated']['metadata'] = str(metadata_path)\n",
    "            episode_data['metadata'].update(metadata)\n",
    "            \n",
    "            print(f\"   ‚úÖ Metadata saved: {metadata_path}\")\n",
    "            \n",
    "            episode_data['status'] = 'completed'\n",
    "            \n",
    "            print(f\"\\nüéâ Episode {episode_number} completed successfully!\")\n",
    "            print(f\"   üìÅ Output directory: {episode_dir}\")\n",
    "            print(f\"   üìÑ Files: {len(episode_data['files_generated'])} generated\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            episode_data['status'] = 'error'\n",
    "            episode_data['error'] = str(e)\n",
    "            print(f\"‚ùå Episode {episode_number} generation failed: {e}\")\n",
    "        \n",
    "        return episode_data\n",
    "    \n",
    "    def _format_script_for_reading(self, script: PodcastScriptStructure) -> str:\n",
    "        \"\"\"Format structured script for human reading\"\"\"\n",
    "        \n",
    "        return f\"\"\"# {script.podcast_title}\n",
    "\n",
    "## Introduction\n",
    "{script.introduction}\n",
    "\n",
    "## Research Context\n",
    "{script.research_context}\n",
    "\n",
    "## Methods Summary\n",
    "{script.methods_summary}\n",
    "\n",
    "## Key Findings\n",
    "{chr(10).join(f\"{i+1}. {finding}\" for i, finding in enumerate(script.key_findings))}\n",
    "\n",
    "## Institute Connection\n",
    "{script.institute_connection}\n",
    "\n",
    "## Implications and Significance\n",
    "{script.implications_and_significance}\n",
    "\n",
    "## Conclusion\n",
    "{script.conclusion}\n",
    "\n",
    "---\n",
    "*Generated by UBMI-IFC Podcast Pipeline*\n",
    "\"\"\"\n",
    "    \n",
    "    async def generate_all_episodes(self, max_episodes: int = None) -> List[Dict]:\n",
    "        \"\"\"Generate podcast episodes for all similarity matches.\"\"\"\n",
    "        max_episodes = min(max_episodes or len(similarity_data['top_matches']), \n",
    "                            self.script_generator.rate_limit['rpd'], \n",
    "                            self.voice_synthesizer.rate_limit['rpd'])\n",
    "    \n",
    "        print(f\"\\nüöÄ GENERATING {max_episodes} PODCAST EPISODES\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "        episodes = []\n",
    "        for i, match in enumerate(similarity_data['top_matches'][:max_episodes], 1):\n",
    "            episode_data = await self.generate_podcast_episode(match, i)\n",
    "            episodes.append(episode_data)\n",
    "            self.generated_podcasts.append(episode_data)\n",
    "    \n",
    "        return episodes\n",
    "        \n",
    "        # # Generate series metadata\n",
    "        # series_metadata = {\n",
    "        #     'series_title': 'UBMI-IFC Research Frontiers',\n",
    "        #     'description': 'Exploring cutting-edge research and its connections to institute work',\n",
    "        #     'total_episodes': len(episodes),\n",
    "        #     'generation_date': datetime.now().isoformat(),\n",
    "        #     'episodes': [\n",
    "        #         {\n",
    "        #             'episode_number': ep['episode_number'],\n",
    "        #             'title': ep.get('metadata', {}).get('title', 'Unknown'),\n",
    "        #             'status': ep['status'],\n",
    "        #             'similarity_score': ep.get('metadata', {}).get('similarity_score', 0)\n",
    "        #         } for ep in episodes\n",
    "        #     ]\n",
    "        # }\n",
    "        \n",
    "        # # Save series metadata\n",
    "        # series_metadata_path = podcast_output_dir / \"series_metadata.json\"\n",
    "        # with open(series_metadata_path, 'w', encoding='utf-8') as f:\n",
    "        #     json.dump(series_metadata, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # print(f\"\\nüìä PIPELINE SUMMARY\")\n",
    "        # print(\"=\" * 30)\n",
    "        # successful_episodes = sum(1 for ep in episodes if ep['status'] == 'completed')\n",
    "        # print(f\"‚úÖ Episodes completed: {successful_episodes}/{len(episodes)}\")\n",
    "        # print(f\"üìÅ Output directory: {podcast_output_dir}\")\n",
    "        # print(f\"üíæ Series metadata: {series_metadata_path}\")\n",
    "        \n",
    "        # return episodes\n",
    "\n",
    "# Initialize complete pipeline\n",
    "complete_pipeline = CompletePodcastPipeline(script_generator, voice_synthesizer)\n",
    "\n",
    "print(\"‚úÖ Complete podcast pipeline assembled\")\n",
    "print(f\"   Components: Script generation + Voice synthesis\")\n",
    "print(f\"   Output: Complete podcast episodes with audio\")\n",
    "print(f\"   Ready to process {len(similarity_data['top_matches'])} similarity matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "VSC-8. GENERATE PODCASTS",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ STARTING AGGREGATE PODCAST GENERATION\n",
      "==================================================\n",
      "‚ö†Ô∏è Gemini generation failed: Unknown field for Schema: maxLength. Using mock script.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for PodcastScriptStructure\nimplications_and_significance\n  String should have at least 100 characters [type=string_too_short, input_value='These findings could lea...ed medicine approaches.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_too_short",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 44\u001b[0m\n\u001b[1;32m     38\u001b[0m aggregate_match \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maggregate_prompt\u001b[39m\u001b[38;5;124m'\u001b[39m: aggregate_prompt,\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity_score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean([m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity_score\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m matches])) \u001b[38;5;28;01mif\u001b[39;00m matches \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     41\u001b[0m }\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Generate the aggregated script (async)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m script \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m script_generator\u001b[38;5;241m.\u001b[39mgenerate_script(aggregate_match)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Save and synthesize as before\u001b[39;00m\n\u001b[1;32m     47\u001b[0m episode_dir \u001b[38;5;241m=\u001b[39m podcast_output_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_aggregate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[36], line 65\u001b[0m, in \u001b[0;36mPodcastScriptGenerator.generate_script\u001b[0;34m(self, similarity_match)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ö†Ô∏è Gemini generation failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Using mock script.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_mock_script\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_match\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecent_pubmed_article\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_match\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmatched_institute_article\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimilarity_match\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msimilarity_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 122\u001b[0m, in \u001b[0;36mPodcastScriptGenerator._generate_mock_script\u001b[0;34m(self, recent_article, institute_article, similarity_score)\u001b[0m\n\u001b[1;32m    119\u001b[0m         field \u001b[38;5;241m=\u001b[39m detected_field\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPodcastScriptStructure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpodcast_title\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBreakthrough in \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtitle\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintroduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWelcome to Research Frontiers, exploring breakthroughs in \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfield\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m. Today, we dive into research from \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mjournal\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m that could transform treatments.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresearch_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe field of \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfield\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m is evolving rapidly. This study represents a key advance in understanding disease mechanisms.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethods_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResearchers used advanced techniques to investigate cellular processes, revealing new insights.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_findings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNovel mechanisms for therapeutic targets were identified.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKey pathways involved in disease were uncovered.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFindings suggest potential for new treatments.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstitute_connection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThis connects to our institute\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms work on \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minstitute_title\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, building on prior research.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimplications_and_significance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThese findings could lead to better treatments and personalized medicine approaches.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconclusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThis shows the power of collaboration. We\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mre optimistic about future breakthroughs.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/pydantic/main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    252\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    260\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for PodcastScriptStructure\nimplications_and_significance\n  String should have at least 100 characters [type=string_too_short, input_value='These findings could lea...ed medicine approaches.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_too_short"
     ]
    }
   ],
   "source": [
    "# 8. GENERATE AGGREGATE PODCAST EPISODE (REPLACEMENT)\n",
    "print(\"üé¨ STARTING AGGREGATE PODCAST GENERATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Limit how many matches to include in the aggregate to avoid too-long prompts\n",
    "max_matches = config.get('pipeline', {}).get('aggregate_max_matches', 5)\n",
    "matches = similarity_data['top_matches'][:max_matches]\n",
    "\n",
    "# Helper: truncate long fields to keep prompt short\n",
    "def truncate(text: str, max_chars: int = 300) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    txt = text.replace(\"\\n\", \" \").strip()\n",
    "    return txt if len(txt) <= max_chars else txt[:max_chars].rstrip() + \"...\"\n",
    "\n",
    "# Build a concise aggregate prompt\n",
    "def build_compact_aggregate_prompt(matches):\n",
    "    prompt_lines = [\n",
    "        \"You are an expert science communicator. Create a single podcast episode that synthesizes the following recent research and their connections to our institute's work.\",\n",
    "        \"Be concise and audio-friendly. Return output following the PodcastScriptStructure schema.\"\n",
    "    ]\n",
    "    for i, match in enumerate(matches, 1):\n",
    "        recent = match.get('recent_pubmed_article', {})\n",
    "        institute = match.get('matched_institute_article', {})\n",
    "        prompt_lines.append(\n",
    "            f\"Match #{i} (sim={match.get('similarity_score',0):.3f}): Recent: {truncate(recent.get('title',''),200)} | Journal: {truncate(recent.get('journal',''),80)} | PMID: {recent.get('pmid','N/A')}\"\n",
    "        )\n",
    "        # include a short abstract snippet\n",
    "        prompt_lines.append(f\"Abstract snippet: {truncate(recent.get('abstract',''),300)}\")\n",
    "        prompt_lines.append(f\"Institute work: {truncate(institute.get('title',''),200)} ({institute.get('year','N/A')})\")\n",
    "        prompt_lines.append(\"\")  # spacer\n",
    "    prompt_lines.append(\"Synthesize overall themes, highlight similarities/differences, and explain implications for the field and for our institute.\")\n",
    "    return \"\\n\".join(prompt_lines)\n",
    "\n",
    "# Build and call generator with a single aggregated prompt\n",
    "aggregate_prompt = build_compact_aggregate_prompt(matches)\n",
    "\n",
    "aggregate_match = {\n",
    "    'aggregate_prompt': aggregate_prompt,\n",
    "    'similarity_score': float(np.mean([m.get('similarity_score',0) for m in matches])) if matches else 0.0\n",
    "}\n",
    "\n",
    "# Generate the aggregated script (async)\n",
    "script = await script_generator.generate_script(aggregate_match)\n",
    "\n",
    "# Save and synthesize as before\n",
    "episode_dir = podcast_output_dir / \"episode_aggregate\"\n",
    "episode_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "script_json_path = episode_dir / \"script_structured.json\"\n",
    "with open(script_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(script.model_dump(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "script_md_path = episode_dir / \"script_readable.md\"\n",
    "readable_script = complete_pipeline._format_script_for_reading(script)\n",
    "with open(script_md_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(readable_script)\n",
    "\n",
    "audio_path = episode_dir / \"podcast_audio.mp3\"\n",
    "audio_success = await voice_synthesizer.synthesize_speech(script, audio_path)\n",
    "\n",
    "print(f\"\\nüéâ AGGREGATE PODCAST GENERATION COMPLETED!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ Output directory: {episode_dir}\")\n",
    "print(f\"‚úÖ Script: {script_json_path}\")\n",
    "print(f\"‚úÖ Markdown: {script_md_path}\")\n",
    "if audio_success:\n",
    "    print(f\"‚úÖ Audio: {audio_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå Audio synthesis failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VSC-9. ANALYSIS AND EXPORT",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. ANALYSIS AND EXPORT\n",
    "print(\"üìä ANALYZING GENERATED PODCASTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze the generated podcasts\n",
    "def analyze_podcast_quality(episodes: List[Dict]) -> Dict:\n",
    "    \"\"\"Analyze quality metrics of generated podcasts\"\"\"\n",
    "    \n",
    "    analysis = {\n",
    "        'total_episodes': len(episodes),\n",
    "        'successful_episodes': 0,\n",
    "        'failed_episodes': 0,\n",
    "        'total_duration_minutes': 0,\n",
    "        'average_similarity_score': 0,\n",
    "        'script_metrics': {\n",
    "            'average_word_count': 0,\n",
    "            'average_findings_count': 0\n",
    "        },\n",
    "        'quality_scores': [],\n",
    "        'research_fields': {}\n",
    "    }\n",
    "    \n",
    "    successful_episodes = [ep for ep in episodes if ep['status'] == 'completed']\n",
    "    analysis['successful_episodes'] = len(successful_episodes)\n",
    "    analysis['failed_episodes'] = len(episodes) - len(successful_episodes)\n",
    "    \n",
    "    if successful_episodes:\n",
    "        # Duration analysis\n",
    "        durations = [ep.get('metadata', {}).get('duration_minutes', 0) for ep in successful_episodes]\n",
    "        analysis['total_duration_minutes'] = sum(durations)\n",
    "        analysis['average_duration_minutes'] = np.mean(durations)\n",
    "        \n",
    "        # Similarity score analysis\n",
    "        similarity_scores = [ep.get('metadata', {}).get('similarity_score', 0) for ep in successful_episodes]\n",
    "        analysis['average_similarity_score'] = np.mean(similarity_scores)\n",
    "        analysis['similarity_score_range'] = [min(similarity_scores), max(similarity_scores)]\n",
    "        \n",
    "        # Script analysis\n",
    "        word_counts = []\n",
    "        findings_counts = []\n",
    "        \n",
    "        for ep in successful_episodes:\n",
    "            script_data = ep.get('script', {})\n",
    "            if script_data:\n",
    "                # Count words in all script sections\n",
    "                all_text = ' '.join([\n",
    "                    script_data.get('introduction', ''),\n",
    "                    script_data.get('research_context', ''),\n",
    "                    script_data.get('methods_summary', ''),\n",
    "                    ' '.join(script_data.get('key_findings', [])),\n",
    "                    script_data.get('institute_connection', ''),\n",
    "                    script_data.get('implications_and_significance', ''),\n",
    "                    script_data.get('conclusion', '')\n",
    "                ])\n",
    "                word_counts.append(len(all_text.split()))\n",
    "                findings_counts.append(len(script_data.get('key_findings', [])))\n",
    "        \n",
    "        if word_counts:\n",
    "            analysis['script_metrics']['average_word_count'] = int(np.mean(word_counts))\n",
    "            analysis['script_metrics']['word_count_range'] = [min(word_counts), max(word_counts)]\n",
    "        \n",
    "        if findings_counts:\n",
    "            analysis['script_metrics']['average_findings_count'] = np.mean(findings_counts)\n",
    "        \n",
    "        # Research field analysis\n",
    "        for ep in successful_episodes:\n",
    "            title = ep.get('metadata', {}).get('recent_article', {}).get('title', '').lower()\n",
    "            \n",
    "            # Simple field detection\n",
    "            if any(word in title for word in ['neural', 'brain', 'neuron']):\n",
    "                field = 'Neuroscience'\n",
    "            elif any(word in title for word in ['cancer', 'tumor', 'oncology']):\n",
    "                field = 'Oncology'\n",
    "            elif any(word in title for word in ['immune', 'antibody', 'vaccine']):\n",
    "                field = 'Immunology'\n",
    "            elif any(word in title for word in ['gene', 'genetic', 'dna']):\n",
    "                field = 'Genetics'\n",
    "            elif any(word in title for word in ['heart', 'cardiac', 'cardiovascular']):\n",
    "                field = 'Cardiology'\n",
    "            else:\n",
    "                field = 'Other'\n",
    "            \n",
    "            analysis['research_fields'][field] = analysis['research_fields'].get(field, 0) + 1\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Perform analysis\n",
    "podcast_analysis = analyze_podcast_quality(generated_episodes)\n",
    "\n",
    "# Display analysis results\n",
    "print(f\"\\nüìà PODCAST QUALITY ANALYSIS:\")\n",
    "print(f\"   Episodes generated: {podcast_analysis['total_episodes']}\")\n",
    "print(f\"   Success rate: {podcast_analysis['successful_episodes']}/{podcast_analysis['total_episodes']} ({podcast_analysis['successful_episodes']/podcast_analysis['total_episodes']:.1%})\")\n",
    "\n",
    "if podcast_analysis['successful_episodes'] > 0:\n",
    "    print(f\"\\n‚è±Ô∏è DURATION METRICS:\")\n",
    "    print(f\"   Total content: {podcast_analysis['total_duration_minutes']:.1f} minutes\")\n",
    "    print(f\"   Average episode: {podcast_analysis.get('average_duration_minutes', 0):.1f} minutes\")\n",
    "    \n",
    "    print(f\"\\nüìä SIMILARITY METRICS:\")\n",
    "    print(f\"   Average similarity: {podcast_analysis['average_similarity_score']:.3f}\")\n",
    "    print(f\"   Similarity range: {podcast_analysis['similarity_score_range'][0]:.3f} - {podcast_analysis['similarity_score_range'][1]:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìù SCRIPT METRICS:\")\n",
    "    print(f\"   Average words: {podcast_analysis['script_metrics']['average_word_count']}\")\n",
    "    print(f\"   Average findings: {podcast_analysis['script_metrics']['average_findings_count']:.1f}\")\n",
    "    \n",
    "    if podcast_analysis['research_fields']:\n",
    "        print(f\"\\nüî¨ RESEARCH FIELDS:\")\n",
    "        for field, count in podcast_analysis['research_fields'].items():\n",
    "            print(f\"   {field}: {count} episode(s)\")\n",
    "\n",
    "# Save analysis results\n",
    "analysis_path = podcast_output_dir / \"podcast_analysis.json\"\n",
    "with open(analysis_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(podcast_analysis, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"\\nüíæ Analysis saved to: {analysis_path}\")\n",
    "\n",
    "# Generate RSS feed for podcast distribution\n",
    "def generate_rss_feed(episodes: List[Dict], output_path: Path):\n",
    "    \"\"\"Generate RSS feed for podcast distribution\"\"\"\n",
    "    \n",
    "    from xml.etree.ElementTree import Element, SubElement, tostring\n",
    "    from xml.dom import minidom\n",
    "    \n",
    "    # Create RSS structure\n",
    "    rss = Element('rss', version='2.0')\n",
    "    rss.set('xmlns:itunes', 'http://www.itunes.com/dtds/podcast-1.0.dtd')\n",
    "    \n",
    "    channel = SubElement(rss, 'channel')\n",
    "    \n",
    "    # Channel information\n",
    "    SubElement(channel, 'title').text = 'UBMI-IFC Research Frontiers'\n",
    "    SubElement(channel, 'description').text = 'Exploring cutting-edge research and its connections to institute work'\n",
    "    SubElement(channel, 'language').text = 'en-us'\n",
    "    SubElement(channel, 'category').text = 'Science'\n",
    "    SubElement(channel, 'pubDate').text = datetime.now().strftime('%a, %d %b %Y %H:%M:%S %z')\n",
    "    \n",
    "    # Add episodes\n",
    "    for episode in episodes:\n",
    "        if episode['status'] == 'completed':\n",
    "            item = SubElement(channel, 'item')\n",
    "            \n",
    "            metadata = episode.get('metadata', {})\n",
    "            \n",
    "            SubElement(item, 'title').text = metadata.get('title', f\"Episode {episode['episode_number']}\")\n",
    "            SubElement(item, 'description').text = metadata.get('description', 'Research podcast episode')\n",
    "            SubElement(item, 'pubDate').text = datetime.fromisoformat(metadata.get('generation_date', datetime.now().isoformat())).strftime('%a, %d %b %Y %H:%M:%S %z')\n",
    "            \n",
    "            # Add audio enclosure if available\n",
    "            audio_file = episode.get('files_generated', {}).get('audio_mp3')\n",
    "            if audio_file and Path(audio_file).exists():\n",
    "                file_size = Path(audio_file).stat().st_size\n",
    "                enclosure = SubElement(item, 'enclosure')\n",
    "                enclosure.set('url', f\"file://{audio_file}\")  # In production, use actual URL\n",
    "                enclosure.set('length', str(file_size))\n",
    "                enclosure.set('type', 'audio/mpeg')\n",
    "    \n",
    "    # Pretty print XML\n",
    "    rough_string = tostring(rss, 'unicode')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(reparsed.toprettyxml(indent=\"  \"))\n",
    "\n",
    "# Generate RSS feed\n",
    "rss_path = podcast_output_dir / \"podcast_feed.xml\"\n",
    "generate_rss_feed(generated_episodes, rss_path)\n",
    "\n",
    "print(f\"\\nüì° RSS feed generated: {rss_path}\")\n",
    "print(f\"\\nüéâ PODCAST GENERATION PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ All outputs saved to: {podcast_output_dir}\")\n",
    "print(f\"üìä Analysis available in: podcast_analysis.json\")\n",
    "print(f\"üì° RSS feed ready: podcast_feed.xml\")\n",
    "print(f\"\\nüöÄ Your podcast is ready for distribution!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
