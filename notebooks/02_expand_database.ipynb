{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11c74b3",
   "metadata": {},
   "source": [
    "# Building IFC Publications Database\n",
    "\n",
    "This notebook implements multiple strategies to build a comprehensive database of Instituto de Fisiolog√≠a Celular publications:\n",
    "\n",
    "1. **PDF Acquisition**: Sci-Hub integration + BibTeX export for Zotero\n",
    "2. **Affiliation Mining**: Extract all variations of institute names from existing PDFs\n",
    "3. **PubMed Search Strategy**: Use discovered affiliations to find more papers\n",
    "4. **Database Expansion**: Automated workflow to grow the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78786ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import bibtexparser\n",
    "from bibtexparser.bwriter import BibTexWriter\n",
    "from bibtexparser.bibdatabase import BibDatabase\n",
    "from pypdf import PdfReader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b7c3c",
   "metadata": {},
   "source": [
    "## 1. Load Existing Publications Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3866a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 publications\n",
      "Sample publication:\n",
      "{\n",
      "  \"title\": \"Cardiac physiology under metabolic stress conditions\",\n",
      "  \"authors\": \"Hern√°ndez-Campos, L., L√≥pez-Mart√≠n, R.\",\n",
      "  \"journal\": \"Cardiovascular Research\",\n",
      "  \"year\": 2024,\n",
      "  \"doi\": \"10.1093/cvr/cvz098\",\n",
      "  \"pubmed_id\": \"38234567\",\n",
      "  \"ifc_url\": \"https://www.ifc.unam.mx/publicacion.php?ut=000234567890\",\n",
      "  \"abstract\": \"Heart function during metabolic stress was analyzed using isolated perfused heart preparations. Our results demonstrate significant changes in...\",\n",
      "  \"keywords\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load your existing publications\n",
    "with open('../data/raw/test_ifc_publications.json', 'r', encoding='utf-8') as f:\n",
    "    publications = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(publications)} publications\")\n",
    "print(\"Sample publication:\")\n",
    "print(json.dumps(publications[1], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e9272",
   "metadata": {},
   "source": [
    "## 2. PDF Acquisition Strategy\n",
    "\n",
    "### Option A: Sci-Hub Integration (own implementation)\n",
    "\n",
    "> ‚ö†Ô∏è USE OPTION C\n",
    "\n",
    "- Not tested for CAPTCHAs\n",
    "- Use method B or C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cc8f9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Trying direct download approach...\n",
      "Downloading DOI: 10.1523/JNEUROSCI.1234-24.2024\n",
      "Downloading DOI: 10.1093/cvr/cvz098\n",
      "\n",
      "Download summary: 0/2 papers downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def direct_doi_download(dois, output_dir='../papers/downloaded/direct'):\n",
    "    \"\"\"Directly download papers from Sci-Hub using DOIs\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # List of Sci-Hub mirrors to try\n",
    "    mirrors = [\n",
    "        \"https://sci-hub.se/\",\n",
    "        \"https://sci-hub.st/\",\n",
    "        \"https://sci-hub.ru/\",\n",
    "        # Add more mirrors as needed\n",
    "    ]\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    })\n",
    "    \n",
    "    success_count = 0\n",
    "    \n",
    "    for doi in dois:\n",
    "        print(f\"Downloading DOI: {doi}\")\n",
    "        \n",
    "        for mirror in mirrors:\n",
    "            try:\n",
    "                url = f\"{mirror}{doi}\"\n",
    "                response = session.get(url, timeout=10)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Check if response is PDF\n",
    "                    if 'application/pdf' in response.headers.get('content-type', ''):\n",
    "                        # Save PDF\n",
    "                        filename = f\"{doi.replace('/', '_')}.pdf\"\n",
    "                        filepath = os.path.join(output_dir, filename)\n",
    "                        \n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            f.write(response.content)\n",
    "                            \n",
    "                        print(f\"‚úÖ Downloaded to {filepath}\")\n",
    "                        success_count += 1\n",
    "                        break  # Move to next DOI after successful download\n",
    "                    else:\n",
    "                        # Handle HTML response (Sci-Hub page)\n",
    "                        # You'd need a more sophisticated parser to extract the PDF link from the HTML\n",
    "                        pass\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Failed with {mirror}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        time.sleep(2)  # Be respectful\n",
    "    \n",
    "    print(f\"\\nDownload summary: {success_count}/{len(dois)} papers downloaded\")\n",
    "    return success_count\n",
    "\n",
    "# Usage example\n",
    "print(\"\\nüìÑ Trying direct download approach...\")\n",
    "direct_doi_download(sample_dois)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4aaf7",
   "metadata": {},
   "source": [
    "### Option B: BibTeX Export for Zotero\n",
    "\n",
    "> ‚ö†Ô∏è USE OPTION C\n",
    "\n",
    "> Use one of the multiple zotero -> sci-hub plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73c94b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing improved BibTeX creation...\n",
      "üìö Created BibTeX file with 2 entries: ../data/processed/ifc_publications.bib\n",
      "Import this file into Zotero to download PDFs automatically\n",
      "\n",
      "üîç Sample author formatting:\n",
      "1. Original: Garc√≠a-L√≥pez, M., Rodr√≠guez-Silva, A., Mendoza-P√©rez, J.\n",
      "   BibTeX:   Garc√≠a-L√≥pez, M. and Rodr√≠guez-Silva, A. and Mendoza-P√©rez, J.\n",
      "2. Original: Hern√°ndez-Campos, L., L√≥pez-Mart√≠n, R.\n",
      "   BibTeX:   Hern√°ndez-Campos, L. and L√≥pez-Mart√≠n, R.\n",
      "\n",
      "BibTeX file created at: ../data/processed/ifc_publications.bib\n",
      "\n",
      "üìÑ Sample BibTeX entries:\n",
      "@article{GarcaLpez2024_ifc_0,\n",
      " abstract = {We investigated the cellular and molecular mechanisms underlying memory formation in hippocampal circuits. Using electrophysiological recordings and optogenetic manipulations, we found that...},\n",
      " author = {Garc√≠a-L√≥pez, M. and Rodr√≠guez-Silva, A. and Mendoza-P√©rez, J.},\n",
      " doi = {10.1523/JNEUROSCI.1234-24.2024},\n",
      " journal = {Journal of Neuroscience},\n",
      " note = {Instituto de Fisiolog√≠a Celular, UNAM},\n",
      " pmid = {38123456},\n",
      " title = {Neural mechanisms of memory formation in hippocampal circuits},\n",
      " url = {https://www.ifc.unam.mx/publicacion.php?ut=000123456789},\n",
      " year = {2024}\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_bibtex_from_publications(publications, output_file='../data/processed/ifc_publications.bib'):\n",
    "    \"\"\"Convert JSON publications to BibTeX format for Zotero import\"\"\"\n",
    "    \n",
    "    db = BibDatabase()\n",
    "    entries = []\n",
    "    \n",
    "    def format_authors_for_bibtex(author_string):\n",
    "        \"\"\"Convert author string to proper BibTeX format\"\"\"\n",
    "        if not author_string:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Split by commas and clean each author\n",
    "        authors = [author.strip() for author in author_string.split(',')]\n",
    "        \n",
    "        # Group authors (assuming they come in pairs: LastName, FirstName)\n",
    "        formatted_authors = []\n",
    "        i = 0\n",
    "        while i < len(authors):\n",
    "            if i + 1 < len(authors):\n",
    "                # Check if next item looks like a first name (short, no hyphens typically)\n",
    "                next_item = authors[i + 1].strip()\n",
    "                if (len(next_item) <= 3 or \n",
    "                    (len(next_item.split()) == 1 and '.' in next_item) or\n",
    "                    re.match(r'^[A-Z]\\.?$', next_item)):\n",
    "                    # This is likely a first name/initial\n",
    "                    last_name = authors[i].strip()\n",
    "                    first_name = next_item\n",
    "                    formatted_authors.append(f\"{last_name}, {first_name}\")\n",
    "                    i += 2\n",
    "                else:\n",
    "                    # This is likely a full name or last name only\n",
    "                    formatted_authors.append(authors[i].strip())\n",
    "                    i += 1\n",
    "            else:\n",
    "                # Last author, no pair\n",
    "                formatted_authors.append(authors[i].strip())\n",
    "                i += 1\n",
    "        \n",
    "        # Join with \" and \" for BibTeX format\n",
    "        return \" and \".join(formatted_authors)\n",
    "    \n",
    "    for i, pub in enumerate(publications):\n",
    "        # Create a unique citation key\n",
    "        first_author = pub['authors'].split(',')[0].strip() if pub['authors'] else 'Unknown'\n",
    "        first_author_clean = re.sub(r'[^a-zA-Z]', '', first_author)\n",
    "        citation_key = f\"{first_author_clean}{pub['year']}_ifc_{i}\"\n",
    "        \n",
    "        # Format authors properly for BibTeX\n",
    "        formatted_authors = format_authors_for_bibtex(pub['authors'])\n",
    "        \n",
    "        entry = {\n",
    "            'ENTRYTYPE': 'article',\n",
    "            'ID': citation_key,\n",
    "            'title': pub['title'],\n",
    "            'author': formatted_authors,  # Now properly formatted\n",
    "            'journal': pub['journal'],\n",
    "            'year': str(pub['year']),\n",
    "            'abstract': pub.get('abstract', ''),\n",
    "            'url': pub.get('ifc_url', ''),\n",
    "            'note': 'Instituto de Fisiolog√≠a Celular, UNAM'\n",
    "        }\n",
    "        \n",
    "        if pub.get('doi'):\n",
    "            entry['doi'] = pub['doi']\n",
    "            \n",
    "        if pub.get('pubmed_id'):\n",
    "            entry['pmid'] = pub['pubmed_id']\n",
    "            \n",
    "        entries.append(entry)\n",
    "    \n",
    "    db.entries = entries\n",
    "    \n",
    "    # Write to file\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    writer = BibTexWriter()\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(writer.write(db))\n",
    "    \n",
    "    print(f\"üìö Created BibTeX file with {len(entries)} entries: {output_file}\")\n",
    "    print(\"Import this file into Zotero to download PDFs automatically\")\n",
    "    \n",
    "    # Show sample formatted authors for verification\n",
    "    print(\"\\nüîç Sample author formatting:\")\n",
    "    for i, entry in enumerate(entries[:3]):\n",
    "        print(f\"{i+1}. Original: {publications[i]['authors']}\")\n",
    "        print(f\"   BibTeX:   {entry['author']}\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# Test the improved function\n",
    "print(\"üîß Testing improved BibTeX creation...\")\n",
    "bibtex_file = create_bibtex_from_publications(publications)\n",
    "print(f\"\\nBibTeX file created at: {bibtex_file}\")\n",
    "\n",
    "# Let's also check the actual BibTeX content\n",
    "print(\"\\nüìÑ Sample BibTeX entries:\")\n",
    "with open(bibtex_file, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    # Show first entry\n",
    "    first_entry_end = content.find('\\n}\\n') + 3\n",
    "    print(content[:first_entry_end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad48f81",
   "metadata": {},
   "source": [
    "### Option C: PyPaperBot\n",
    "\n",
    "- [Repo](https://github.com/ferru97/PyPaperBot)\n",
    "\n",
    "- Download papers given a query\n",
    "- Download papers given paper's DOIs\n",
    "- Generate Bibtex of the downloaded paper\n",
    "- Filter downloaded paper by year, journal and citations number\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "- Multiple Download Methods: DOI-based downloads (most reliable)\n",
    "- Google Scholar queries\n",
    "- BibTeX-only generation\n",
    "- Flexible Modes: Download PDFs only, BibTeX only, or both\n",
    "- IFC-Specific Queries: Pre-configured searches for the institute\n",
    "- Deduplication: Automatic removal of duplicate downloads\n",
    "- Rate Limiting: Respectful delays between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97a40a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPaperBot in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (1.4.1)\n",
      "Collecting undetected-chromedriver\n",
      "  Downloading undetected-chromedriver-3.5.5.tar.gz (65 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: astroid<=2.5,>=2.4.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (4.13.5)\n",
      "Requirement already satisfied: bibtexparser>=1.2.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (1.4.3)\n",
      "Requirement already satisfied: certifi>=2020.6.20 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2025.8.3)\n",
      "Requirement already satisfied: chardet>=3.0.4 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (5.2.0)\n",
      "Requirement already satisfied: colorama>=0.4.3 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (0.4.6)\n",
      "Requirement already satisfied: crossref-commons>=0.0.7 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (0.0.7)\n",
      "Requirement already satisfied: future>=0.18.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (1.0.0)\n",
      "Requirement already satisfied: HTMLParser>=0.0.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (0.0.2)\n",
      "Requirement already satisfied: idna<3,>=2.10 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.10)\n",
      "Requirement already satisfied: isort>=5.4.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (5.13.2)\n",
      "Requirement already satisfied: lazy-object-proxy>=1.4.3 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (1.12.0)\n",
      "Requirement already satisfied: mccabe>=0.6.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (0.6.1)\n",
      "Requirement already satisfied: numpy in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.2.6)\n",
      "Requirement already satisfied: pandas in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.3.2)\n",
      "Requirement already satisfied: pyChainedProxy>=1.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (1.3)\n",
      "Requirement already satisfied: pylint>=2.6.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.7.1)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2025.2)\n",
      "Requirement already satisfied: ratelimit>=2.2.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.24.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.32.5)\n",
      "Requirement already satisfied: six>=1.15.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>=2.0.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.8)\n",
      "Requirement already satisfied: toml>=0.10.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (0.10.2)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.12.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (1.12.1)\n",
      "Requirement already satisfied: selenium>=4.9.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from undetected-chromedriver) (4.35.0)\n",
      "Requirement already satisfied: websockets in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from beautifulsoup4>=4.9.1->PyPaperBot) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from requests>=2.24.0->PyPaperBot) (3.4.3)\n",
      "Requirement already satisfied: trio~=0.30.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from trio~=0.30.0->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from trio~=0.30.0->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: outcome in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from trio~=0.30.0->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from trio~=0.30.0->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from trio~=0.30.0->selenium>=4.9.0->undetected-chromedriver) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from trio-websocket~=0.12.2->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from pandas->PyPaperBot) (2025.2)\n",
      "Building wheels for collected packages: undetected-chromedriver\n",
      "\u001b[33m  DEPRECATION: Building 'undetected-chromedriver' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'undetected-chromedriver'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for undetected-chromedriver (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for undetected-chromedriver: filename=undetected_chromedriver-3.5.5-py3-none-any.whl size=47061 sha256=aaba7f374a2c9d08a5e10d72c01b8765046a8ff9e4a4f51e7b7903ad6115484f\n",
      "  Stored in directory: /home/santi/.cache/pip/wheels/cf/a1/db/e1275b6f7259aacd6b045f8bfcb1fcbc93827a3916ba55d5b7\n",
      "Successfully built undetected-chromedriver\n",
      "Installing collected packages: undetected-chromedriver\n",
      "Successfully installed undetected-chromedriver-3.5.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPaperBot undetected-chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596d171",
   "metadata": {},
   "source": [
    "Chrome Installation Check\n",
    "\n",
    "(required by undetected_chromedriver):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "170f8fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chrome/Chromium found at: /usr/bin/chromium-browser\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_chrome_installed():\n",
    "    \"\"\"Check if Chrome/Chromium is installed on the system\"\"\"\n",
    "    chrome_paths = [\n",
    "        \"google-chrome\",\n",
    "        \"chromium-browser\",\n",
    "        \"chromium\",\n",
    "        \"/usr/bin/google-chrome\",\n",
    "        \"/usr/bin/chromium-browser\"\n",
    "    ]\n",
    "    \n",
    "    for path in chrome_paths:\n",
    "        try:\n",
    "            result = subprocess.run([\"which\", path], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"‚úÖ Chrome/Chromium found at: {result.stdout.strip()}\")\n",
    "                return True\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(\"‚ùå Chrome/Chromium not found. Please install it for PyPaperBot to work properly.\")\n",
    "    print(\"   On Ubuntu/Debian: sudo apt install chromium-browser\")\n",
    "    print(\"   On Fedora: sudo dnf install chromium\")\n",
    "    return False\n",
    "\n",
    "# Check if Chrome is installed\n",
    "check_chrome_installed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f84fb687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 2 publications\n",
      "Found 2 DOIs to process\n",
      "\n",
      "üß™ Testing PyPaperBot with 2 sample publications...\n",
      "Sample DOIs: ['10.1523/JNEUROSCI.1234-24.2024', '10.1093/cvr/cvz098']\n",
      "\n",
      "üì• Test 1: Downloading PDFs only\n",
      "Executing command: python -m PyPaperBot --doi-file ../papers/downloaded/pdf_only/temp_dois.txt --dwn-dir ../papers/downloaded/pdf_only --restrict 1 --use-doi-as-filename\n",
      "\n",
      "Output:\n",
      "PyPaperBot v1.4.1\n",
      "PyPaperBot is a Python tool for downloading scientific papers using Google Scholar, Crossref and SciHub.\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "Downloading papers from DOIs\n",
      "\n",
      "Searching paper 1 of 2 with DOI 10.1523/JNEUROSCI.1234-24.2024\n",
      "Searching paper 2 of 2 with DOI 10.1093/cvr/cvz098\n",
      "Searching for a sci-hub mirror\n",
      "Trying with https://sci-hub.ee...\n",
      "\n",
      "Using Sci-Hub mirror https://sci-hub.ee\n",
      "Using Sci-DB mirror https://annas-archive.se/scidb/\n",
      "You can use --scidb-mirror and --scidb-mirror to specify your're desired mirror URL\n",
      "\n",
      "Download 1 of 2 -> Challenges and Approaches in the Study of Neural Entrainment\n",
      "Download 2 of 2 -> Cardiovascular Research at the American College of Cardiology Scientific Sessions 2019: the meeting's highlights\n",
      "\n",
      "Work completed!\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "\n",
      "\n",
      "üìö Test 2: Generating BibTeX only\n",
      "Executing command: python -m PyPaperBot --doi-file ../papers/downloaded/bibtex_only/temp_dois.txt --dwn-dir ../papers/downloaded/bibtex_only --restrict 0 --use-doi-as-filename\n",
      "\n",
      "Output:\n",
      "PyPaperBot v1.4.1\n",
      "PyPaperBot is a Python tool for downloading scientific papers using Google Scholar, Crossref and SciHub.\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "Downloading papers from DOIs\n",
      "\n",
      "Searching paper 1 of 2 with DOI 10.1523/JNEUROSCI.1234-24.2024\n",
      "Python 3\n",
      "Searching paper 2 of 2 with DOI 10.1093/cvr/cvz098\n",
      "Python 3\n",
      "\n",
      "Work completed!\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "\n",
      "\n",
      "üîç Test 3: Downloading both PDF and BibTeX\n",
      "Executing command: python -m PyPaperBot --doi-file ../papers/downloaded/combined/temp_dois.txt --dwn-dir ../papers/downloaded/combined --restrict 2 --use-doi-as-filename\n",
      "\n",
      "Output:\n",
      "PyPaperBot v1.4.1\n",
      "PyPaperBot is a Python tool for downloading scientific papers using Google Scholar, Crossref and SciHub.\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "\n",
      "\n",
      "Errors:\n",
      "usage: __main__.py [-h] [--query QUERY] [--skip-words SKIP_WORDS]\n",
      "                   [--cites CITES] [--doi DOI] [--doi-file DOI_FILE]\n",
      "                   [--scholar-pages SCHOLAR_PAGES] [--dwn-dir DWN_DIR]\n",
      "                   [--min-year MIN_YEAR] [--max-dwn-year MAX_DWN_YEAR]\n",
      "                   [--max-dwn-cites MAX_DWN_CITES]\n",
      "                   [--journal-filter JOURNAL_FILTER] [--restrict {0,1}]\n",
      "                   [--scihub-mirror SCIHUB_MIRROR]\n",
      "                   [--annas-archive-mirror ANNAS_ARCHIVE_MIRROR]\n",
      "                   [--scholar-results {1,2,3,4,5,6,7,8,9,10}]\n",
      "                   [--proxy PROXY [PROXY ...]] [--single-proxy SINGLE_PROXY]\n",
      "                   [--selenium-chrome-version SELENIUM_CHROME_VERSION]\n",
      "                   [--use-doi-as-filename]\n",
      "__main__.py: error: argument --restrict: invalid choice: 2 (choose from 0, 1)\n",
      "\n",
      "\n",
      "üìä Download Summary:\n",
      "   Sample DOIs processed: 2\n",
      "   PDF download successful: True\n",
      "   BibTeX download successful: True\n",
      "   Combined download successful: False\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "\n",
    "def download_with_pypaperbot(dois=None, output_dir='../papers/downloaded', \n",
    "                           min_year=None, mode=2, use_doi_filename=True):\n",
    "    \"\"\"\n",
    "    Download papers using PyPaperBot command-line interface with dependency checking\n",
    "    \n",
    "    Args:\n",
    "        dois: List of DOIs or None\n",
    "        output_dir: Where to save outputs\n",
    "        min_year: Minimum publication year\n",
    "        mode: 0=BibTeX only, 1=PDF only, 2=both\n",
    "        use_doi_filename: Use DOI as filename instead of paper title\n",
    "    \"\"\"\n",
    "    # Check for required dependencies\n",
    "    try:\n",
    "        import importlib\n",
    "        if importlib.util.find_spec(\"undetected_chromedriver\") is None:\n",
    "            print(\"Installing missing dependency: undetected-chromedriver\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"undetected-chromedriver\"])\n",
    "            print(\"Dependency installed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not verify/install dependencies: {e}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Base command\n",
    "    cmd = [\"python\", \"-m\", \"PyPaperBot\"]\n",
    "    \n",
    "    # Add arguments based on parameters\n",
    "    if dois:\n",
    "        # For multiple DOIs, create a temporary DOI file\n",
    "        doi_file = os.path.join(output_dir, \"temp_dois.txt\")\n",
    "        with open(doi_file, 'w') as f:\n",
    "            f.write('\\n'.join(dois))\n",
    "        cmd.extend([\"--doi-file\", doi_file])\n",
    "    \n",
    "    # Add output directory\n",
    "    cmd.extend([\"--dwn-dir\", output_dir])\n",
    "    \n",
    "    # Add optional parameters\n",
    "    if min_year:\n",
    "        cmd.extend([\"--min-year\", str(min_year)])\n",
    "    \n",
    "    # Add mode (restrict parameter)\n",
    "    cmd.extend([\"--restrict\", str(mode)])\n",
    "    \n",
    "    # Use DOI as filename if requested\n",
    "    if use_doi_filename:\n",
    "        cmd.append(\"--use-doi-as-filename\")\n",
    "    \n",
    "    # Execute command\n",
    "    print(f\"Executing command: {' '.join(cmd)}\")\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        # Print output\n",
    "        print(\"\\nOutput:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        if result.stderr:\n",
    "            print(\"\\nErrors:\")\n",
    "            print(result.stderr)\n",
    "            \n",
    "        return result.returncode == 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing PyPaperBot: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test with a small sample from the publications\n",
    "print(f\"Working with {len(publications)} publications\")\n",
    "\n",
    "# Extract DOIs for PyPaperBot\n",
    "dois = [pub.get('doi') for pub in publications if pub.get('doi')]\n",
    "print(f\"Found {len(dois)} DOIs to process\")\n",
    "\n",
    "# Test with a small sample (2)\n",
    "print(\"\\nüß™ Testing PyPaperBot with 2 sample publications...\")\n",
    "sample_dois = dois[:2]\n",
    "print(f\"Sample DOIs: {sample_dois}\")\n",
    "\n",
    "# Run PyPaperBot in different modes\n",
    "print(\"\\nüì• Test 1: Downloading PDFs only\")\n",
    "pdf_success = download_with_pypaperbot(sample_dois, output_dir='../papers/downloaded/pdf_only', mode=1)\n",
    "\n",
    "print(\"\\nüìö Test 2: Generating BibTeX only\")\n",
    "bibtex_success = download_with_pypaperbot(sample_dois, output_dir='../papers/downloaded/bibtex_only', mode=0)\n",
    "\n",
    "print(\"\\nüîç Test 3: Downloading both PDF and BibTeX\")\n",
    "combined_success = download_with_pypaperbot(sample_dois, output_dir='../papers/downloaded/combined', mode=2)\n",
    "\n",
    "print(\"\\nüìä Download Summary:\")\n",
    "print(f\"   Sample DOIs processed: {len(sample_dois)}\")\n",
    "print(f\"   PDF download successful: {pdf_success}\")\n",
    "print(f\"   BibTeX download successful: {bibtex_success}\")\n",
    "print(f\"   Combined download successful: {combined_success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e77b2",
   "metadata": {},
   "source": [
    "def run_complete_pipeline(initial_json_path, output_dir='../data/processed'):\n",
    "    \"\"\"Complete automated pipeline to expand publication database\"\"\"\n",
    "    \n",
    "    # ... existing code ...\n",
    "    \n",
    "    # Step 3b: Download PDFs for existing publications\n",
    "    print(\"\\nüì• Step 3b: Downloading PDFs using PyPaperBot\")\n",
    "    dois = [pub.get('doi') for pub in existing_pubs if pub.get('doi')]\n",
    "    pdf_dir = os.path.join(output_dir, 'pdfs')\n",
    "    download_success = download_with_pypaperbot(\n",
    "        dois, \n",
    "        output_dir=pdf_dir,\n",
    "        mode=1  # PDF only\n",
    "    )\n",
    "    print(f\"   PDF download {'successful' if download_success else 'failed'}\")\n",
    "    print(f\"   Check output directory: {pdf_dir}\")\n",
    "    \n",
    "    # ... rest of existing pipeline ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1/1 (5 DOIs)\n",
      "Executing command: python -m PyPaperBot --doi-file papers/mining/01_run1/batch_1/temp_dois.txt --dwn-dir papers/mining/01_run1/batch_1 --restrict 1 --use-doi-as-filename\n",
      "\n",
      "Output:\n",
      "PyPaperBot v1.4.1\n",
      "PyPaperBot is a Python tool for downloading scientific papers using Google Scholar, Crossref and SciHub.\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "Downloading papers from DOIs\n",
      "\n",
      "Searching paper 1 of 5 with DOI 10.1016/j.cell.2025.03.050\n",
      "Searching paper 2 of 5 with DOI 10.1523/JNEUROSCI.1234-24.2024\n",
      "Searching paper 3 of 5 with DOI 10.1073/pnas.2420356122\n",
      "Searching paper 4 of 5 with DOI 10.1364/ol.547539\n",
      "Searching paper 5 of 5 with DOI 10.1016/j.neulet.2025.138361 \n",
      "Searching for a sci-hub mirror\n",
      "Trying with https://sci-hub.ee...\n",
      "\n",
      "Using Sci-Hub mirror https://sci-hub.ee\n",
      "Using Sci-DB mirror https://annas-archive.se/scidb/\n",
      "You can use --scidb-mirror and --scidb-mirror to specify your're desired mirror URL\n",
      "\n",
      "Download 1 of 5 -> Snake venom protection by a cocktail of varespladib and broadly neutralizing human antibodies\n",
      "Download 2 of 5 -> Challenges and Approaches in the Study of Neural Entrainment\n",
      "Download 3 of 5 -> Contextual neural dynamics during time perception in the primate ventral premotor cortex\n",
      "Download 4 of 5 -> Orbital angular momentum coherent state beams\n",
      "Download 5 of 5 -> Computational modeling of orthodromically evoked synaptic potentials of striatal projection neurons of direct and indirect pathways\n",
      "\n",
      "Work completed!\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "\n",
      "\n",
      "Completed 1/1 batches successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bulk_download_with_pypaperbot(all_dois, output_dir, chunk_size=50):\n",
    "    \"\"\"Download papers in chunks to avoid overwhelming the system\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process in chunks\n",
    "    success_count = 0\n",
    "    total_chunks = (len(all_dois) + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for i in range(0, len(all_dois), chunk_size):\n",
    "        chunk = all_dois[i:i+chunk_size]\n",
    "        chunk_num = (i // chunk_size) + 1\n",
    "        \n",
    "        print(f\"\\nProcessing chunk {chunk_num}/{total_chunks} ({len(chunk)} DOIs)\")\n",
    "        chunk_dir = os.path.join(output_dir, f\"batch_{chunk_num}\")\n",
    "        \n",
    "        if download_with_pypaperbot(chunk, output_dir=chunk_dir, mode=1):\n",
    "            success_count += 1\n",
    "            \n",
    "        # Add delay between chunks\n",
    "        if chunk_num < total_chunks:\n",
    "            print(\"Waiting before next batch...\")\n",
    "            time.sleep(30)  # 30 second delay between batches\n",
    "    \n",
    "    print(f\"\\nCompleted {success_count}/{total_chunks} batches successfully\")\n",
    "    return success_count == total_chunks\n",
    "\n",
    "dois_test=['10.1016/j.cell.2025.03.050', '10.1523/JNEUROSCI.1234-24.2024', '10.1073/pnas.2420356122', '10.1364/ol.547539', '10.1016/j.neulet.2025.138361 ']\n",
    "\n",
    "bulk_download_with_pypaperbot(dois_test, 'papers/mining/01_run1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94538dba",
   "metadata": {},
   "source": [
    "Testing PyPaperBot with Real IFC Publications\n",
    "\n",
    "> NOTE: uses above `bulk_download_with_pypaperbot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "141a3d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing PyPaperBot with 5 real publications\n",
      "‚úÖ Loaded 404 publications from /home/santi/Projects/UBMI-IFC-Podcast/data/raw/all_ifc_publications.json\n",
      "üìä Found 402 publications with DOIs\n",
      "\n",
      "üìù Selected 5 DOIs for testing:\n",
      "   1. 10.1016/j.neulet.2020.135247\n",
      "   2. 10.1038/s41598-021-96263-1\n",
      "   3. 10.24875/ric.24000068\n",
      "   4. 10.1016/j.bpj.2019.10.041\n",
      "   5. 10.1016/j.ceca.2023.102800\n",
      "\n",
      "üì• Downloading PDFs to ../papers/test_downloads\n",
      "\n",
      "Processing chunk 1/1 (5 DOIs)\n",
      "Executing command: python -m PyPaperBot --doi-file ../papers/test_downloads/test_run_20250921_184524/batch_1/temp_dois.txt --dwn-dir ../papers/test_downloads/test_run_20250921_184524/batch_1 --restrict 1 --use-doi-as-filename\n",
      "\n",
      "Output:\n",
      "PyPaperBot v1.4.1\n",
      "PyPaperBot is a Python tool for downloading scientific papers using Google Scholar, Crossref and SciHub.\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "Downloading papers from DOIs\n",
      "\n",
      "Searching paper 1 of 5 with DOI 10.1016/j.neulet.2020.135247\n",
      "Searching paper 2 of 5 with DOI 10.1038/s41598-021-96263-1\n",
      "Searching paper 3 of 5 with DOI 10.24875/ric.24000068\n",
      "Searching paper 4 of 5 with DOI 10.1016/j.bpj.2019.10.041\n",
      "Searching paper 5 of 5 with DOI 10.1016/j.ceca.2023.102800\n",
      "Searching for a sci-hub mirror\n",
      "Trying with https://sci-hub.ee...\n",
      "\n",
      "Using Sci-Hub mirror https://sci-hub.ee\n",
      "Using Sci-DB mirror https://annas-archive.se/scidb/\n",
      "You can use --scidb-mirror and --scidb-mirror to specify your're desired mirror URL\n",
      "\n",
      "Download 1 of 5 -> Characterization of the expression of dystrophins and dystrophin-associated proteins during embryonic neural stem/progenitor cell differentiation\n",
      "Download 2 of 5 -> Dynamic landscape of chromatin accessibility and transcriptomic changes during differentiation of human embryonic stem cells into dopaminergic neurons\n",
      "Download 3 of 5 -> Immunomodulation as a Treatment for Parkinson‚Äòs Disease in Current Trials: A Systematic Review and Meta-Analysis\n",
      "Download 4 of 5 -> The Contribution of the Ankyrin Repeat Domain of TRPV1 as a Thermal Module\n",
      "Download 5 of 5 -> The conducting state of TRPA1 modulates channel lateral mobility\n",
      "\n",
      "Work completed!\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "\n",
      "\n",
      "Completed 1/1 batches successfully\n",
      "\n",
      "‚úÖ Test successful! PDFs downloaded to ../papers/test_downloads/test_run_20250921_184524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Define test function to load DOIs from your real publication data\n",
    "def test_pypaperbot_with_real_publications(json_path, sample_size=5, output_dir='../papers/test_downloads'):\n",
    "    \"\"\"\n",
    "    Test PyPaperBot with a small sample of real publications\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to the JSON file containing publications with DOIs\n",
    "        sample_size: Number of publications to test (default: 5)\n",
    "        output_dir: Directory to save downloaded PDFs\n",
    "    \"\"\"\n",
    "    print(f\"üß™ Testing PyPaperBot with {sample_size} real publications\")\n",
    "    \n",
    "    # Load publications from JSON file\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            publications = json.load(f)\n",
    "        print(f\"‚úÖ Loaded {len(publications)} publications from {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading publications: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Extract DOIs from publications\n",
    "    dois = [pub.get('doi') for pub in publications if pub.get('doi')]\n",
    "    print(f\"üìä Found {len(dois)} publications with DOIs\")\n",
    "    \n",
    "    if not dois:\n",
    "        print(\"‚ùå No DOIs found in the publications data\")\n",
    "        return False\n",
    "    \n",
    "    # Select a random sample of DOIs\n",
    "    if len(dois) > sample_size:\n",
    "        sample_dois = random.sample(dois, sample_size)\n",
    "    else:\n",
    "        sample_dois = dois\n",
    "        print(f\"‚ö†Ô∏è Requested {sample_size} samples but only {len(dois)} DOIs available\")\n",
    "    \n",
    "    print(f\"\\nüìù Selected {len(sample_dois)} DOIs for testing:\")\n",
    "    for i, doi in enumerate(sample_dois):\n",
    "        print(f\"   {i+1}. {doi}\")\n",
    "    \n",
    "    # Download PDFs using PyPaperBot\n",
    "    print(f\"\\nüì• Downloading PDFs to {output_dir}\")\n",
    "    \n",
    "    # Create timestamp for this test run\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    test_dir = os.path.join(output_dir, f\"test_run_{timestamp}\")\n",
    "    \n",
    "    # Use the bulk download function with a single chunk\n",
    "    success = bulk_download_with_pypaperbot(\n",
    "        sample_dois, \n",
    "        output_dir=test_dir,\n",
    "        chunk_size=len(sample_dois)  # Process all in one chunk\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\n‚úÖ Test successful! PDFs downloaded to {test_dir}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Test failed. Check logs for errors.\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "# Run the test with the real publications\n",
    "ifc_publications_path = '/home/santi/Projects/UBMI-IFC-Podcast/data/raw/all_ifc_publications.json'\n",
    "test_pypaperbot_with_real_publications(ifc_publications_path, sample_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e7d5a",
   "metadata": {},
   "source": [
    "### Affiliation mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f4b4c",
   "metadata": {},
   "source": [
    "> affiliation mining system:\n",
    "\n",
    "- Extracts text from PDFs using PyMuPDF\n",
    "- Uses both regex and NLP for affiliation detection\n",
    "- Supports Spanish and English processing\n",
    "- Groups similar affiliations automatically\n",
    "- Generates PubMed search variations from discovered affiliations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816bf78",
   "metadata": {},
   "source": [
    "> NOTE‚ö†Ô∏è\n",
    "\n",
    "spaCy:\n",
    "\n",
    "- Tokenizes the text into words, punctuation, etc.\n",
    "- Part-of-speech tags each token\n",
    "- Dependency parses to understand grammatical relationships\n",
    "- Named Entity Recognition identifies spans as organizations, people, locations, etc.\n",
    "Classification assigns labels like \"ORG\" (organization), \"PERSON\", \"GPE\" (geopolitical entity)\n",
    "\n",
    "```python entity_recognition_process\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"ORG\":  # Organization entity\n",
    "        print(ent.text)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c45fcd",
   "metadata": {},
   "source": [
    "#### spacy installation\n",
    "\n",
    "```python\n",
    "# Install Python packages\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Download spaCy language models\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download es_core_news_sm\n",
    "\n",
    "# Optional: Download larger, more accurate models\n",
    "python -m spacy download en_core_web_md\n",
    "python -m spacy download es_core_news_md\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb1873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_18201/572880994.py\", line 1, in <module>\n",
      "    import spacy\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70116b66",
   "metadata": {},
   "source": [
    "#### PDF text Extraaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "064c278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF using PyMuPDF (faster and more accurate than PyPDF)\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        \n",
    "        # Extract text from each page\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {os.path.basename(pdf_path)}: {e}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        if 'doc' in locals():\n",
    "            doc.close()\n",
    "\n",
    "def batch_process_pdfs(pdf_dir, limit=None):\n",
    "    \"\"\"Process multiple PDFs and extract text\"\"\"\n",
    "    pdf_files = glob.glob(os.path.join(pdf_dir, \"**\", \"*.pdf\"), recursive=True)\n",
    "    \n",
    "    if limit:\n",
    "        pdf_files = pdf_files[:limit]\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    results = {}\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if text:\n",
    "            results[filename] = text\n",
    "    \n",
    "    print(f\"Successfully extracted text from {len(results)} PDFs\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75ff6e",
   "metadata": {},
   "source": [
    "pipeline to connect the PDF processing with affiliation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa72c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_affiliations_from_pdfs(pdf_dir, output_json=None, limit=None):\n",
    "    \"\"\"\n",
    "    Extract affiliations from PDFs and return structured data\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir: Directory containing PDFs to process\n",
    "        output_json: Optional path to save results as JSON\n",
    "        limit: Maximum number of PDFs to process\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with affiliation data\n",
    "    \"\"\"\n",
    "    # 1. Initialize the affiliation miner\n",
    "    print(\"üîç Initializing affiliation miner...\")\n",
    "    miner = EnhancedAffiliationMiner()\n",
    "    \n",
    "    # 2. Extract text from PDFs\n",
    "    print(\"\\nüìÑ Extracting text from PDFs...\")\n",
    "    pdf_texts = batch_process_pdfs(pdf_dir, limit)\n",
    "    \n",
    "    # 3. Mine affiliations from each PDF\n",
    "    print(\"\\nüè¢ Mining affiliations from extracted text...\")\n",
    "    all_affiliations = set()\n",
    "    pdf_affiliations = {}\n",
    "    \n",
    "    for filename, text in tqdm(pdf_texts.items(), desc=\"Mining affiliations\"):\n",
    "        # Process only the first few pages where affiliations typically appear\n",
    "        first_pages_text = text[:20000]  # Adjust based on typical affiliation location\n",
    "        affiliations = miner.extract_affiliations_advanced_nlp(first_pages_text)\n",
    "        \n",
    "        if affiliations:\n",
    "            pdf_affiliations[filename] = list(affiliations)\n",
    "            all_affiliations.update(affiliations)\n",
    "    \n",
    "    # 4. Cluster similar affiliations\n",
    "    print(f\"\\nüß© Clustering {len(all_affiliations)} discovered affiliations...\")\n",
    "    clusters = miner.analyze_affiliations_with_clustering(list(all_affiliations))\n",
    "    \n",
    "    # 5. Generate PubMed search variations\n",
    "    print(\"\\nüîé Generating PubMed search variations...\")\n",
    "    pubmed_variations = generate_pubmed_search_variations(clusters)\n",
    "    \n",
    "    # 6. Compile results\n",
    "    results = {\n",
    "        'total_pdfs_processed': len(pdf_texts),\n",
    "        'total_affiliations_found': len(all_affiliations),\n",
    "        'affiliation_clusters': [\n",
    "            {'representative': cluster[0], 'variations': cluster} \n",
    "            for cluster in clusters\n",
    "        ],\n",
    "        'pubmed_search_variations': pubmed_variations,\n",
    "        'pdf_affiliations': pdf_affiliations\n",
    "    }\n",
    "    \n",
    "    # 7. Save results if requested\n",
    "    if output_json:\n",
    "        import json\n",
    "        os.makedirs(os.path.dirname(output_json), exist_ok=True)\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\n‚úÖ Results saved to {output_json}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_pubmed_search_variations(affiliation_clusters):\n",
    "    \"\"\"Generate PubMed search variations from affiliation clusters\"\"\"\n",
    "    search_variations = []\n",
    "    \n",
    "    # Process each cluster\n",
    "    for cluster in affiliation_clusters:\n",
    "        # Use the first (representative) affiliation from each cluster\n",
    "        if cluster:\n",
    "            rep_affiliation = cluster[0]\n",
    "            \n",
    "            # Clean and format for PubMed\n",
    "            # Remove common punctuation and normalize spaces\n",
    "            clean_aff = re.sub(r'[,.:]', '', rep_affiliation)\n",
    "            clean_aff = re.sub(r'\\s+', ' ', clean_aff).strip()\n",
    "            \n",
    "            # Add [Affiliation] tag for PubMed\n",
    "            pubmed_variation = f\"{clean_aff}[Affiliation]\"\n",
    "            search_variations.append(pubmed_variation)\n",
    "    \n",
    "    return search_variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f92d69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the build_search_queries method in PubmedSearcher class\n",
    "def build_search_queries(self, affiliation_variations=None):\n",
    "    \"\"\"Build comprehensive search queries for different affiliation variations\"\"\"\n",
    "    \n",
    "    if affiliation_variations is None:\n",
    "        # Default variations based on your institute\n",
    "        affiliation_variations = [\n",
    "            \"Instituto de Fisiologia Celular[Affiliation]\",\n",
    "            \"Institute of Cellular Physiology[Affiliation]\",\n",
    "            \"IFC UNAM[Affiliation]\",\n",
    "            \"Departamento de Neurobiologia UNAM[Affiliation]\",\n",
    "            \"Universidad Nacional Autonoma Mexico Fisiologia[Affiliation]\",\n",
    "            \"National Autonomous University Mexico Cellular Physiology[Affiliation]\"\n",
    "        ]\n",
    "    \n",
    "    # Filter out variations that are too generic or too long\n",
    "    filtered_variations = []\n",
    "    for var in affiliation_variations:\n",
    "        # Remove the [Affiliation] suffix if present for checking\n",
    "        check_var = var.replace(\"[Affiliation]\", \"\").strip().lower()\n",
    "        \n",
    "        # Skip variations that are too short (likely noise) \n",
    "        # or don't contain key terms related to your institute\n",
    "        if len(check_var) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Skip variations without key identifiers\n",
    "        if not any(term in check_var for term in [\"fisiol\", \"physiol\", \"mexico\", \"unam\", \"ifc\", \"cellular\"]):\n",
    "            continue\n",
    "            \n",
    "        filtered_variations.append(var)\n",
    "    \n",
    "    queries = []\n",
    "    \n",
    "    # Individual affiliation searches\n",
    "    for aff in filtered_variations[:10]:  # Limit to top 10 to avoid excessive queries\n",
    "        queries.append(aff)\n",
    "        \n",
    "    # Combined searches with time ranges\n",
    "    recent_query = f\"({' OR '.join(filtered_variations[:3])}) AND (2020:2024[pdat])\"\n",
    "    historical_query = f\"({' OR '.join(filtered_variations[:3])}) AND (2010:2019[pdat])\"\n",
    "    \n",
    "    queries.extend([recent_query, historical_query])\n",
    "    \n",
    "    return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59316156",
   "metadata": {},
   "source": [
    "complete pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afa3adad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Enhanced Affiliation Mining Demo...\n",
      "‚úÖ Loaded en_core_web_sm\n",
      "‚úÖ Loaded es_core_news_sm\n",
      "üß™ Testing enhanced affiliation extraction...\n",
      "\n",
      "üß† Enhanced NLP extraction found 6 affiliations:\n",
      "   ‚Ä¢ Fisiolog√≠a Celular\n",
      "   ‚Ä¢ IFC-UNAM\n",
      "   ‚Ä¢ Instituto de Fisiolog√≠a Celular\n",
      "   ‚Ä¢ Instituto de Fisiolog√≠a Celular,\n",
      "   ‚Ä¢ National Autonomous University of Mexico\n",
      "   ‚Ä¢ UNAM\n",
      "    Centro de Investigaci√≥n\n",
      "\n",
      "üîó Found 4 similarity clusters:\n",
      "   Cluster 1: 1 variations\n",
      "      - IFC-UNAM\n",
      "   Cluster 2: 3 variations\n",
      "      - Instituto de Fisiolog√≠a Celular,\n",
      "      - Instituto de Fisiolog√≠a Celular\n",
      "      - Fisiolog√≠a Celular\n",
      "   Cluster 3: 1 variations\n",
      "      - UNAM\n",
      "    Centro de Investigaci√≥n\n",
      "   Cluster 4: 1 variations\n",
      "      - National Autonomous University of Mexico\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "class EnhancedAffiliationMiner:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with advanced spaCy features\"\"\"\n",
    "        self.nlp_models = {}\n",
    "        self.matchers = {}\n",
    "        self.load_nlp_models()\n",
    "        self.setup_custom_matchers()\n",
    "        \n",
    "    def load_nlp_models(self):\n",
    "        \"\"\"Load spaCy models with error handling\"\"\"\n",
    "        models_to_load = {\n",
    "            'en': 'en_core_web_sm',\n",
    "            'es': 'es_core_news_sm'\n",
    "        }\n",
    "        \n",
    "        for lang, model_name in models_to_load.items():\n",
    "            try:\n",
    "                nlp = spacy.load(model_name)\n",
    "                # Add custom pipeline components\n",
    "                if not nlp.has_pipe('merge_entities'):\n",
    "                    nlp.add_pipe('merge_entities')\n",
    "                \n",
    "                self.nlp_models[lang] = nlp\n",
    "                print(f\"‚úÖ Loaded {model_name}\")\n",
    "                \n",
    "                # Setup matcher for this language\n",
    "                self.matchers[lang] = Matcher(nlp.vocab)\n",
    "                \n",
    "            except OSError:\n",
    "                print(f\"‚ùå {model_name} not found. Install with:\")\n",
    "                print(f\"   python -m spacy download {model_name}\")\n",
    "    \n",
    "    def setup_custom_matchers(self):\n",
    "        \"\"\"Setup custom pattern matchers for institutional names\"\"\"\n",
    "        \n",
    "        # Patterns for Spanish institutions\n",
    "        if 'es' in self.matchers:\n",
    "            spanish_patterns = [\n",
    "                # Instituto de X patterns\n",
    "                [{\"LOWER\": \"instituto\"}, {\"LOWER\": \"de\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # Universidad patterns\n",
    "                [{\"LOWER\": \"universidad\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                [{\"LOWER\": \"universidad\"}, {\"LOWER\": \"nacional\"}, {\"LOWER\": \"aut√≥noma\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"m√©xico\"}],\n",
    "                \n",
    "                # Departamento patterns\n",
    "                [{\"LOWER\": \"departamento\"}, {\"LOWER\": \"de\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # IFC patterns\n",
    "                [{\"TEXT\": {\"REGEX\": r\"IFC-?UNAM\"}}],\n",
    "            ]\n",
    "            \n",
    "            for i, pattern in enumerate(spanish_patterns):\n",
    "                self.matchers['es'].add(f\"SPANISH_INSTITUTION_{i}\", [pattern])\n",
    "        \n",
    "        # Patterns for English institutions\n",
    "        if 'en' in self.matchers:\n",
    "            english_patterns = [\n",
    "                # University of X patterns\n",
    "                [{\"LOWER\": \"university\"}, {\"LOWER\": \"of\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # Institute of X patterns\n",
    "                [{\"LOWER\": \"institute\"}, {\"LOWER\": \"of\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # Department of X patterns\n",
    "                [{\"LOWER\": \"department\"}, {\"LOWER\": \"of\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # National Autonomous University of Mexico\n",
    "                [{\"LOWER\": \"national\"}, {\"LOWER\": \"autonomous\"}, {\"LOWER\": \"university\"}, \n",
    "                 {\"LOWER\": \"of\"}, {\"LOWER\": \"mexico\"}],\n",
    "            ]\n",
    "            \n",
    "            for i, pattern in enumerate(english_patterns):\n",
    "                self.matchers['en'].add(f\"ENGLISH_INSTITUTION_{i}\", [pattern])\n",
    "    \n",
    "    def detect_language_advanced(self, text):\n",
    "        \"\"\"Advanced language detection\"\"\"\n",
    "        try:\n",
    "            # Use langdetect for primary detection\n",
    "            detected = detect(text[:1000])  # Use first 1000 chars for speed\n",
    "            \n",
    "            # Validate with keyword analysis\n",
    "            spanish_keywords = ['de', 'del', 'la', 'el', 'y', 'universidad', 'instituto']\n",
    "            english_keywords = ['of', 'the', 'and', 'university', 'institute', 'department']\n",
    "            \n",
    "            text_lower = text.lower()\n",
    "            spanish_count = sum(1 for kw in spanish_keywords if kw in text_lower)\n",
    "            english_count = sum(1 for kw in english_keywords if kw in text_lower)\n",
    "            \n",
    "            # Override detection if keyword analysis is strong\n",
    "            if spanish_count > english_count * 1.5:\n",
    "                return 'es'\n",
    "            elif english_count > spanish_count * 1.5:\n",
    "                return 'en'\n",
    "            else:\n",
    "                return detected if detected in ['es', 'en'] else 'en'\n",
    "                \n",
    "        except:\n",
    "            return 'en'  # Default to English\n",
    "    \n",
    "    def extract_affiliations_advanced_nlp(self, text):\n",
    "        \"\"\"Advanced NER + custom patterns for affiliation extraction\"\"\"\n",
    "        language = self.detect_language_advanced(text)\n",
    "        \n",
    "        if language not in self.nlp_models:\n",
    "            print(f\"‚ö†Ô∏è No model available for language: {language}\")\n",
    "            return set()\n",
    "        \n",
    "        nlp = self.nlp_models[language]\n",
    "        matcher = self.matchers[language]\n",
    "        \n",
    "        affiliations = set()\n",
    "        \n",
    "        # Process text in chunks to handle large documents\n",
    "        max_length = 1000000\n",
    "        text_chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "        \n",
    "        for chunk in text_chunks:\n",
    "            try:\n",
    "                doc = nlp(chunk)\n",
    "                \n",
    "                # Method 1: Standard NER for organizations\n",
    "                for ent in doc.ents:\n",
    "                    if ent.label_ == \"ORG\":\n",
    "                        org_text = ent.text.strip()\n",
    "                        if self.is_relevant_affiliation(org_text):\n",
    "                            affiliations.add(org_text)\n",
    "                \n",
    "                # Method 2: Custom pattern matching\n",
    "                matches = matcher(doc)\n",
    "                for match_id, start, end in matches:\n",
    "                    span = doc[start:end]\n",
    "                    affiliation_text = span.text.strip()\n",
    "                    if len(affiliation_text) > 5:\n",
    "                        affiliations.add(affiliation_text)\n",
    "                \n",
    "                # Method 3: Context-based extraction\n",
    "                # Look for sentences containing institutional indicators\n",
    "                for sent in doc.sents:\n",
    "                    sent_text = sent.text.strip()\n",
    "                    if self.contains_institutional_indicators(sent_text, language):\n",
    "                        # Extract the institutional part\n",
    "                        extracted = self.extract_institutional_part(sent_text, language)\n",
    "                        if extracted:\n",
    "                            affiliations.add(extracted)\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing chunk: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return affiliations\n",
    "    \n",
    "    def is_relevant_affiliation(self, org_text):\n",
    "        \"\"\"Check if organization text is relevant to our search\"\"\"\n",
    "        relevant_keywords = [\n",
    "            'instituto', 'institute', 'universidad', 'university',\n",
    "            'departamento', 'department', 'unam', 'ifc', 'mexico',\n",
    "            'fisiolog', 'physiolog', 'celular', 'cellular', 'neurobiolog'\n",
    "        ]\n",
    "        \n",
    "        org_lower = org_text.lower()\n",
    "        return (len(org_text) > 10 and \n",
    "                any(keyword in org_lower for keyword in relevant_keywords))\n",
    "    \n",
    "    def contains_institutional_indicators(self, text, language):\n",
    "        \"\"\"Check if text contains institutional indicators\"\"\"\n",
    "        if language == 'es':\n",
    "            indicators = [\n",
    "                'instituto de', 'universidad', 'departamento de', \n",
    "                'centro de', 'facultad de', 'unam'\n",
    "            ]\n",
    "        else:\n",
    "            indicators = [\n",
    "                'institute of', 'university of', 'department of',\n",
    "                'center of', 'faculty of', 'unam'\n",
    "            ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        return any(indicator in text_lower for indicator in indicators)\n",
    "    \n",
    "    def extract_institutional_part(self, sentence, language):\n",
    "        \"\"\"Extract the institutional part from a sentence\"\"\"\n",
    "        # Use regex patterns to extract institutional names\n",
    "        if language == 'es':\n",
    "            patterns = [\n",
    "                r'Instituto\\s+de\\s+[A-Za-z√Å√°√â√©√ç√≠√ì√≥√ö√∫√ë√±\\s,]+?(?:,|\\.|\\s+UNAM)',\n",
    "                r'Universidad\\s+[A-Za-z√Å√°√â√©√ç√≠√ì√≥√ö√∫√ë√±\\s,]+?(?:,|\\.)',\n",
    "                r'Departamento\\s+de\\s+[A-Za-z√Å√°√â√©√ç√≠√ì√≥√ö√∫√ë√±\\s,]+?(?:,|\\.)'\n",
    "            ]\n",
    "        else:\n",
    "            patterns = [\n",
    "                r'Institute\\s+of\\s+[A-Za-z\\s,]+?(?:,|\\.|\\s+UNAM)',\n",
    "                r'University\\s+of\\s+[A-Za-z\\s,]+?(?:,|\\.)',\n",
    "                r'Department\\s+of\\s+[A-Za-z\\s,]+?(?:,|\\.)'\n",
    "            ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group().strip()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def analyze_affiliations_with_clustering(self, affiliations_list):\n",
    "        \"\"\"Advanced analysis with similarity clustering\"\"\"\n",
    "        from difflib import SequenceMatcher\n",
    "        \n",
    "        def similarity(a, b):\n",
    "            return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "        \n",
    "        # Group similar affiliations\n",
    "        clusters = []\n",
    "        processed = set()\n",
    "        \n",
    "        for affiliation in affiliations_list:\n",
    "            if affiliation in processed:\n",
    "                continue\n",
    "                \n",
    "            # Find similar affiliations\n",
    "            cluster = [affiliation]\n",
    "            processed.add(affiliation)\n",
    "            \n",
    "            for other in affiliations_list:\n",
    "                if other not in processed and similarity(affiliation, other) > 0.7:\n",
    "                    cluster.append(other)\n",
    "                    processed.add(other)\n",
    "            \n",
    "            if len(cluster) >= 1:\n",
    "                clusters.append(cluster)\n",
    "        \n",
    "        return clusters\n",
    "\n",
    "# Usage example and demo\n",
    "def demo_enhanced_mining():\n",
    "    \"\"\"Demonstrate enhanced affiliation mining\"\"\"\n",
    "    miner = EnhancedAffiliationMiner()\n",
    "    \n",
    "    sample_text = \"\"\"\n",
    "    Instituto de Fisiolog√≠a Celular, Universidad Nacional Aut√≥noma de M√©xico, \n",
    "    Ciudad Universitaria, M√©xico, D.F. 04510, M√©xico\n",
    "    \n",
    "    Department of Cellular Physiology, National Autonomous University of Mexico,\n",
    "    Mexico City, Mexico\n",
    "    \n",
    "    Departamento de Neurobiolog√≠a, Instituto de Fisiolog√≠a Celular, UNAM\n",
    "    Centro de Investigaci√≥n y de Estudios Avanzados del IPN\n",
    "    \n",
    "    IFC-UNAM, Circuito Exterior s/n, Ciudad Universitaria\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üß™ Testing enhanced affiliation extraction...\")\n",
    "    \n",
    "    # Advanced NLP extraction\n",
    "    affiliations = miner.extract_affiliations_advanced_nlp(sample_text)\n",
    "    \n",
    "    print(f\"\\nüß† Enhanced NLP extraction found {len(affiliations)} affiliations:\")\n",
    "    for affiliation in sorted(affiliations):\n",
    "        print(f\"   ‚Ä¢ {affiliation}\")\n",
    "    \n",
    "    # Clustering analysis\n",
    "    clusters = miner.analyze_affiliations_with_clustering(list(affiliations))\n",
    "    print(f\"\\nüîó Found {len(clusters)} similarity clusters:\")\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        print(f\"   Cluster {i+1}: {len(cluster)} variations\")\n",
    "        for variation in cluster:\n",
    "            print(f\"      - {variation}\")\n",
    "    \n",
    "    return affiliations\n",
    "\n",
    "# Run enhanced demo\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting Enhanced Affiliation Mining Demo...\")\n",
    "    demo_results = demo_enhanced_mining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e7b79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pdfs_and_search_pubmed(pdf_dir, output_dir='../data/processed/affiliations', \n",
    "                                 limit_pdfs=None, max_results_per_query=20):\n",
    "    \"\"\"\n",
    "    Complete pipeline: Extract affiliations from PDFs and search PubMed\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir: Directory containing PDFs to process\n",
    "        output_dir: Directory for saving outputs\n",
    "        limit_pdfs: Maximum number of PDFs to process (None for all)\n",
    "        max_results_per_query: Maximum results per PubMed query\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Mine affiliations from PDFs\n",
    "    print(\"üîé Step 1: Mining affiliations from PDFs...\")\n",
    "    affiliations_output = os.path.join(output_dir, \"discovered_affiliations.json\")\n",
    "    affiliation_results = mine_affiliations_from_pdfs(\n",
    "        pdf_dir=pdf_dir,\n",
    "        output_json=affiliations_output,\n",
    "        limit=limit_pdfs\n",
    "    )\n",
    "    \n",
    "    # Extract PubMed search variations\n",
    "    pubmed_variations = affiliation_results.get('pubmed_search_variations', [])\n",
    "    if not pubmed_variations:\n",
    "        print(\"‚ö†Ô∏è No valid PubMed search variations found. Using default variations.\")\n",
    "    else:\n",
    "        print(f\"üîç Found {len(pubmed_variations)} PubMed search variations\")\n",
    "        print(\"\\nSample variations:\")\n",
    "        for i, var in enumerate(pubmed_variations[:5]):\n",
    "            print(f\"   {i+1}. {var}\")\n",
    "    \n",
    "    # Step 2: Search PubMed with discovered affiliations\n",
    "    print(\"\\nüîç Step 2: Searching PubMed with discovered affiliations...\")\n",
    "    searcher = PubmedSearcher()\n",
    "    articles = searcher.comprehensive_search(\n",
    "        affiliation_variations=pubmed_variations,\n",
    "        max_per_query=max_results_per_query\n",
    "    )\n",
    "    \n",
    "    # Step 3: Save PubMed results\n",
    "    print(f\"\\nüìä Found {len(articles)} articles from PubMed\")\n",
    "    pubmed_output = os.path.join(output_dir, \"pubmed_results.json\")\n",
    "    with open(pubmed_output, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ PubMed results saved to {pubmed_output}\")\n",
    "    \n",
    "    # Step 4: Summary\n",
    "    print(\"\\nüìã Pipeline Summary:\")\n",
    "    print(f\"   PDFs processed: {affiliation_results['total_pdfs_processed']}\")\n",
    "    print(f\"   Unique affiliations found: {affiliation_results['total_affiliations_found']}\")\n",
    "    print(f\"   Affiliation clusters: {len(affiliation_results['affiliation_clusters'])}\")\n",
    "    print(f\"   PubMed search variations: {len(pubmed_variations)}\")\n",
    "    print(f\"   PubMed articles found: {len(articles)}\")\n",
    "    \n",
    "    return {\n",
    "        'affiliation_results': affiliation_results,\n",
    "        'pubmed_articles': articles\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833401e",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "636aeeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (4.67.1)\n",
      "Testing affiliation extraction on PDFs in ../papers/test_downloads\n",
      "üîç Initializing affiliation miner...\n",
      "‚úÖ Loaded en_core_web_sm\n",
      "‚úÖ Loaded es_core_news_sm\n",
      "\n",
      "üìÑ Extracting text from PDFs...\n",
      "Found 3 PDF files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted text from 3 PDFs\n",
      "\n",
      "üè¢ Mining affiliations from extracted text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mining affiliations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß© Clustering 12 discovered affiliations...\n",
      "\n",
      "üîé Generating PubMed search variations...\n",
      "\n",
      "‚úÖ Results saved to ../data/processed/test_affiliations.json\n",
      "\n",
      "üè¢ Discovered affiliations:\n",
      "\n",
      "‚Ä¢ Main variation: Departamento de Infectolog√≠a\n",
      "  Other variations:\n",
      "  - Departamento de Fisiolog√≠a\n",
      "  - Departamento de Infectolog√≠a e Inmunolog√≠a,\n",
      "  - Departamento de Gen√©tica\n",
      "\n",
      "‚Ä¢ Main variation: Instituto de Fisiolog√≠a Celular-Neurociencias\n",
      "  Other variations:\n",
      "  - Instituto de Fisiolog√≠a Celular,\n",
      "  - Instituto de Fisiolog√≠a Celular\n",
      "\n",
      "‚Ä¢ Main variation: Departamento de Gen√©tica y Biolog√≠a Molecular,\n",
      "\n",
      "‚Ä¢ Main variation: Instituto Polit√©cnico Nacional\n",
      "\n",
      "‚Ä¢ Main variation: Instituto Nacional de \n",
      "Perinatolog√≠a\n",
      "\n",
      "üîç PubMed search variations:\n",
      "   1. Departamento de Infectolog√≠a[Affiliation]\n",
      "   2. Instituto de Fisiolog√≠a Celular-Neurociencias[Affiliation]\n",
      "   3. Departamento de Gen√©tica y Biolog√≠a Molecular[Affiliation]\n",
      "   4. Instituto Polit√©cnico Nacional[Affiliation]\n",
      "   5. Instituto Nacional de Perinatolog√≠a[Affiliation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Install PyMuPDF if not already installed\n",
    "!pip install pymupdf tqdm\n",
    "\n",
    "# Test the affiliation extraction on the downloaded PDFs\n",
    "import os\n",
    "\n",
    "# Get the path to the recently downloaded PDFs\n",
    "test_pdf_dir = '../papers/test_downloads'  # Update with your actual path\n",
    "\n",
    "# Test with a small number of PDFs first\n",
    "print(f\"Testing affiliation extraction on PDFs in {test_pdf_dir}\")\n",
    "test_results = mine_affiliations_from_pdfs(\n",
    "    pdf_dir=test_pdf_dir,\n",
    "    output_json='../data/processed/test_affiliations.json',\n",
    "    limit=5  # Process up to 5 PDFs\n",
    ")\n",
    "\n",
    "# Display the discovered affiliations\n",
    "print(\"\\nüè¢ Discovered affiliations:\")\n",
    "for cluster in test_results['affiliation_clusters'][:5]:  # Show top 5 clusters\n",
    "    print(f\"\\n‚Ä¢ Main variation: {cluster['representative']}\")\n",
    "    if len(cluster['variations']) > 1:\n",
    "        print(\"  Other variations:\")\n",
    "        for var in cluster['variations'][1:]:\n",
    "            print(f\"  - {var}\")\n",
    "\n",
    "# Show how these could be used for PubMed search\n",
    "print(\"\\nüîç PubMed search variations:\")\n",
    "for i, var in enumerate(test_results['pubmed_search_variations'][:5]):\n",
    "    print(f\"   {i+1}. {var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90cf7d",
   "metadata": {},
   "source": [
    "Review affiliations before merging\n",
    "Manual review step before searching PubMed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "373018dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_and_select_affiliations(affiliation_clusters):\n",
    "    \"\"\"\n",
    "    Interactive review of discovered affiliation clusters before PubMed search\n",
    "    \n",
    "    Args:\n",
    "        affiliation_clusters: List of affiliation clusters discovered from PDFs\n",
    "        \n",
    "    Returns:\n",
    "        List of approved affiliation variations for PubMed search\n",
    "    \"\"\"\n",
    "    print(\"\\nüìã AFFILIATION REVIEW\\n\")\n",
    "    print(\"Review the following affiliation clusters discovered from PDFs:\")\n",
    "    print(\"Select which clusters to include in PubMed search\\n\")\n",
    "    \n",
    "    approved_clusters = []\n",
    "    approved_variations = []\n",
    "    \n",
    "    for i, cluster in enumerate(affiliation_clusters):\n",
    "        representative = cluster[0]\n",
    "        print(f\"\\nCluster {i+1}: {representative}\")\n",
    "        \n",
    "        if len(cluster) > 1:\n",
    "            print(\"  Variations:\")\n",
    "            for j, variation in enumerate(cluster[1:]):\n",
    "                print(f\"    {j+1}. {variation}\")\n",
    "        \n",
    "        # Ask for approval\n",
    "        while True:\n",
    "            choice = input(f\"\\nInclude cluster {i+1} in PubMed search? (y/n): \").lower()\n",
    "            if choice in ('y', 'yes', 'n', 'no'):\n",
    "                break\n",
    "            print(\"Please answer 'y' or 'n'\")\n",
    "        \n",
    "        if choice in ('y', 'yes'):\n",
    "            approved_clusters.append(cluster)\n",
    "            approved_variations.extend(cluster)\n",
    "            print(f\"‚úÖ Cluster {i+1} approved\")\n",
    "        else:\n",
    "            print(f\"‚ùå Cluster {i+1} excluded\")\n",
    "    \n",
    "    print(f\"\\nüìä Summary: Approved {len(approved_clusters)}/{len(affiliation_clusters)} clusters\")\n",
    "    print(f\"Total of {len(approved_variations)} affiliation variations will be used for PubMed search\")\n",
    "    \n",
    "    return approved_variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a93212",
   "metadata": {},
   "source": [
    "we should updaate the PDF processing and PubMed search workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba1898ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pdfs_and_search_pubmed_with_review(pdf_dir, output_dir='../data/processed/affiliations', \n",
    "                                             limit_pdfs=None, max_results_per_query=20):\n",
    "    \"\"\"\n",
    "    Complete pipeline with manual review: Extract affiliations, review them, then search PubMed\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir: Directory containing PDFs to process\n",
    "        output_dir: Directory for saving outputs\n",
    "        limit_pdfs: Maximum number of PDFs to process (None for all)\n",
    "        max_results_per_query: Maximum results per PubMed query\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Mine affiliations from PDFs\n",
    "    print(\"üîé Step 1: Mining affiliations from PDFs...\")\n",
    "    affiliations_output = os.path.join(output_dir, \"discovered_affiliations.json\")\n",
    "    affiliation_results = mine_affiliations_from_pdfs(\n",
    "        pdf_dir=pdf_dir,\n",
    "        output_json=affiliations_output,\n",
    "        limit=limit_pdfs\n",
    "    )\n",
    "    \n",
    "    clusters = affiliation_results.get('affiliation_clusters', [])\n",
    "    \n",
    "    # Step 2: Manual review of affiliations (NEW)\n",
    "    print(\"\\nüîç Step 2: Reviewing discovered affiliations...\")\n",
    "    if not clusters:\n",
    "        print(\"‚ö†Ô∏è No affiliation clusters found.\")\n",
    "        approved_variations = []\n",
    "    else:\n",
    "        # Extract clusters from results\n",
    "        raw_clusters = [cluster['variations'] for cluster in clusters]\n",
    "        approved_variations = review_and_select_affiliations(raw_clusters)\n",
    "    \n",
    "    # Step 3: Format approved variations for PubMed search\n",
    "    if not approved_variations:\n",
    "        print(\"‚ö†Ô∏è No affiliations approved. Using default affiliations for PubMed search.\")\n",
    "        pubmed_variations = None  # Will use defaults in PubmedSearcher\n",
    "    else:\n",
    "        pubmed_variations = []\n",
    "        for variation in approved_variations:\n",
    "            # Clean and format for PubMed\n",
    "            clean_aff = re.sub(r'[,.:]', '', variation)\n",
    "            clean_aff = re.sub(r'\\s+', ' ', clean_aff).strip()\n",
    "            pubmed_variation = f\"{clean_aff}[Affiliation]\"\n",
    "            pubmed_variations.append(pubmed_variation)\n",
    "    \n",
    "        print(f\"üîç Generated {len(pubmed_variations)} PubMed search variations\")\n",
    "        print(\"\\nSample variations:\")\n",
    "        for i, var in enumerate(pubmed_variations[:5]):\n",
    "            print(f\"   {i+1}. {var}\")\n",
    "    \n",
    "    # Step 4: Search PubMed with approved affiliations\n",
    "    print(\"\\nüîç Step 4: Searching PubMed with approved affiliations...\")\n",
    "    searcher = PubmedSearcher()\n",
    "    articles = searcher.comprehensive_search(\n",
    "        affiliation_variations=pubmed_variations,\n",
    "        max_per_query=max_results_per_query\n",
    "    )\n",
    "    \n",
    "    # Step 5: Save PubMed results\n",
    "    print(f\"\\nüìä Found {len(articles)} articles from PubMed\")\n",
    "    pubmed_output = os.path.join(output_dir, \"pubmed_results.json\")\n",
    "    with open(pubmed_output, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ PubMed results saved to {pubmed_output}\")\n",
    "    \n",
    "    # Step 6: Summary\n",
    "    print(\"\\nüìã Pipeline Summary:\")\n",
    "    print(f\"   PDFs processed: {affiliation_results['total_pdfs_processed']}\")\n",
    "    print(f\"   Unique affiliations found: {affiliation_results['total_affiliations_found']}\")\n",
    "    print(f\"   Affiliation clusters reviewed: {len(clusters)}\")\n",
    "    print(f\"   Approved variations: {len(approved_variations)}\")\n",
    "    print(f\"   PubMed articles found: {len(articles)}\")\n",
    "    \n",
    "    return {\n",
    "        'affiliation_results': affiliation_results,\n",
    "        'approved_variations': approved_variations,\n",
    "        'pubmed_articles': articles\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9257b9",
   "metadata": {},
   "source": [
    "## 4. PubMed Search Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b7f64e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting comprehensive PubMed search...\n",
      "Found 2248 results for query: Instituto de Fisiologia Celular[Affiliation]...\n",
      "\n",
      "üìä Test search found 5 articles\n",
      "\n",
      "Sample result:\n",
      "Title: Multistable bimodal perceptual coding within the ventral premotor cortex....\n",
      "Authors: Andrade-Ortega, Bernardo; D√≠az, H√©ctor; Bayones, Lucas; Alvarez, Manuel; Zainos, Antonio; Rivera-Yos...\n",
      "PMID: 40971437\n",
      "\n",
      "üîç Running search 1/8\n",
      "Found 2248 results for query: Instituto de Fisiologia Celular[Affiliation]...\n",
      "Added 20 new articles (total: 20)\n",
      "\n",
      "üîç Running search 2/8\n",
      "Found 88 results for query: Institute of Cellular Physiology[Affiliation]...\n",
      "Added 20 new articles (total: 40)\n",
      "\n",
      "üîç Running search 3/8\n",
      "Found 522 results for query: IFC UNAM[Affiliation]...\n",
      "Added 12 new articles (total: 52)\n",
      "\n",
      "üîç Running search 4/8\n",
      "Found 9 results for query: Departamento de Neurobiologia UNAM[Affiliation]...\n",
      "Added 9 new articles (total: 61)\n",
      "\n",
      "üîç Running search 5/8\n",
      "Found 3968 results for query: Universidad Nacional Autonoma Mexico Fisiologia[Af...\n",
      "Added 10 new articles (total: 71)\n",
      "\n",
      "üîç Running search 6/8\n",
      "Found 59 results for query: National Autonomous University Mexico Cellular Phy...\n",
      "Added 7 new articles (total: 78)\n",
      "\n",
      "üîç Running search 7/8\n",
      "Found 619 results for query: (Instituto de Fisiologia Celular[Affiliation] OR I...\n",
      "Added 16 new articles (total: 94)\n",
      "\n",
      "üîç Running search 8/8\n",
      "Found 929 results for query: (Instituto de Fisiologia Celular[Affiliation] OR I...\n",
      "Added 20 new articles (total: 114)\n",
      "\n",
      "üéâ Found 114 total unique articles from PubMed\n"
     ]
    }
   ],
   "source": [
    "class PubmedSearcher:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "        \n",
    "    def build_search_queries(self, affiliation_variations=None):\n",
    "        \"\"\"Build comprehensive search queries for different affiliation variations\"\"\"\n",
    "        \n",
    "        if affiliation_variations is None:\n",
    "            # Default variations based on your institute\n",
    "            affiliation_variations = [\n",
    "                \"Instituto de Fisiologia Celular[Affiliation]\",\n",
    "                \"Institute of Cellular Physiology[Affiliation]\",\n",
    "                \"IFC UNAM[Affiliation]\",\n",
    "                \"Departamento de Neurobiologia UNAM[Affiliation]\",\n",
    "                \"Universidad Nacional Autonoma Mexico Fisiologia[Affiliation]\",\n",
    "                \"National Autonomous University Mexico Cellular Physiology[Affiliation]\"\n",
    "            ]\n",
    "        \n",
    "        queries = []\n",
    "        \n",
    "        # Individual affiliation searches\n",
    "        for aff in affiliation_variations:\n",
    "            queries.append(aff)\n",
    "            \n",
    "        # Combined searches with time ranges\n",
    "        recent_query = f\"({' OR '.join(affiliation_variations[:3])}) AND (2020:2024[pdat])\"\n",
    "        historical_query = f\"({' OR '.join(affiliation_variations[:3])}) AND (2010:2019[pdat])\"\n",
    "        \n",
    "        queries.extend([recent_query, historical_query])\n",
    "        \n",
    "        return queries\n",
    "    \n",
    "    def search_pubmed(self, query, max_results=100):\n",
    "        \"\"\"Search PubMed with a given query\"\"\"\n",
    "        \n",
    "        # Step 1: Search\n",
    "        search_url = f\"{self.base_url}esearch.fcgi\"\n",
    "        search_params = {\n",
    "            'db': 'pubmed',\n",
    "            'term': query,\n",
    "            'retmax': max_results,\n",
    "            'retmode': 'json'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(search_url, params=search_params)\n",
    "            search_data = response.json()\n",
    "            \n",
    "            pmids = search_data['esearchresult']['idlist']\n",
    "            total_count = int(search_data['esearchresult']['count'])\n",
    "            \n",
    "            print(f\"Found {total_count} results for query: {query[:50]}...\")\n",
    "            \n",
    "            if not pmids:\n",
    "                return []\n",
    "            \n",
    "            # Step 2: Fetch details\n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "            \n",
    "            fetch_url = f\"{self.base_url}efetch.fcgi\"\n",
    "            fetch_params = {\n",
    "                'db': 'pubmed',\n",
    "                'id': ','.join(pmids),\n",
    "                'retmode': 'xml'\n",
    "            }\n",
    "            \n",
    "            fetch_response = requests.get(fetch_url, params=fetch_params)\n",
    "            \n",
    "            # Parse XML (simplified - you might want to use xml.etree.ElementTree)\n",
    "            articles = self.parse_pubmed_xml(fetch_response.text)\n",
    "            \n",
    "            return articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching PubMed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def parse_pubmed_xml(self, xml_content):\n",
    "        \"\"\"Simple XML parsing for PubMed results (you might want to improve this)\"\"\"\n",
    "        import xml.etree.ElementTree as ET\n",
    "        \n",
    "        articles = []\n",
    "        \n",
    "        try:\n",
    "            root = ET.fromstring(xml_content)\n",
    "            \n",
    "            for article in root.findall('.//PubmedArticle'):\n",
    "                try:\n",
    "                    # Extract basic info\n",
    "                    pmid = article.find('.//PMID').text\n",
    "                    \n",
    "                    title_elem = article.find('.//ArticleTitle')\n",
    "                    title = title_elem.text if title_elem is not None else \"No title\"\n",
    "                    \n",
    "                    # Authors\n",
    "                    authors = []\n",
    "                    for author in article.findall('.//Author'):\n",
    "                        lastname = author.find('.//LastName')\n",
    "                        firstname = author.find('.//ForeName')\n",
    "                        if lastname is not None:\n",
    "                            author_name = lastname.text\n",
    "                            if firstname is not None:\n",
    "                                author_name += f\", {firstname.text}\"\n",
    "                            authors.append(author_name)\n",
    "                    \n",
    "                    # Journal and year\n",
    "                    journal_elem = article.find('.//Journal/Title')\n",
    "                    journal = journal_elem.text if journal_elem is not None else \"Unknown\"\n",
    "                    \n",
    "                    year_elem = article.find('.//PubDate/Year')\n",
    "                    year = int(year_elem.text) if year_elem is not None else None\n",
    "                    \n",
    "                    # Abstract\n",
    "                    abstract_elem = article.find('.//Abstract/AbstractText')\n",
    "                    abstract = abstract_elem.text if abstract_elem is not None else \"\"\n",
    "                    \n",
    "                    # DOI\n",
    "                    doi_elem = article.find('.//ELocationID[@EIdType=\"doi\"]')\n",
    "                    doi = doi_elem.text if doi_elem is not None else None\n",
    "                    \n",
    "                    article_data = {\n",
    "                        'pmid': pmid,\n",
    "                        'title': title,\n",
    "                        'authors': '; '.join(authors),\n",
    "                        'journal': journal,\n",
    "                        'year': year,\n",
    "                        'abstract': abstract,\n",
    "                        'doi': doi\n",
    "                    }\n",
    "                    \n",
    "                    articles.append(article_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing article: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing XML: {e}\")\n",
    "            \n",
    "        return articles\n",
    "    \n",
    "    def comprehensive_search(self, affiliation_variations=None, max_per_query=50):\n",
    "        \"\"\"\n",
    "        Run comprehensive search with all query variations\n",
    "        \n",
    "        Args:\n",
    "            affiliation_variations: Optional list of affiliation variations to use\n",
    "            max_per_query: Maximum results per query\n",
    "        \n",
    "        Returns:\n",
    "            List of articles found\n",
    "        \"\"\"\n",
    "        queries = self.build_search_queries(affiliation_variations)\n",
    "        all_articles = []\n",
    "        seen_pmids = set()\n",
    "        \n",
    "        for i, query in enumerate(queries):\n",
    "            print(f\"\\nüîç Running search {i+1}/{len(queries)}\")\n",
    "            articles = self.search_pubmed(query, max_per_query)\n",
    "            \n",
    "            # Deduplicate\n",
    "            new_articles = []\n",
    "            for article in articles:\n",
    "                if article['pmid'] not in seen_pmids:\n",
    "                    seen_pmids.add(article['pmid'])\n",
    "                    new_articles.append(article)\n",
    "            \n",
    "            all_articles.extend(new_articles)\n",
    "            print(f\"Added {len(new_articles)} new articles (total: {len(all_articles)})\")\n",
    "            \n",
    "            time.sleep(1)  # Be respectful to NCBI\n",
    "            \n",
    "        return all_articles\n",
    "# Run comprehensive PubMed search\n",
    "print(\"üîç Starting comprehensive PubMed search...\")\n",
    "searcher = PubmedSearcher()\n",
    "\n",
    "# Test with a single query first\n",
    "test_articles = searcher.search_pubmed(\"Instituto de Fisiologia Celular[Affiliation]\", max_results=5)\n",
    "print(f\"\\nüìä Test search found {len(test_articles)} articles\")\n",
    "\n",
    "if test_articles:\n",
    "    print(\"\\nSample result:\")\n",
    "    sample = test_articles[0]\n",
    "    print(f\"Title: {sample['title'][:100]}...\")\n",
    "    print(f\"Authors: {sample['authors'][:100]}...\")\n",
    "    print(f\"PMID: {sample['pmid']}\")\n",
    "\n",
    "new_articles = searcher.comprehensive_search(max_per_query=20)\n",
    "print(f\"\\nüéâ Found {len(new_articles)} total unique articles from PubMed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df88f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c93f53bd",
   "metadata": {},
   "source": [
    "## 5. Database Integration & Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a62ae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Database expansion simulation\n",
      "Current database size: 2\n",
      "Processing 1 potential new publications...\n",
      "\n",
      "üìä Database expansion complete:\n",
      "   Original publications: 2\n",
      "   New publications added: 1\n",
      "   Total publications: 3\n",
      "   Saved to: ../data/processed/expanded_ifc_publications.json\n"
     ]
    }
   ],
   "source": [
    "def merge_publication_databases(existing_pubs, new_pubs, output_file='../data/processed/expanded_ifc_publications.json'):\n",
    "    \"\"\"Merge existing publications with newly found ones, removing duplicates\"\"\"\n",
    "    \n",
    "    # Create lookup sets for deduplication\n",
    "    existing_dois = {pub.get('doi') for pub in existing_pubs if pub.get('doi')}\n",
    "    existing_pmids = {pub.get('pubmed_id') for pub in existing_pubs if pub.get('pubmed_id')}\n",
    "    existing_titles = {pub.get('title', '').lower().strip() for pub in existing_pubs if pub.get('title')}\n",
    "    \n",
    "    merged_pubs = existing_pubs.copy()\n",
    "    new_count = 0\n",
    "    \n",
    "    print(f\"Processing {len(new_pubs)} potential new publications...\")\n",
    "    \n",
    "    for pub in new_pubs:\n",
    "        is_duplicate = False\n",
    "        \n",
    "        # Check for duplicates\n",
    "        if pub.get('doi') and pub['doi'] in existing_dois:\n",
    "            is_duplicate = True\n",
    "        elif pub.get('pmid') and pub['pmid'] in existing_pmids:\n",
    "            is_duplicate = True\n",
    "        elif pub.get('title', '').lower().strip() in existing_titles:\n",
    "            is_duplicate = True\n",
    "            \n",
    "        if not is_duplicate:\n",
    "            # Convert PubMed format to your format\n",
    "            converted_pub = {\n",
    "                'title': pub.get('title', ''),\n",
    "                'authors': pub.get('authors', ''),\n",
    "                'journal': pub.get('journal', ''),\n",
    "                'year': pub.get('year'),\n",
    "                'doi': pub.get('doi'),\n",
    "                'pubmed_id': pub.get('pmid'),\n",
    "                'ifc_url': None,  # Not available from PubMed\n",
    "                'abstract': pub.get('abstract', ''),\n",
    "                'keywords': None\n",
    "            }\n",
    "            \n",
    "            merged_pubs.append(converted_pub)\n",
    "            new_count += 1\n",
    "            \n",
    "            # Update tracking sets (FIXED - removed erroneous import)\n",
    "            if pub.get('doi'):\n",
    "                existing_dois.add(pub['doi'])\n",
    "            if pub.get('pmid'):\n",
    "                existing_pmids.add(pub['pmid'])\n",
    "            existing_titles.add(pub.get('title', '').lower().strip())\n",
    "    \n",
    "    # Save expanded database\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_pubs, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nüìä Database expansion complete:\")\n",
    "    print(f\"   Original publications: {len(existing_pubs)}\")\n",
    "    print(f\"   New publications added: {new_count}\")\n",
    "    print(f\"   Total publications: {len(merged_pubs)}\")\n",
    "    print(f\"   Saved to: {output_file}\")\n",
    "    \n",
    "    return merged_pubs\n",
    "\n",
    "# Demo with existing data\n",
    "print(\"üìà Database expansion simulation\")\n",
    "print(f\"Current database size: {len(publications)}\")\n",
    "\n",
    "# Create some demo \"new\" publications\n",
    "demo_new_pubs = [\n",
    "    {\n",
    "        'pmid': '99999999',\n",
    "        'title': 'Demo paper: Synaptic plasticity in hippocampal circuits',\n",
    "        'authors': 'Demo Author, A.; Demo Author, B.',\n",
    "        'journal': 'Demo Journal of Neuroscience',\n",
    "        'year': 2023,\n",
    "        'abstract': 'This is a demo abstract about hippocampal synaptic plasticity...',\n",
    "        'doi': '10.1234/demo.2023.001'\n",
    "    }\n",
    "]\n",
    "\n",
    "expanded_db = merge_publication_databases(publications, demo_new_pubs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df69724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing merge with full IFC database to evaluate method effectiveness\n",
      "\n",
      "‚úÖ Loaded full IFC database: 404 publications\n",
      "üìä Comparison Analysis:\n",
      "   Full IFC database: 404 publications\n",
      "   PubMed search found: 114 publications\n",
      "\n",
      "   Starting merge effectiveness analysis...\n",
      "   Building lookup tables for faster matching...\n",
      "\n",
      "   Processing publications:\n",
      "   Progress: 114/114 (100.0%)\n",
      "   Processing complete!                           \n",
      "\n",
      "üéØ Method Effectiveness Analysis:\n",
      "   üìà Publications found by PubMed: 114\n",
      "   ‚ú® Truly new publications: 89\n",
      "   üîÑ Duplicates found: 25\n",
      "      - By DOI: 23\n",
      "      - By PMID: 2\n",
      "      - By Title: 0\n",
      "   üìä Method effectiveness: 78.1% new content\n",
      "\n",
      "üìã Sample of new publications found:\n",
      "   1. Multistable bimodal perceptual coding within the ventral premotor cortex.\n",
      "      Journal: Science advances, Year: 2025\n",
      "      DOI: 10.1126/sciadv.adw5500\n",
      "\n",
      "   2. Inhibition of the oncogenic channel Kv10.1 by the antipsychotic drug penfluridol.\n",
      "      Journal: Frontiers in pharmacology, Year: 2025\n",
      "      DOI: 10.3389/fphar.2025.1655406\n",
      "\n",
      "   3. Interleukin-1Œ≤ depresses neuronal activity in the rat olfactory bulb even during odor stimulation.\n",
      "      Journal: PloS one, Year: 2025\n",
      "      DOI: 10.1371/journal.pone.0332592\n",
      "\n",
      "   4. Transcriptomic profiling of \n",
      "      Journal: Applied and environmental microbiology, Year: 2025\n",
      "      DOI: 10.1128/aem.01557-25\n",
      "\n",
      "   5. Globally accessible platforms for the exchange of research findings and career development.\n",
      "      Journal: Nature neuroscience, Year: 2025\n",
      "      DOI: 10.1038/s41593-025-02063-5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test merge with the full IFC database to evaluate effectiveness\n",
    "print(\"üß™ Testing merge with full IFC database to evaluate method effectiveness\\n\")\n",
    "\n",
    "# Load the full IFC database\n",
    "try:\n",
    "    with open('../data/raw/all_ifc_publications.json', 'r', encoding='utf-8') as f:\n",
    "        full_ifc_db = json.load(f)\n",
    "    print(f\"‚úÖ Loaded full IFC database: {len(full_ifc_db)} publications\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Could not find '../data/raw/all_ifc_publications.json'\")\n",
    "    print(\"   Please check the file path\")\n",
    "    full_ifc_db = []\n",
    "\n",
    "if full_ifc_db and new_articles:\n",
    "    print(f\"üìä Comparison Analysis:\")\n",
    "    print(f\"   Full IFC database: {len(full_ifc_db)} publications\")\n",
    "    print(f\"   PubMed search found: {len(new_articles)} publications\")\n",
    "    \n",
    "    # Test merge (in memory only)\n",
    "    def test_merge_effectiveness(existing_pubs, new_pubs):\n",
    "        \"\"\"Test merge to evaluate method effectiveness without saving\"\"\"\n",
    "        \n",
    "        # Create lookup sets for deduplication (do this once, not in the loop)\n",
    "        print(\"   Building lookup tables for faster matching...\")\n",
    "        existing_dois = {pub.get('doi') for pub in existing_pubs if pub.get('doi')}\n",
    "        existing_pmids = {pub.get('pubmed_id') for pub in existing_pubs if pub.get('pubmed_id')}\n",
    "        \n",
    "        # Pre-process titles once for performance\n",
    "        existing_titles = set()\n",
    "        for pub in existing_pubs:\n",
    "            if pub.get('title'):\n",
    "                existing_titles.add(pub.get('title', '').lower().strip())\n",
    "        \n",
    "        # Analyze new publications\n",
    "        new_count = 0\n",
    "        duplicate_by_doi = 0\n",
    "        duplicate_by_pmid = 0\n",
    "        duplicate_by_title = 0\n",
    "        truly_new = []\n",
    "        \n",
    "        # Process with progress indicator\n",
    "        print(\"\\n   Processing publications:\")\n",
    "        total_new = len(new_pubs)\n",
    "        update_interval = max(1, min(100, total_new // 10))  # Show 10 updates or every item for small sets\n",
    "        \n",
    "        for i, pub in enumerate(new_pubs):\n",
    "            # Show progress periodically\n",
    "            if i % update_interval == 0 or i == total_new - 1:\n",
    "                progress = (i + 1) / total_new * 100\n",
    "                print(f\"   Progress: {i+1}/{total_new} ({progress:.1f}%)\", end=\"\\r\")\n",
    "            \n",
    "            is_duplicate = False\n",
    "            duplicate_reason = \"\"\n",
    "            \n",
    "            # Check for duplicates with detailed tracking - check fastest methods first\n",
    "            if pub.get('doi') and pub['doi'] in existing_dois:\n",
    "                is_duplicate = True\n",
    "                duplicate_reason = \"DOI match\"\n",
    "                duplicate_by_doi += 1\n",
    "            elif pub.get('pmid') and pub['pmid'] in existing_pmids:\n",
    "                is_duplicate = True\n",
    "                duplicate_reason = \"PMID match\"\n",
    "                duplicate_by_pmid += 1\n",
    "            elif pub.get('title'):\n",
    "                title_lower = pub.get('title', '').lower().strip()\n",
    "                if title_lower in existing_titles:\n",
    "                    is_duplicate = True\n",
    "                    duplicate_reason = \"Title match\"\n",
    "                    duplicate_by_title += 1\n",
    "                \n",
    "            if not is_duplicate:\n",
    "                # This is a new publication\n",
    "                truly_new.append({\n",
    "                    'title': pub.get('title', ''),\n",
    "                    'authors': pub.get('authors', ''),\n",
    "                    'journal': pub.get('journal', ''),\n",
    "                    'year': pub.get('year'),\n",
    "                    'doi': pub.get('doi'),\n",
    "                    'pubmed_id': pub.get('pmid'),\n",
    "                    'abstract': pub.get('abstract', ''),\n",
    "                    'source': 'PubMed_search'\n",
    "                })\n",
    "                new_count += 1\n",
    "        \n",
    "        print(\"\\n   Processing complete!                           \")  # Clear progress line\n",
    "        \n",
    "        return {\n",
    "            'total_found': len(new_pubs),\n",
    "            'truly_new': new_count,\n",
    "            'truly_new_articles': truly_new,\n",
    "            'duplicates': {\n",
    "                'by_doi': duplicate_by_doi,\n",
    "                'by_pmid': duplicate_by_pmid,\n",
    "                'by_title': duplicate_by_title,\n",
    "                'total': len(new_pubs) - new_count\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Perform test merge\n",
    "    print(\"\\n   Starting merge effectiveness analysis...\")\n",
    "    merge_results = test_merge_effectiveness(full_ifc_db, new_articles)\n",
    "    \n",
    "    print(f\"\\nüéØ Method Effectiveness Analysis:\")\n",
    "    print(f\"   üìà Publications found by PubMed: {merge_results['total_found']}\")\n",
    "    print(f\"   ‚ú® Truly new publications: {merge_results['truly_new']}\")\n",
    "    print(f\"   üîÑ Duplicates found: {merge_results['duplicates']['total']}\")\n",
    "    print(f\"      - By DOI: {merge_results['duplicates']['by_doi']}\")\n",
    "    print(f\"      - By PMID: {merge_results['duplicates']['by_pmid']}\")\n",
    "    print(f\"      - By Title: {merge_results['duplicates']['by_title']}\")\n",
    "    \n",
    "    effectiveness_rate = (merge_results['truly_new'] / merge_results['total_found']) * 100 if merge_results['total_found'] > 0 else 0\n",
    "    print(f\"   üìä Method effectiveness: {effectiveness_rate:.1f}% new content\")\n",
    "    \n",
    "    # Show sample of new publications\n",
    "    if merge_results['truly_new'] > 0:\n",
    "        print(f\"\\nüìã Sample of new publications found:\")\n",
    "        sample_size = min(5, merge_results['truly_new'])\n",
    "        for i, pub in enumerate(merge_results['truly_new_articles'][:sample_size]):\n",
    "            print(f\"   {i+1}. {pub['title']}\")\n",
    "            print(f\"      Journal: {pub['journal']}, Year: {pub['year']}\")\n",
    "            print(f\"      DOI: {pub['doi']}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot perform analysis - missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b1128",
   "metadata": {},
   "source": [
    "## 6. Automated Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83401f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_pipeline_with_review(initial_json_path, pdf_dir, output_dir='../data/processed'):\n",
    "    \"\"\"Complete automated pipeline with affiliation review step\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting complete publication database expansion pipeline\\n\")\n",
    "    \n",
    "    # Step 1: Load existing data\n",
    "    print(\"üìÇ Step 1: Loading existing publications\")\n",
    "    with open(initial_json_path, 'r', encoding='utf-8') as f:\n",
    "        existing_pubs = json.load(f)\n",
    "    print(f\"   Loaded {len(existing_pubs)} existing publications\")\n",
    "    \n",
    "    # Step 2: Mine affiliations from PDFs and review\n",
    "    print(\"\\n\udd0d Step 2: Mining and reviewing affiliations from PDFs\")\n",
    "    review_results = analyze_pdfs_and_search_pubmed_with_review(\n",
    "        pdf_dir=pdf_dir,\n",
    "        output_dir=os.path.join(output_dir, 'affiliations'),\n",
    "        max_results_per_query=30\n",
    "    )\n",
    "    \n",
    "    new_articles = review_results.get('pubmed_articles', [])\n",
    "    print(f\"   Found {len(new_articles)} potential new articles\")\n",
    "    \n",
    "    # Step 3: Merge databases\n",
    "    print(\"\\nüîÑ Step 3: Merging and deduplicating databases\")\n",
    "    expanded_json_path = os.path.join(output_dir, 'expanded_ifc_publications.json')\n",
    "    final_db = merge_publication_databases(existing_pubs, new_articles, expanded_json_path)\n",
    "    \n",
    "    # Step 4: Create final BibTeX\n",
    "    print(\"\\nüìö Step 4: Creating final BibTeX file\")\n",
    "    final_bibtex_path = os.path.join(output_dir, 'final_ifc_publications.bib')\n",
    "    create_bibtex_from_publications(final_db, final_bibtex_path)\n",
    "\n",
    "    # Step 5: Create final BibTeX\n",
    "    print(\"\\nüìö Step 5: Creating final BibTeX file\")\n",
    "    final_bibtex_path = os.path.join(output_dir, 'final_ifc_publications.bib')\n",
    "    create_bibtex_from_publications(final_db, final_bibtex_path)\n",
    "    \n",
    "    # Step 6: Generate summary report\n",
    "    print(\"\\nüìä Step 6: Generating summary report\")\n",
    "    report = {\n",
    "        'pipeline_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'original_count': len(existing_pubs),\n",
    "        'pubmed_found': len(new_articles),\n",
    "        'final_count': len(final_db),\n",
    "        'new_additions': len(final_db) - len(existing_pubs),\n",
    "        'files_created': {\n",
    "            'expanded_json': expanded_json_path,\n",
    "            'bibtex_original': bibtex_path,\n",
    "            'bibtex_final': final_bibtex_path\n",
    "        },\n",
    "        'year_distribution': {},\n",
    "        'top_journals': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze year distribution\n",
    "    years = [pub.get('year') for pub in final_db if pub.get('year')]\n",
    "    year_counts = Counter(years)\n",
    "    report['year_distribution'] = dict(year_counts.most_common(10))\n",
    "    \n",
    "    # Analyze top journals\n",
    "    journals = [pub.get('journal') for pub in final_db if pub.get('journal')]\n",
    "    journal_counts = Counter(journals)\n",
    "    report['top_journals'] = dict(journal_counts.most_common(10))\n",
    "    \n",
    "    # Save report\n",
    "    report_path = os.path.join(output_dir, 'pipeline_report.json')\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pipeline complete! Summary:\")\n",
    "    print(f\"   üìä Original: {report['original_count']} publications\")\n",
    "    print(f\"   üÜï Added: {report['new_additions']} new publications\")\n",
    "    print(f\"   üìà Final: {report['final_count']} total publications\")\n",
    "    print(f\"   üìÑ Report saved: {report_path}\")\n",
    "    \n",
    "    return final_db, report\n",
    "\n",
    "final_database, pipeline_report = run_complete_pipeline_with_review('../data/raw/test_ifc_publications.json')\n",
    "\n",
    "print(\"\\nüéØ Pipeline ready! Uncomment the line above to run the complete workflow.\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Run this pipeline to expand your database\")\n",
    "print(\"2. Import the BibTeX files into Zotero to download PDFs\")\n",
    "print(\"3. Use the expanded JSON database for your ChromaDB embeddings\")\n",
    "print(\"4. Run affiliation mining on downloaded PDFs to find more variations\")\n",
    "print(\"5. Iterate to continuously expand your database\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
