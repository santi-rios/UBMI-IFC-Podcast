{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d64e1a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to path: /home/santi/Projects/UBMI-IFC-Podcast/src\n",
      "Current working directory: /home/santi/Projects/UBMI-IFC-Podcast/notebooks\n",
      "‚úì Successfully imported DatabaseExpansionPipeline\n",
      "‚úÖ Loaded en_core_web_sm\n",
      "‚úÖ Loaded en_core_web_sm\n",
      "‚úÖ Loaded es_core_news_sm\n",
      "‚úì Successfully created pipeline instance\n",
      "‚úÖ Loaded es_core_news_sm\n",
      "‚úì Successfully created pipeline instance\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to Python path\n",
    "project_root = os.path.dirname(os.path.abspath(''))\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "print(f\"Added to path: {src_path}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Now import the pipeline\n",
    "try:\n",
    "    from pipeline.workflow import DatabaseExpansionPipeline\n",
    "    print(\"‚úì Successfully imported DatabaseExpansionPipeline\")\n",
    "    \n",
    "    # Test instantiation\n",
    "    pipeline = DatabaseExpansionPipeline()\n",
    "    print(\"‚úì Successfully created pipeline instance\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Import error: {e}\")\n",
    "    print(f\"Available in sys.path: {sys.path}\")\n",
    "    \n",
    "    # Try to list what's actually available\n",
    "    import os\n",
    "    if os.path.exists(src_path):\n",
    "        print(f\"Contents of src directory:\")\n",
    "        for item in os.listdir(src_path):\n",
    "            print(f\"  - {item}\")\n",
    "        \n",
    "        pipeline_path = os.path.join(src_path, 'pipeline')\n",
    "        if os.path.exists(pipeline_path):\n",
    "            print(f\"Contents of pipeline directory:\")\n",
    "            for item in os.listdir(pipeline_path):\n",
    "                print(f\"  - {item}\")\n",
    "    else:\n",
    "        print(f\"src directory doesn't exist at: {src_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289fc4d3",
   "metadata": {},
   "source": [
    "## üîß Refactored Modular Structure\n",
    "\n",
    "This notebook has been refactored to use a clean modular architecture. The code is now organized into:\n",
    "\n",
    "- **`pdf_acquisition/`**: Download research papers (DirectDownloader, PyPaperBotWrapper)\n",
    "- **`publication_management/`**: Handle BibTeX and database operations (BibTexManager, PublicationDatabase)\n",
    "- **`text_extraction/`**: Extract text from PDF files (PDFTextExtractor)\n",
    "- **`affiliation_mining/`**: Mine institutional affiliations (EnhancedAffiliationMiner, AffiliationClustering)  \n",
    "- **`pubmed/`**: Search and retrieve PubMed articles (EnhancedPubmedSearcher)\n",
    "- **`data_quality/`**: Keyword extraction and classification (KeywordExtractor, PublicationClassifier)\n",
    "- **`pipeline/`**: Main workflow orchestration (DatabaseExpansionPipeline)\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "You can now use either:\n",
    "1. **Individual modules** for specific tasks\n",
    "2. **Pipeline class** for complete workflows\n",
    "\n",
    "Below are examples of both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa5337c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Example 1: Using individual modules\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PublicationDatabase' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìñ Example 1: Using individual modules\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load publications using the database module\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m db_manager \u001b[38;5;241m=\u001b[39m \u001b[43mPublicationDatabase\u001b[49m()\n\u001b[1;32m      6\u001b[0m publications \u001b[38;5;241m=\u001b[39m db_manager\u001b[38;5;241m.\u001b[39mload_publications(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/test_ifc_publications.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(publications)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m publications using PublicationDatabase module\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PublicationDatabase' is not defined"
     ]
    }
   ],
   "source": [
    "# Example 1: Using individual modules\n",
    "print(\"üìñ Example 1: Using individual modules\")\n",
    "\n",
    "# Load publications using the database module\n",
    "db_manager = PublicationDatabase()\n",
    "publications = db_manager.load_publications('../data/raw/test_ifc_publications.json')\n",
    "print(f\"Loaded {len(publications)} publications using PublicationDatabase module\")\n",
    "\n",
    "# Create BibTeX using the bibtex module  \n",
    "bibtex_manager = BibTexManager()\n",
    "bibtex_file = bibtex_manager.create_bibtex_from_publications(\n",
    "    publications, \n",
    "    '../data/processed/modular_example.bib'\n",
    ")\n",
    "print(f\"Created BibTeX file: {bibtex_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Example 2: Using the complete pipeline\n",
    "print(\"üöÄ Example 2: Using the complete pipeline\")\n",
    "print(\"This would run the entire workflow:\")\n",
    "print(\"1. Load existing publications\")\n",
    "print(\"2. Mine affiliations from PDFs\") \n",
    "print(\"3. Search PubMed with discovered affiliations\")\n",
    "print(\"4. Merge and deduplicate databases\")\n",
    "print(\"5. Generate final BibTeX and reports\")\n",
    "\n",
    "# Uncomment the following line to run the complete pipeline:\n",
    "# final_db, report = pipeline.run_complete_pipeline_with_review(\n",
    "#     initial_json_path='../data/raw/all_ifc_publications.json',\n",
    "#     pdf_dir='../papers/downloaded',\n",
    "#     output_dir='../data/processed'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec49f14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Original Implementation (Legacy Code)\n",
    "\n",
    "The code below represents the original notebook implementation that has been refactored into the modular structure above. This code is preserved for reference and comparison purposes.\n",
    "\n",
    "**Note**: You can now replace the following lengthy code blocks with simple module imports and function calls as shown in the examples above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c74b3",
   "metadata": {},
   "source": [
    "# Building IFC Publications Database\n",
    "\n",
    "This notebook implements multiple strategies to build a comprehensive database of Instituto de Fisiolog√≠a Celular publications:\n",
    "\n",
    "1. **PDF Acquisition**: Sci-Hub integration + BibTeX export for Zotero\n",
    "2. **Affiliation Mining**: Extract all variations of institute names from existing PDFs\n",
    "3. **PubMed Search Strategy**: Use discovered affiliations to find more papers\n",
    "4. **Database Expansion**: Automated workflow to grow the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78786ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import bibtexparser\n",
    "from bibtexparser.bwriter import BibTexWriter\n",
    "from bibtexparser.bibdatabase import BibDatabase\n",
    "from pypdf import PdfReader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b7c3c",
   "metadata": {},
   "source": [
    "## 1. Load Existing Publications Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3866a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 404 publications\n",
      "Sample publication:\n",
      "{\n",
      "  \"title\": \"An automatic representation of peptides for effective antimicrobial activity classification\",\n",
      "  \"authors\": \"Beltran, J. A., Del Rio, G., & Brizuela, C. A.\",\n",
      "  \"journal\": \"Computational and Structural Biotechnology Journal, 18, 455?463\",\n",
      "  \"year\": 2020,\n",
      "  \"doi\": \"10.1016/j.csbj.2020.02.002\",\n",
      "  \"pubmed_id\": \"32180904\",\n",
      "  \"ifc_url\": \"https://www.ifc.unam.mx/publicacion.php?ut=000607742800020\",\n",
      "  \"abstract\": \"ABSTRACTAntimicrobial peptides (AMPs) are a promising alternative to small-molecules-based antibiotics. These peptides are part of most living organisms' innate defense system. In order to computationally identify new AMPs within the peptides these organisms produce, an automatic AMP/non-AMP classifier is required. In order to have an efficient classifier, a set of robust features that can capture what differentiates an AMP from another that is not, has to be selected. However, the number of candidate descriptors is large (in the order of thousands) to allow for an exhaustive search of all possible combinations. Therefore, efficient and effective feature selection techniques are required. In this work, we propose an efficient wrapper technique to solve the feature selection problem for AMPs identification. The method is based on a Genetic Algorithm that uses a variable-length chromosome for representing the selected features and uses an objective function that considers the Mathew Correlation Coefficient and the number of selected features. Computational experiments show that the proposed method can produce competitive results regarding sensitivity, specificity, and MCC. Furthermore, the best classification results are achieved by using only 39 out of 272 molecular descriptors.\",\n",
      "  \"keywords\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load your existing publications\n",
    "with open('../data/raw/all_ifc_publications.json', 'r', encoding='utf-8') as f:\n",
    "    publications = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(publications)} publications\")\n",
    "print(\"Sample publication:\")\n",
    "print(json.dumps(publications[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e9272",
   "metadata": {},
   "source": [
    "## 2. PDF Acquisition Strategy\n",
    "\n",
    "### Option A: Sci-Hub Integration (own implementation)\n",
    "\n",
    "> ‚ö†Ô∏è USE OPTION C\n",
    "\n",
    "- Not tested for CAPTCHAs\n",
    "- Use method B or C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc8f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_doi_download(dois, output_dir='../papers/downloaded/direct'):\n",
    "    \"\"\"Directly download papers from Sci-Hub using DOIs\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # List of Sci-Hub mirrors to try\n",
    "    mirrors = [\n",
    "        \"https://sci-hub.se/\",\n",
    "        \"https://sci-hub.st/\",\n",
    "        \"https://sci-hub.ru/\",\n",
    "        # Add more mirrors as needed\n",
    "    ]\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    })\n",
    "    \n",
    "    success_count = 0\n",
    "    \n",
    "    for doi in dois:\n",
    "        print(f\"Downloading DOI: {doi}\")\n",
    "        \n",
    "        for mirror in mirrors:\n",
    "            try:\n",
    "                url = f\"{mirror}{doi}\"\n",
    "                response = session.get(url, timeout=10)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Check if response is PDF\n",
    "                    if 'application/pdf' in response.headers.get('content-type', ''):\n",
    "                        # Save PDF\n",
    "                        filename = f\"{doi.replace('/', '_')}.pdf\"\n",
    "                        filepath = os.path.join(output_dir, filename)\n",
    "                        \n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            f.write(response.content)\n",
    "                            \n",
    "                        print(f\"‚úÖ Downloaded to {filepath}\")\n",
    "                        success_count += 1\n",
    "                        break  # Move to next DOI after successful download\n",
    "                    else:\n",
    "                        # Handle HTML response (Sci-Hub page)\n",
    "                        # You'd need a more sophisticated parser to extract the PDF link from the HTML\n",
    "                        pass\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Failed with {mirror}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        time.sleep(2)  # Be respectful\n",
    "    \n",
    "    print(f\"\\nDownload summary: {success_count}/{len(dois)} papers downloaded\")\n",
    "    return success_count\n",
    "\n",
    "# Usage example\n",
    "print(\"\\nüìÑ Trying direct download approach...\")\n",
    "direct_doi_download(sample_dois)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4aaf7",
   "metadata": {},
   "source": [
    "### Option B: BibTeX Export for Zotero\n",
    "\n",
    "> ‚ö†Ô∏è USE OPTION C\n",
    "\n",
    "> Use one of the multiple zotero -> sci-hub plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c94b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing improved BibTeX creation...\n",
      "üìö Created BibTeX file with 404 entries: ../data/processed/all_ifc_publications.bib\n",
      "Import this file into Zotero to download PDFs automatically\n",
      "\n",
      "üîç Sample author formatting:\n",
      "1. Original: Beltran, J. A., Del Rio, G., & Brizuela, C. A.\n",
      "   BibTeX:   Beltran and J. A. and Del Rio, G. and & Brizuela and C. A.\n",
      "2. Original: Mart√≠nez?Gonz√°lez, K., Islas?Hern√°ndez, A., Mart√≠nez?Ezquerro, J. D., Berm√∫dez?Rattoni, F., & Garcia?delaTorre, P.\n",
      "   BibTeX:   Mart√≠nez?Gonz√°lez, K. and Islas?Hern√°ndez, A. and Mart√≠nez?Ezquerro and J. D. and Berm√∫dez?Rattoni, F. and & Garcia?delaTorre, P.\n",
      "3. Original: Yin, W., Cerda-Hern√°ndez, N., Castillo-Morales, A., Ruiz-Tejada-Segura, M. L., Monz√≥n-Sandoval, J., Moreno-Castilla, P., ? Guti√©rrez, H.\n",
      "   BibTeX:   Yin, W. and Cerda-Hern√°ndez, N. and Castillo-Morales, A. and Ruiz-Tejada-Segura and M. L. and Monz√≥n-Sandoval, J. and Moreno-Castilla, P. and ? Guti√©rrez, H.\n",
      "\n",
      "BibTeX file created at: ../data/processed/all_ifc_publications.bib\n",
      "\n",
      "üìÑ Sample BibTeX entries:\n",
      "@article{Abbruzzini2021_ifc_235,\n",
      " abstract = {ABSTRACTPurpose: The production of Technosols is a sustainable strategy to reuse urban wastes and to regenerate degraded sites. However, little is known regarding the role of the activity of enzymes associated with carbon and nutrients cycling on organic degradation and microbial activity in these soils. Methods: A controlled experiment was conducted with Technosols made from construction wastes, wood chips, and compost or compost plus biochar, in order to evaluate their organic matter (OM) degradation potential and functioning through the activity of enzymes and microbial community composition. Results: The Technosols had organic carbon contents from 13 to 30¬†g¬†kg‚àí1, carbon-to-nitrogen ratio from 10 to 20, and available phosphorus from 92 to 376¬†mg¬†kg‚àí1. The Technosols with biochar and compost had alkaline pH and higher contents of organic carbon and available phosphorus compared to Technosols with compost alone. The mixture of wood chips and compost presented the highest enzyme activities, and might be the most appropriate for Technosol‚Äôs production. The mixture of concrete and excavation waste with compost and compost plus biochar displayed a potential for OM decomposition comparable to that of wood chips with compost plus biochar. These results suggest that the bacterial and archaeal fingerprint is similar among the Technosols, although differences are observed in the relative abundances of their taxa. Conclusions: Substrate composition affects the processes of OM transformation, microbial biomass activity, and composition. The mixture of wood chips and compost presented the highest enzyme activities during the incubation period, and might be the most appropriate for its application as a Technosol. The mixture of concrete and excavation waste with either compost or compost plus biochar displayed a potential for organic matter decomposition that was comparable to that of the mixture of wood chips with compost plus biochar. The microbial communities in these Technosols are not significantly different yet, but the bioavailability of nutrients derived from the changes in the soil matrix (by adding construction waste and biochar) is influencing soil enzymatic activity.},\n",
      " author = {Abbruzzini and T. F. and Reyes-Ortigoza and A. L. and Alc√°ntara-Hern√°ndez and R. J. and Mora, L. and Flores, L. and & Prado, B.},\n",
      " doi = {10.1007/s11368-021-03062-2},\n",
      " journal = {Journal of Soils and Sediments},\n",
      " note = {Instituto de Fisiolog√≠a Celular, UNAM},\n",
      " title = {Chemical, biochemical, and microbiological properties of Technosols produced from urban inorganic and organic wastes},\n",
      " url = {https://www.ifc.unam.mx/publicacion.php?scopus=85114778157},\n",
      " year = {2021}\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_bibtex_from_publications(publications, output_file='../data/processed/all_ifc_publications.bib'):\n",
    "    \"\"\"Convert JSON publications to BibTeX format for Zotero import\"\"\"\n",
    "    \n",
    "    db = BibDatabase()\n",
    "    entries = []\n",
    "    \n",
    "    def format_authors_for_bibtex(author_string):\n",
    "        \"\"\"Convert author string to proper BibTeX format\"\"\"\n",
    "        if not author_string:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Split by commas and clean each author\n",
    "        authors = [author.strip() for author in author_string.split(',')]\n",
    "        \n",
    "        # Group authors (assuming they come in pairs: LastName, FirstName)\n",
    "        formatted_authors = []\n",
    "        i = 0\n",
    "        while i < len(authors):\n",
    "            if i + 1 < len(authors):\n",
    "                # Check if next item looks like a first name (short, no hyphens typically)\n",
    "                next_item = authors[i + 1].strip()\n",
    "                if (len(next_item) <= 3 or \n",
    "                    (len(next_item.split()) == 1 and '.' in next_item) or\n",
    "                    re.match(r'^[A-Z]\\.?$', next_item)):\n",
    "                    # This is likely a first name/initial\n",
    "                    last_name = authors[i].strip()\n",
    "                    first_name = next_item\n",
    "                    formatted_authors.append(f\"{last_name}, {first_name}\")\n",
    "                    i += 2\n",
    "                else:\n",
    "                    # This is likely a full name or last name only\n",
    "                    formatted_authors.append(authors[i].strip())\n",
    "                    i += 1\n",
    "            else:\n",
    "                # Last author, no pair\n",
    "                formatted_authors.append(authors[i].strip())\n",
    "                i += 1\n",
    "        \n",
    "        # Join with \" and \" for BibTeX format\n",
    "        return \" and \".join(formatted_authors)\n",
    "    \n",
    "    for i, pub in enumerate(publications):\n",
    "        # Create a unique citation key\n",
    "        first_author = pub['authors'].split(',')[0].strip() if pub['authors'] else 'Unknown'\n",
    "        first_author_clean = re.sub(r'[^a-zA-Z]', '', first_author)\n",
    "        citation_key = f\"{first_author_clean}{pub['year']}_ifc_{i}\"\n",
    "        \n",
    "        # Format authors properly for BibTeX\n",
    "        formatted_authors = format_authors_for_bibtex(pub['authors'])\n",
    "        \n",
    "        entry = {\n",
    "            'ENTRYTYPE': 'article',\n",
    "            'ID': citation_key,\n",
    "            'title': pub['title'],\n",
    "            'author': formatted_authors,  # Now properly formatted\n",
    "            'journal': pub['journal'],\n",
    "            'year': str(pub['year']),\n",
    "            'abstract': pub.get('abstract', ''),\n",
    "            'url': pub.get('ifc_url', ''),\n",
    "            'note': 'Instituto de Fisiolog√≠a Celular, UNAM'\n",
    "        }\n",
    "        \n",
    "        if pub.get('doi'):\n",
    "            entry['doi'] = pub['doi']\n",
    "            \n",
    "        if pub.get('pubmed_id'):\n",
    "            entry['pmid'] = pub['pubmed_id']\n",
    "            \n",
    "        entries.append(entry)\n",
    "    \n",
    "    db.entries = entries\n",
    "    \n",
    "    # Write to file\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    writer = BibTexWriter()\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(writer.write(db))\n",
    "    \n",
    "    print(f\"üìö Created BibTeX file with {len(entries)} entries: {output_file}\")\n",
    "    print(\"Import this file into Zotero to download PDFs automatically\")\n",
    "    \n",
    "    # Show sample formatted authors for verification\n",
    "    print(\"\\nüîç Sample author formatting:\")\n",
    "    for i, entry in enumerate(entries[:3]):\n",
    "        print(f\"{i+1}. Original: {publications[i]['authors']}\")\n",
    "        print(f\"   BibTeX:   {entry['author']}\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# Test the improved function\n",
    "print(\"üîß Testing improved BibTeX creation...\")\n",
    "bibtex_file = create_bibtex_from_publications(publications)\n",
    "print(f\"\\nBibTeX file created at: {bibtex_file}\")\n",
    "\n",
    "# Let's also check the actual BibTeX content\n",
    "print(\"\\nüìÑ Sample BibTeX entries:\")\n",
    "with open(bibtex_file, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    # Show first entry\n",
    "    first_entry_end = content.find('\\n}\\n') + 3\n",
    "    print(content[:first_entry_end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad48f81",
   "metadata": {},
   "source": [
    "### Option C: PyPaperBot\n",
    "\n",
    "- [Repo](https://github.com/ferru97/PyPaperBot)\n",
    "\n",
    "- Download papers given a query\n",
    "- Download papers given paper's DOIs\n",
    "- Generate Bibtex of the downloaded paper\n",
    "- Filter downloaded paper by year, journal and citations number\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "- Multiple Download Methods: DOI-based downloads (most reliable)\n",
    "- Google Scholar queries\n",
    "- BibTeX-only generation\n",
    "- Flexible Modes: Download PDFs only, BibTeX only, or both\n",
    "- IFC-Specific Queries: Pre-configured searches for the institute\n",
    "- Deduplication: Automatic removal of duplicate downloads\n",
    "- Rate Limiting: Respectful delays between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a40a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPaperBot in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (1.4.1)\n",
      "Requirement already satisfied: undetected-chromedriver in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (3.5.5)\n",
      "Requirement already satisfied: astroid<=2.5,>=2.4.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (4.13.5)\n",
      "Requirement already satisfied: bibtexparser>=1.2.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (1.4.3)\n",
      "Requirement already satisfied: certifi>=2020.6.20 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2025.8.3)\n",
      "Requirement already satisfied: chardet>=3.0.4 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (5.2.0)\n",
      "Requirement already satisfied: colorama>=0.4.3 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (0.4.6)\n",
      "Requirement already satisfied: crossref-commons>=0.0.7 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (0.0.7)\n",
      "Requirement already satisfied: future>=0.18.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (1.0.0)\n",
      "Requirement already satisfied: HTMLParser>=0.0.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (0.0.2)\n",
      "Requirement already satisfied: idna<3,>=2.10 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.10)\n",
      "Requirement already satisfied: isort>=5.4.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (5.13.2)\n",
      "Requirement already satisfied: lazy-object-proxy>=1.4.3 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (1.12.0)\n",
      "Requirement already satisfied: mccabe>=0.6.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (0.6.1)\n",
      "Requirement already satisfied: numpy in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.2.6)\n",
      "Requirement already satisfied: pandas in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.3.2)\n",
      "Requirement already satisfied: pyChainedProxy>=1.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (1.3)\n",
      "Requirement already satisfied: pylint>=2.6.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.7.1)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2025.2)\n",
      "Requirement already satisfied: ratelimit>=2.2.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.24.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.32.5)\n",
      "Requirement already satisfied: six>=1.15.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>=2.0.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.8)\n",
      "Requirement already satisfied: toml>=0.10.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (0.10.2)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.12.1 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from PyPaperBot) (1.12.1)\n",
      "Requirement already satisfied: selenium>=4.9.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from undetected-chromedriver) (4.35.0)\n",
      "Requirement already satisfied: websockets in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from beautifulsoup4>=4.9.1->PyPaperBot) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from requests>=2.24.0->PyPaperBot) (3.4.3)\n",
      "Requirement already satisfied: trio~=0.30.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from trio~=0.30.0->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from trio~=0.30.0->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: outcome in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from trio~=0.30.0->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from trio~=0.30.0->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from trio~=0.30.0->selenium>=4.9.0->undetected-chromedriver) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from trio-websocket~=0.12.2->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from pandas->PyPaperBot) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install PyPaperBot undetected-chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596d171",
   "metadata": {},
   "source": [
    "Chrome Installation Check\n",
    "\n",
    "(required by undetected_chromedriver):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f8fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Chrome/Chromium not found. Please install it for PyPaperBot to work properly.\n",
      "   On Ubuntu/Debian: sudo apt install chromium-browser\n",
      "   On Fedora: sudo dnf install chromium\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_chrome_installed():\n",
    "    \"\"\"Check if Chrome/Chromium is installed on the system\"\"\"\n",
    "    chrome_paths = [\n",
    "        \"google-chrome\",\n",
    "        \"chromium-browser\",\n",
    "        \"chromium\",\n",
    "        \"/usr/bin/google-chrome\",\n",
    "        \"/usr/bin/chromium-browser\"\n",
    "    ]\n",
    "    \n",
    "    for path in chrome_paths:\n",
    "        try:\n",
    "            result = subprocess.run([\"which\", path], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"‚úÖ Chrome/Chromium found at: {result.stdout.strip()}\")\n",
    "                return True\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(\"‚ùå Chrome/Chromium not found. Please install it for PyPaperBot to work properly.\")\n",
    "    print(\"   On Ubuntu/Debian: sudo apt install chromium-browser\")\n",
    "    print(\"   On Fedora: sudo dnf install chromium\")\n",
    "    return False\n",
    "\n",
    "# Check if Chrome is installed\n",
    "check_chrome_installed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84fb687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 404 publications\n",
      "Found 402 DOIs to process\n",
      "\n",
      "üì• Test 1: Downloading PDFs only\n",
      "Executing command: python -m PyPaperBot --doi-file ../papers/downloaded/pdf_only/temp_dois.txt --dwn-dir ../papers/downloaded/pdf_only --restrict 1 --use-doi-as-filename\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 88\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# print(f\"Sample DOIs: {sample_dois}\")\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Run PyPaperBot in different modes\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müì• Test 1: Downloading PDFs only\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m pdf_success \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_with_pypaperbot\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_dois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../papers/downloaded/pdf_only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìö Test 2: Generating BibTeX only\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# bibtex_success = download_with_pypaperbot(sample_dois, output_dir='../papers/downloaded/bibtex_only', mode=0)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 58\u001b[0m, in \u001b[0;36mdownload_with_pypaperbot\u001b[0;34m(dois, output_dir, min_year, mode, use_doi_filename)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting command: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cmd)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Print output\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1154\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1154\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:2021\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2015\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2016\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2019\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2021\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "\n",
    "def download_with_pypaperbot(dois=None, output_dir='../papers/downloaded', \n",
    "                           min_year=None, mode=2, use_doi_filename=True):\n",
    "    \"\"\"\n",
    "    Download papers using PyPaperBot command-line interface with dependency checking\n",
    "    \n",
    "    Args:\n",
    "        dois: List of DOIs or None\n",
    "        output_dir: Where to save outputs\n",
    "        min_year: Minimum publication year\n",
    "        mode: 0=BibTeX only, 1=PDF only, 2=both\n",
    "        use_doi_filename: Use DOI as filename instead of paper title\n",
    "    \"\"\"\n",
    "    # Check for required dependencies\n",
    "    try:\n",
    "        import importlib\n",
    "        if importlib.util.find_spec(\"undetected_chromedriver\") is None:\n",
    "            print(\"Installing missing dependency: undetected-chromedriver\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"undetected-chromedriver\"])\n",
    "            print(\"Dependency installed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not verify/install dependencies: {e}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Base command\n",
    "    cmd = [\"python\", \"-m\", \"PyPaperBot\"]\n",
    "    \n",
    "    # Add arguments based on parameters\n",
    "    if dois:\n",
    "        # For multiple DOIs, create a temporary DOI file\n",
    "        doi_file = os.path.join(output_dir, \"temp_dois.txt\")\n",
    "        with open(doi_file, 'w') as f:\n",
    "            f.write('\\n'.join(dois))\n",
    "        cmd.extend([\"--doi-file\", doi_file])\n",
    "    \n",
    "    # Add output directory\n",
    "    cmd.extend([\"--dwn-dir\", output_dir])\n",
    "    \n",
    "    # Add optional parameters\n",
    "    if min_year:\n",
    "        cmd.extend([\"--min-year\", str(min_year)])\n",
    "    \n",
    "    # Add mode (restrict parameter)\n",
    "    cmd.extend([\"--restrict\", str(mode)])\n",
    "    \n",
    "    # Use DOI as filename if requested\n",
    "    if use_doi_filename:\n",
    "        cmd.append(\"--use-doi-as-filename\")\n",
    "    \n",
    "    # Execute command\n",
    "    print(f\"Executing command: {' '.join(cmd)}\")\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        # Print output\n",
    "        print(\"\\nOutput:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        if result.stderr:\n",
    "            print(\"\\nErrors:\")\n",
    "            print(result.stderr)\n",
    "            \n",
    "        return result.returncode == 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing PyPaperBot: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test with a small sample from the publications\n",
    "print(f\"Working with {len(publications)} publications\")\n",
    "\n",
    "# Extract DOIs for PyPaperBot\n",
    "dois = [pub.get('doi') for pub in publications if pub.get('doi')]\n",
    "print(f\"Found {len(dois)} DOIs to process\")\n",
    "\n",
    "# Test with a small sample (2)\n",
    "# print(\"\\nüß™ Testing PyPaperBot with 2 sample publications...\")\n",
    "# sample_dois = dois[:2]\n",
    "sample_dois = dois\n",
    "# print(f\"Sample DOIs: {sample_dois}\")\n",
    "\n",
    "# Run PyPaperBot in different modes\n",
    "print(\"\\nüì• Test 1: Downloading PDFs only\")\n",
    "pdf_success = download_with_pypaperbot(sample_dois, output_dir='../papers/downloaded/pdf_only', mode=1)\n",
    "\n",
    "print(\"\\nüìö Test 2: Generating BibTeX only\")\n",
    "# bibtex_success = download_with_pypaperbot(sample_dois, output_dir='../papers/downloaded/bibtex_only', mode=0)\n",
    "\n",
    "print(\"\\nüîç Test 3: Downloading both PDF and BibTeX\")\n",
    "# combined_success = download_with_pypaperbot(sample_dois, output_dir='../papers/downloaded/combined', mode=2)\n",
    "\n",
    "print(\"\\nüìä Download Summary:\")\n",
    "print(f\"   Sample DOIs processed: {len(sample_dois)}\")\n",
    "print(f\"   PDF download successful: {pdf_success}\")\n",
    "print(f\"   BibTeX download successful: {bibtex_success}\")\n",
    "print(f\"   Combined download successful: {combined_success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e77b2",
   "metadata": {},
   "source": [
    "def run_complete_pipeline(initial_json_path, output_dir='../data/processed'):\n",
    "    \"\"\"Complete automated pipeline to expand publication database\"\"\"\n",
    "    \n",
    "    # ... existing code ...\n",
    "    \n",
    "    # Step 3b: Download PDFs for existing publications\n",
    "    print(\"\\nüì• Step 3b: Downloading PDFs using PyPaperBot\")\n",
    "    dois = [pub.get('doi') for pub in existing_pubs if pub.get('doi')]\n",
    "    pdf_dir = os.path.join(output_dir, 'pdfs')\n",
    "    download_success = download_with_pypaperbot(\n",
    "        dois, \n",
    "        output_dir=pdf_dir,\n",
    "        mode=1  # PDF only\n",
    "    )\n",
    "    print(f\"   PDF download {'successful' if download_success else 'failed'}\")\n",
    "    print(f\"   Check output directory: {pdf_dir}\")\n",
    "    \n",
    "    # ... rest of existing pipeline ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1/1 (5 DOIs)\n",
      "Executing command: python -m PyPaperBot --doi-file papers/mining/01_run1/batch_1/temp_dois.txt --dwn-dir papers/mining/01_run1/batch_1 --restrict 1 --use-doi-as-filename\n",
      "\n",
      "Output:\n",
      "PyPaperBot v1.4.1\n",
      "PyPaperBot is a Python tool for downloading scientific papers using Google Scholar, Crossref and SciHub.\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "Downloading papers from DOIs\n",
      "\n",
      "Searching paper 1 of 5 with DOI 10.1016/j.cell.2025.03.050\n",
      "Searching paper 2 of 5 with DOI 10.1523/JNEUROSCI.1234-24.2024\n",
      "Searching paper 3 of 5 with DOI 10.1073/pnas.2420356122\n",
      "Searching paper 4 of 5 with DOI 10.1364/ol.547539\n",
      "Searching paper 5 of 5 with DOI 10.1016/j.neulet.2025.138361 \n",
      "Searching for a sci-hub mirror\n",
      "Trying with https://sci-hub.ee...\n",
      "\n",
      "Using Sci-Hub mirror https://sci-hub.ee\n",
      "Using Sci-DB mirror https://annas-archive.se/scidb/\n",
      "You can use --scidb-mirror and --scidb-mirror to specify your're desired mirror URL\n",
      "\n",
      "Download 1 of 5 -> Snake venom protection by a cocktail of varespladib and broadly neutralizing human antibodies\n",
      "Download 2 of 5 -> Challenges and Approaches in the Study of Neural Entrainment\n",
      "Download 3 of 5 -> Contextual neural dynamics during time perception in the primate ventral premotor cortex\n",
      "Download 4 of 5 -> Orbital angular momentum coherent state beams\n",
      "Download 5 of 5 -> Computational modeling of orthodromically evoked synaptic potentials of striatal projection neurons of direct and indirect pathways\n",
      "\n",
      "Work completed!\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "\n",
      "\n",
      "Completed 1/1 batches successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bulk_download_with_pypaperbot(all_dois, output_dir, chunk_size=50):\n",
    "    \"\"\"Download papers in chunks to avoid overwhelming the system\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process in chunks\n",
    "    success_count = 0\n",
    "    total_chunks = (len(all_dois) + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for i in range(0, len(all_dois), chunk_size):\n",
    "        chunk = all_dois[i:i+chunk_size]\n",
    "        chunk_num = (i // chunk_size) + 1\n",
    "        \n",
    "        print(f\"\\nProcessing chunk {chunk_num}/{total_chunks} ({len(chunk)} DOIs)\")\n",
    "        chunk_dir = os.path.join(output_dir, f\"batch_{chunk_num}\")\n",
    "        \n",
    "        if download_with_pypaperbot(chunk, output_dir=chunk_dir, mode=1):\n",
    "            success_count += 1\n",
    "            \n",
    "        # Add delay between chunks\n",
    "        if chunk_num < total_chunks:\n",
    "            print(\"Waiting before next batch...\")\n",
    "            time.sleep(30)  # 30 second delay between batches\n",
    "    \n",
    "    print(f\"\\nCompleted {success_count}/{total_chunks} batches successfully\")\n",
    "    return success_count == total_chunks\n",
    "\n",
    "dois_test=['10.1016/j.cell.2025.03.050', '10.1523/JNEUROSCI.1234-24.2024', '10.1073/pnas.2420356122', '10.1364/ol.547539', '10.1016/j.neulet.2025.138361 ']\n",
    "\n",
    "bulk_download_with_pypaperbot(dois_test, 'papers/mining/01_run1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94538dba",
   "metadata": {},
   "source": [
    "Testing PyPaperBot with Real IFC Publications\n",
    "\n",
    "> NOTE: uses above `bulk_download_with_pypaperbot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a3d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing PyPaperBot with 5 real publications\n",
      "‚úÖ Loaded 404 publications from /home/santi/Projects/UBMI-IFC-Podcast/data/raw/all_ifc_publications.json\n",
      "üìä Found 402 publications with DOIs\n",
      "\n",
      "üìù Selected 5 DOIs for testing:\n",
      "   1. 10.3389/ffunb.2024.1378590\n",
      "   2. 10.1371/journal.pone.0242749\n",
      "   3. 10.1016/j.neuroscience.2020.08.025\n",
      "   4. 10.1016/j.arcmed.2021.09.001\n",
      "   5. 10.3390/ijms25126491\n",
      "\n",
      "üì• Downloading PDFs to ../papers/test_downloads\n",
      "\n",
      "Processing chunk 1/1 (5 DOIs)\n",
      "Executing command: python -m PyPaperBot --doi-file ../papers/test_downloads/test_run_20250921_205321/batch_1/temp_dois.txt --dwn-dir ../papers/test_downloads/test_run_20250921_205321/batch_1 --restrict 1 --use-doi-as-filename\n",
      "\n",
      "Output:\n",
      "PyPaperBot v1.4.1\n",
      "PyPaperBot is a Python tool for downloading scientific papers using Google Scholar, Crossref and SciHub.\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "Downloading papers from DOIs\n",
      "\n",
      "Searching paper 1 of 5 with DOI 10.3389/ffunb.2024.1378590\n",
      "Searching paper 2 of 5 with DOI 10.1371/journal.pone.0242749\n",
      "Searching paper 3 of 5 with DOI 10.1016/j.neuroscience.2020.08.025\n",
      "Searching paper 4 of 5 with DOI 10.1016/j.arcmed.2021.09.001\n",
      "Searching paper 5 of 5 with DOI 10.3390/ijms25126491\n",
      "Searching for a sci-hub mirror\n",
      "Trying with https://sci-hub.ee...\n",
      "\n",
      "Using Sci-Hub mirror https://sci-hub.ee\n",
      "Using Sci-DB mirror https://annas-archive.se/scidb/\n",
      "You can use --scidb-mirror and --scidb-mirror to specify your're desired mirror URL\n",
      "\n",
      "Download 1 of 5 -> In Rhodotorula mucilaginosa, active oxidative metabolism increases carotenoids to inactivate excess reactive oxygen species\n",
      "Download 2 of 5 -> Heterogeneous expression of CFTR in insulin-secreting Œ≤-cells of the normal human islet\n",
      "Download 3 of 5 -> Spontaneous Activity of Neuronal Ensembles in Mouse Motor Cortex: Changes after GABAergic Blockade\n",
      "Download 4 of 5 -> On the Right Track to Decoding the Enigma of Sepsis\n",
      "Download 5 of 5 -> Lysophosphatidic Acid Receptor 3 (LPA3): Signaling and Phosphorylation Sites\n",
      "\n",
      "Work completed!\n",
      "        -Join the telegram channel to stay updated --> https://t.me/pypaperbotdatawizards <--\n",
      "        -If you like this project, you can share a cup of coffee at --> https://www.paypal.com/paypalme/ferru97 <-- :)\n",
      "\n",
      "\n",
      "\n",
      "Completed 1/1 batches successfully\n",
      "\n",
      "‚úÖ Test successful! PDFs downloaded to ../papers/test_downloads/test_run_20250921_205321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Define test function to load DOIs from your real publication data\n",
    "def test_pypaperbot_with_real_publications(json_path, sample_size=5, output_dir='../papers/test_downloads'):\n",
    "    \"\"\"\n",
    "    Test PyPaperBot with a small sample of real publications\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to the JSON file containing publications with DOIs\n",
    "        sample_size: Number of publications to test (default: 5)\n",
    "        output_dir: Directory to save downloaded PDFs\n",
    "    \"\"\"\n",
    "    print(f\"üß™ Testing PyPaperBot with {sample_size} real publications\")\n",
    "    \n",
    "    # Load publications from JSON file\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            publications = json.load(f)\n",
    "        print(f\"‚úÖ Loaded {len(publications)} publications from {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading publications: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Extract DOIs from publications\n",
    "    dois = [pub.get('doi') for pub in publications if pub.get('doi')]\n",
    "    print(f\"üìä Found {len(dois)} publications with DOIs\")\n",
    "    \n",
    "    if not dois:\n",
    "        print(\"‚ùå No DOIs found in the publications data\")\n",
    "        return False\n",
    "    \n",
    "    # Select a random sample of DOIs\n",
    "    if len(dois) > sample_size:\n",
    "        sample_dois = random.sample(dois, sample_size)\n",
    "    else:\n",
    "        sample_dois = dois\n",
    "        print(f\"‚ö†Ô∏è Requested {sample_size} samples but only {len(dois)} DOIs available\")\n",
    "    \n",
    "    print(f\"\\nüìù Selected {len(sample_dois)} DOIs for testing:\")\n",
    "    for i, doi in enumerate(sample_dois):\n",
    "        print(f\"   {i+1}. {doi}\")\n",
    "    \n",
    "    # Download PDFs using PyPaperBot\n",
    "    print(f\"\\nüì• Downloading PDFs to {output_dir}\")\n",
    "    \n",
    "    # Create timestamp for this test run\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    test_dir = os.path.join(output_dir, f\"test_run_{timestamp}\")\n",
    "    \n",
    "    # Use the bulk download function with a single chunk\n",
    "    success = bulk_download_with_pypaperbot(\n",
    "        sample_dois, \n",
    "        output_dir=test_dir,\n",
    "        chunk_size=len(sample_dois)  # Process all in one chunk\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\n‚úÖ Test successful! PDFs downloaded to {test_dir}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Test failed. Check logs for errors.\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "# Run the test with the real publications\n",
    "ifc_publications_path = '/home/santi/Projects/UBMI-IFC-Podcast/data/raw/all_ifc_publications.json'\n",
    "test_pypaperbot_with_real_publications(ifc_publications_path, sample_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e7d5a",
   "metadata": {},
   "source": [
    "## Affiliation mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f4b4c",
   "metadata": {},
   "source": [
    "> affiliation mining system:\n",
    "\n",
    "- Extracts text from PDFs using PyMuPDF\n",
    "- Uses both regex and NLP for affiliation detection\n",
    "- Supports Spanish and English processing\n",
    "- Groups similar affiliations automatically\n",
    "- Generates PubMed search variations from discovered affiliations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816bf78",
   "metadata": {},
   "source": [
    "> NOTE‚ö†Ô∏è\n",
    "\n",
    "spaCy:\n",
    "\n",
    "- Tokenizes the text into words, punctuation, etc.\n",
    "- Part-of-speech tags each token\n",
    "- Dependency parses to understand grammatical relationships\n",
    "- Named Entity Recognition identifies spans as organizations, people, locations, etc.\n",
    "Classification assigns labels like \"ORG\" (organization), \"PERSON\", \"GPE\" (geopolitical entity)\n",
    "\n",
    "```python entity_recognition_process\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"ORG\":  # Organization entity\n",
    "        print(ent.text)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c45fcd",
   "metadata": {},
   "source": [
    "#### spacy installation\n",
    "\n",
    "```python\n",
    "# Install Python packages\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Download spaCy language models\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download es_core_news_sm\n",
    "\n",
    "# Optional: Download larger, more accurate models\n",
    "python -m spacy download en_core_web_md\n",
    "python -m spacy download es_core_news_md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70116b66",
   "metadata": {},
   "source": [
    "#### PDF text Extraaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "064c278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF using PyMuPDF (faster and more accurate than PyPDF)\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        \n",
    "        # Extract text from each page\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {os.path.basename(pdf_path)}: {e}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        if 'doc' in locals():\n",
    "            doc.close()\n",
    "\n",
    "def batch_process_pdfs(pdf_dir, limit=None):\n",
    "    \"\"\"Process multiple PDFs and extract text\"\"\"\n",
    "    pdf_files = glob.glob(os.path.join(pdf_dir, \"**\", \"*.pdf\"), recursive=True)\n",
    "    \n",
    "    if limit:\n",
    "        pdf_files = pdf_files[:limit]\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    results = {}\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if text:\n",
    "            results[filename] = text\n",
    "    \n",
    "    print(f\"Successfully extracted text from {len(results)} PDFs\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75ff6e",
   "metadata": {},
   "source": [
    "pipeline to connect the PDF processing with affiliation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa72c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_affiliations_from_pdfs(pdf_dir, output_json=None, limit=None):\n",
    "    \"\"\"\n",
    "    Extract affiliations from PDFs and return structured data\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir: Directory containing PDFs to process\n",
    "        output_json: Optional path to save results as JSON\n",
    "        limit: Maximum number of PDFs to process\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with affiliation data\n",
    "    \"\"\"\n",
    "    # 1. Initialize the affiliation miner\n",
    "    print(\"üîç Initializing affiliation miner...\")\n",
    "    miner = EnhancedAffiliationMiner()\n",
    "    \n",
    "    # 2. Extract text from PDFs\n",
    "    print(\"\\nüìÑ Extracting text from PDFs...\")\n",
    "    pdf_texts = batch_process_pdfs(pdf_dir, limit)\n",
    "    \n",
    "    # 3. Mine affiliations from each PDF\n",
    "    print(\"\\nüè¢ Mining affiliations from extracted text...\")\n",
    "    all_affiliations = set()\n",
    "    pdf_affiliations = {}\n",
    "    \n",
    "    for filename, text in tqdm(pdf_texts.items(), desc=\"Mining affiliations\"):\n",
    "        # Process only the first few pages where affiliations typically appear\n",
    "        first_pages_text = text[:20000]  # Adjust based on typical affiliation location\n",
    "        affiliations = miner.extract_affiliations_advanced_nlp(first_pages_text)\n",
    "        \n",
    "        if affiliations:\n",
    "            pdf_affiliations[filename] = list(affiliations)\n",
    "            all_affiliations.update(affiliations)\n",
    "    \n",
    "    # 4. Cluster similar affiliations\n",
    "    print(f\"\\nüß© Clustering {len(all_affiliations)} discovered affiliations...\")\n",
    "    clusters = miner.analyze_affiliations_with_clustering(list(all_affiliations))\n",
    "    \n",
    "    # 5. Generate PubMed search variations\n",
    "    print(\"\\nüîé Generating PubMed search variations...\")\n",
    "    pubmed_variations = generate_pubmed_search_variations(clusters)\n",
    "    \n",
    "    # 6. Compile results\n",
    "    results = {\n",
    "        'total_pdfs_processed': len(pdf_texts),\n",
    "        'total_affiliations_found': len(all_affiliations),\n",
    "        'affiliation_clusters': [\n",
    "            {'representative': cluster[0], 'variations': cluster} \n",
    "            for cluster in clusters\n",
    "        ],\n",
    "        'pubmed_search_variations': pubmed_variations,\n",
    "        'pdf_affiliations': pdf_affiliations\n",
    "    }\n",
    "    \n",
    "    # 7. Save results if requested\n",
    "    if output_json:\n",
    "        import json\n",
    "        os.makedirs(os.path.dirname(output_json), exist_ok=True)\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\n‚úÖ Results saved to {output_json}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_pubmed_search_variations(affiliation_clusters):\n",
    "    \"\"\"Generate PubMed search variations from affiliation clusters\"\"\"\n",
    "    search_variations = []\n",
    "    \n",
    "    # Process each cluster\n",
    "    for cluster in affiliation_clusters:\n",
    "        # Use the first (representative) affiliation from each cluster\n",
    "        if cluster:\n",
    "            rep_affiliation = cluster[0]\n",
    "            \n",
    "            # Clean and format for PubMed\n",
    "            # Remove common punctuation and normalize spaces\n",
    "            clean_aff = re.sub(r'[,.:]', '', rep_affiliation)\n",
    "            clean_aff = re.sub(r'\\s+', ' ', clean_aff).strip()\n",
    "            \n",
    "            # Add [Affiliation] tag for PubMed\n",
    "            pubmed_variation = f\"{clean_aff}[Affiliation]\"\n",
    "            search_variations.append(pubmed_variation)\n",
    "    \n",
    "    return search_variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea8a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def mine_affiliations_from_pdfs(pdf_dir, output_json=None, limit=None):\n",
    "    \"\"\"\n",
    "    Extract affiliations from PDFs and return structured data\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir: Directory containing PDFs to process\n",
    "        output_json: Optional path to save results as JSON\n",
    "        limit: Maximum number of PDFs to process\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with affiliation data\n",
    "    \"\"\"\n",
    "    # 1. Initialize the affiliation miner\n",
    "    print(\"üîç Initializing affiliation miner...\")\n",
    "    miner = EnhancedAffiliationMiner()\n",
    "    \n",
    "    # 2. Extract text from PDFs\n",
    "    print(\"\\nüìÑ Extracting text from PDFs...\")\n",
    "    pdf_texts = batch_process_pdfs(pdf_dir, limit)\n",
    "    \n",
    "    try:\n",
    "        # 3. Mine affiliations from each PDF\n",
    "        print(\"\\nüè¢ Mining affiliations from extracted text...\")\n",
    "        all_affiliations = set()\n",
    "        pdf_affiliations = {}\n",
    "        \n",
    "        for filename, text in tqdm(pdf_texts.items(), desc=\"Mining affiliations\"):\n",
    "            try:\n",
    "                # Process only the first few pages where affiliations typically appear\n",
    "                first_pages_text = text[:20000]  # Adjust based on typical affiliation location\n",
    "                affiliations = miner.extract_affiliations_advanced_nlp(first_pages_text)\n",
    "                \n",
    "                if affiliations:\n",
    "                    pdf_affiliations[filename] = list(affiliations)\n",
    "                    all_affiliations.update(affiliations)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 4. Cluster similar affiliations\n",
    "        print(f\"\\nüß© Clustering {len(all_affiliations)} discovered affiliations...\")\n",
    "        clusters = miner.analyze_affiliations_with_clustering(list(all_affiliations))\n",
    "        \n",
    "        # 5. Generate PubMed search variations\n",
    "        print(\"\\nüîé Generating PubMed search variations...\")\n",
    "        pubmed_variations = generate_pubmed_search_variations(clusters)\n",
    "        \n",
    "        # 6. Compile results\n",
    "        results = {\n",
    "            'total_pdfs_processed': len(pdf_texts),\n",
    "            'total_affiliations_found': len(all_affiliations),\n",
    "            'affiliation_clusters': [\n",
    "                {'representative': cluster[0], 'variations': cluster} \n",
    "                for cluster in clusters\n",
    "            ],\n",
    "            'pubmed_search_variations': pubmed_variations,\n",
    "            'pdf_affiliations': pdf_affiliations\n",
    "        }\n",
    "        \n",
    "        # 7. Save results if requested\n",
    "        if output_json:\n",
    "            os.makedirs(os.path.dirname(output_json), exist_ok=True)\n",
    "            with open(output_json, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"\\n‚úÖ Results saved to {output_json}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in affiliation mining process: {e}\")\n",
    "        if output_json:\n",
    "            # Save what we have so far as backup\n",
    "            with open(output_json + '.partial', 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    'error': str(e),\n",
    "                    'partial_results': pdf_affiliations\n",
    "                }, f, indent=2, ensure_ascii=False)\n",
    "        return {'total_pdfs_processed': 0, 'total_affiliations_found': 0, \n",
    "                'affiliation_clusters': [], 'pubmed_search_variations': [], \n",
    "                'pdf_affiliations': {}, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f92d69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the build_search_queries method in PubmedSearcher class\n",
    "def build_search_queries(self, affiliation_variations=None):\n",
    "    \"\"\"Build comprehensive search queries for different affiliation variations\"\"\"\n",
    "    \n",
    "    if affiliation_variations is None:\n",
    "        # Default variations based on your institute\n",
    "        affiliation_variations = [\n",
    "            \"Instituto de Fisiologia Celular[Affiliation]\",\n",
    "            \"Institute of Cellular Physiology[Affiliation]\",\n",
    "            \"IFC UNAM[Affiliation]\",\n",
    "            \"Departamento de Neurobiologia UNAM[Affiliation]\",\n",
    "            \"Universidad Nacional Autonoma Mexico Fisiologia[Affiliation]\",\n",
    "            \"National Autonomous University Mexico Cellular Physiology[Affiliation]\"\n",
    "        ]\n",
    "    \n",
    "    # Filter out variations that are too generic or too long\n",
    "    filtered_variations = []\n",
    "    for var in affiliation_variations:\n",
    "        # Remove the [Affiliation] suffix if present for checking\n",
    "        check_var = var.replace(\"[Affiliation]\", \"\").strip().lower()\n",
    "        \n",
    "        # Skip variations that are too short (likely noise) \n",
    "        # or don't contain key terms related to your institute\n",
    "        if len(check_var) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Skip variations without key identifiers\n",
    "        if not any(term in check_var for term in [\"fisiol\", \"physiol\", \"mexico\", \"unam\", \"ifc\", \"cellular\"]):\n",
    "            continue\n",
    "            \n",
    "        filtered_variations.append(var)\n",
    "    \n",
    "    queries = []\n",
    "    \n",
    "    # Individual affiliation searches\n",
    "    for aff in filtered_variations[:10]:  # Limit to top 10 to avoid excessive queries\n",
    "        queries.append(aff)\n",
    "        \n",
    "    # Combined searches with time ranges\n",
    "    recent_query = f\"({' OR '.join(filtered_variations[:3])}) AND (2020:2024[pdat])\"\n",
    "    historical_query = f\"({' OR '.join(filtered_variations[:3])}) AND (2010:2019[pdat])\"\n",
    "    \n",
    "    queries.extend([recent_query, historical_query])\n",
    "    \n",
    "    return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59316156",
   "metadata": {},
   "source": [
    "complete pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afa3adad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Enhanced Affiliation Mining Demo...\n",
      "‚úÖ Loaded en_core_web_sm\n",
      "‚úÖ Loaded en_core_web_sm\n",
      "‚úÖ Loaded es_core_news_sm\n",
      "üß™ Testing enhanced affiliation extraction...\n",
      "‚úÖ Loaded es_core_news_sm\n",
      "üß™ Testing enhanced affiliation extraction...\n",
      "\n",
      "üß† Enhanced NLP extraction found 6 affiliations:\n",
      "   ‚Ä¢ Fisiolog√≠a Celular\n",
      "   ‚Ä¢ IFC-UNAM\n",
      "   ‚Ä¢ Instituto de Fisiolog√≠a Celular\n",
      "   ‚Ä¢ Instituto de Fisiolog√≠a Celular,\n",
      "   ‚Ä¢ National Autonomous University of Mexico\n",
      "   ‚Ä¢ UNAM\n",
      "    Centro de Investigaci√≥n\n",
      "\n",
      "üîó Found 4 similarity clusters:\n",
      "   Cluster 1: 3 variations\n",
      "      - Fisiolog√≠a Celular\n",
      "      - Instituto de Fisiolog√≠a Celular\n",
      "      - Instituto de Fisiolog√≠a Celular,\n",
      "   Cluster 2: 1 variations\n",
      "      - IFC-UNAM\n",
      "   Cluster 3: 1 variations\n",
      "      - National Autonomous University of Mexico\n",
      "   Cluster 4: 1 variations\n",
      "      - UNAM\n",
      "    Centro de Investigaci√≥n\n",
      "\n",
      "üß† Enhanced NLP extraction found 6 affiliations:\n",
      "   ‚Ä¢ Fisiolog√≠a Celular\n",
      "   ‚Ä¢ IFC-UNAM\n",
      "   ‚Ä¢ Instituto de Fisiolog√≠a Celular\n",
      "   ‚Ä¢ Instituto de Fisiolog√≠a Celular,\n",
      "   ‚Ä¢ National Autonomous University of Mexico\n",
      "   ‚Ä¢ UNAM\n",
      "    Centro de Investigaci√≥n\n",
      "\n",
      "üîó Found 4 similarity clusters:\n",
      "   Cluster 1: 3 variations\n",
      "      - Fisiolog√≠a Celular\n",
      "      - Instituto de Fisiolog√≠a Celular\n",
      "      - Instituto de Fisiolog√≠a Celular,\n",
      "   Cluster 2: 1 variations\n",
      "      - IFC-UNAM\n",
      "   Cluster 3: 1 variations\n",
      "      - National Autonomous University of Mexico\n",
      "   Cluster 4: 1 variations\n",
      "      - UNAM\n",
      "    Centro de Investigaci√≥n\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "class EnhancedAffiliationMiner:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with advanced spaCy features\"\"\"\n",
    "        self.nlp_models = {}\n",
    "        self.matchers = {}\n",
    "        self.load_nlp_models()\n",
    "        self.setup_custom_matchers()\n",
    "        \n",
    "    def load_nlp_models(self):\n",
    "        \"\"\"Load spaCy models with error handling\"\"\"\n",
    "        models_to_load = {\n",
    "            'en': 'en_core_web_sm',\n",
    "            'es': 'es_core_news_sm'\n",
    "        }\n",
    "        \n",
    "        for lang, model_name in models_to_load.items():\n",
    "            try:\n",
    "                nlp = spacy.load(model_name)\n",
    "                # Add custom pipeline components\n",
    "                if not nlp.has_pipe('merge_entities'):\n",
    "                    nlp.add_pipe('merge_entities')\n",
    "                \n",
    "                self.nlp_models[lang] = nlp\n",
    "                print(f\"‚úÖ Loaded {model_name}\")\n",
    "                \n",
    "                # Setup matcher for this language\n",
    "                self.matchers[lang] = Matcher(nlp.vocab)\n",
    "                \n",
    "            except OSError:\n",
    "                print(f\"‚ùå {model_name} not found. Install with:\")\n",
    "                print(f\"   python -m spacy download {model_name}\")\n",
    "    \n",
    "    def setup_custom_matchers(self):\n",
    "        \"\"\"Setup custom pattern matchers for institutional names\"\"\"\n",
    "        \n",
    "        # Patterns for Spanish institutions\n",
    "        if 'es' in self.matchers:\n",
    "            spanish_patterns = [\n",
    "                # Instituto de X patterns\n",
    "                [{\"LOWER\": \"instituto\"}, {\"LOWER\": \"de\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # Universidad patterns\n",
    "                [{\"LOWER\": \"universidad\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                [{\"LOWER\": \"universidad\"}, {\"LOWER\": \"nacional\"}, {\"LOWER\": \"aut√≥noma\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"m√©xico\"}],\n",
    "                \n",
    "                # Departamento patterns\n",
    "                [{\"LOWER\": \"departamento\"}, {\"LOWER\": \"de\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # IFC patterns\n",
    "                [{\"TEXT\": {\"REGEX\": r\"IFC-?UNAM\"}}],\n",
    "            ]\n",
    "            \n",
    "            for i, pattern in enumerate(spanish_patterns):\n",
    "                self.matchers['es'].add(f\"SPANISH_INSTITUTION_{i}\", [pattern])\n",
    "        \n",
    "        # Patterns for English institutions\n",
    "        if 'en' in self.matchers:\n",
    "            english_patterns = [\n",
    "                # University of X patterns\n",
    "                [{\"LOWER\": \"university\"}, {\"LOWER\": \"of\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # Institute of X patterns\n",
    "                [{\"LOWER\": \"institute\"}, {\"LOWER\": \"of\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # Department of X patterns\n",
    "                [{\"LOWER\": \"department\"}, {\"LOWER\": \"of\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # National Autonomous University of Mexico\n",
    "                [{\"LOWER\": \"national\"}, {\"LOWER\": \"autonomous\"}, {\"LOWER\": \"university\"}, \n",
    "                 {\"LOWER\": \"of\"}, {\"LOWER\": \"mexico\"}],\n",
    "            ]\n",
    "            \n",
    "            for i, pattern in enumerate(english_patterns):\n",
    "                self.matchers['en'].add(f\"ENGLISH_INSTITUTION_{i}\", [pattern])\n",
    "    \n",
    "    def detect_language_advanced(self, text):\n",
    "        \"\"\"Advanced language detection\"\"\"\n",
    "        try:\n",
    "            # Use langdetect for primary detection\n",
    "            detected = detect(text[:1000])  # Use first 1000 chars for speed\n",
    "            \n",
    "            # Validate with keyword analysis\n",
    "            spanish_keywords = ['de', 'del', 'la', 'el', 'y', 'universidad', 'instituto']\n",
    "            english_keywords = ['of', 'the', 'and', 'university', 'institute', 'department']\n",
    "            \n",
    "            text_lower = text.lower()\n",
    "            spanish_count = sum(1 for kw in spanish_keywords if kw in text_lower)\n",
    "            english_count = sum(1 for kw in english_keywords if kw in text_lower)\n",
    "            \n",
    "            # Override detection if keyword analysis is strong\n",
    "            if spanish_count > english_count * 1.5:\n",
    "                return 'es'\n",
    "            elif english_count > spanish_count * 1.5:\n",
    "                return 'en'\n",
    "            else:\n",
    "                return detected if detected in ['es', 'en'] else 'en'\n",
    "                \n",
    "        except:\n",
    "            return 'en'  # Default to English\n",
    "    \n",
    "    def extract_affiliations_advanced_nlp(self, text):\n",
    "        \"\"\"Advanced NER + custom patterns for affiliation extraction\"\"\"\n",
    "        language = self.detect_language_advanced(text)\n",
    "        \n",
    "        if language not in self.nlp_models:\n",
    "            print(f\"‚ö†Ô∏è No model available for language: {language}\")\n",
    "            return set()\n",
    "        \n",
    "        nlp = self.nlp_models[language]\n",
    "        matcher = self.matchers[language]\n",
    "        \n",
    "        affiliations = set()\n",
    "        \n",
    "        # Process text in chunks to handle large documents\n",
    "        max_length = 1000000\n",
    "        text_chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "        \n",
    "        for chunk in text_chunks:\n",
    "            try:\n",
    "                doc = nlp(chunk)\n",
    "                \n",
    "                # Method 1: Standard NER for organizations\n",
    "                for ent in doc.ents:\n",
    "                    if ent.label_ == \"ORG\":\n",
    "                        org_text = ent.text.strip()\n",
    "                        if self.is_relevant_affiliation(org_text):\n",
    "                            affiliations.add(org_text)\n",
    "                \n",
    "                # Method 2: Custom pattern matching\n",
    "                matches = matcher(doc)\n",
    "                for match_id, start, end in matches:\n",
    "                    span = doc[start:end]\n",
    "                    affiliation_text = span.text.strip()\n",
    "                    if len(affiliation_text) > 5:\n",
    "                        affiliations.add(affiliation_text)\n",
    "                \n",
    "                # Method 3: Context-based extraction\n",
    "                # Look for sentences containing institutional indicators\n",
    "                for sent in doc.sents:\n",
    "                    sent_text = sent.text.strip()\n",
    "                    if self.contains_institutional_indicators(sent_text, language):\n",
    "                        # Extract the institutional part\n",
    "                        extracted = self.extract_institutional_part(sent_text, language)\n",
    "                        if extracted:\n",
    "                            affiliations.add(extracted)\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing chunk: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return affiliations\n",
    "    \n",
    "    def is_relevant_affiliation(self, org_text):\n",
    "        \"\"\"Check if organization text is relevant to our search\"\"\"\n",
    "        relevant_keywords = [\n",
    "            'instituto', 'institute', 'universidad', 'university',\n",
    "            'departamento', 'department', 'unam', 'ifc', 'mexico',\n",
    "            'fisiolog', 'physiolog', 'celular', 'cellular', 'neurobiolog'\n",
    "        ]\n",
    "        \n",
    "        org_lower = org_text.lower()\n",
    "        return (len(org_text) > 10 and \n",
    "                any(keyword in org_lower for keyword in relevant_keywords))\n",
    "    \n",
    "    def contains_institutional_indicators(self, text, language):\n",
    "        \"\"\"Check if text contains institutional indicators\"\"\"\n",
    "        if language == 'es':\n",
    "            indicators = [\n",
    "                'instituto de', 'universidad', 'departamento de', \n",
    "                'centro de', 'facultad de', 'unam'\n",
    "            ]\n",
    "        else:\n",
    "            indicators = [\n",
    "                'institute of', 'university of', 'department of',\n",
    "                'center of', 'faculty of', 'unam'\n",
    "            ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        return any(indicator in text_lower for indicator in indicators)\n",
    "    \n",
    "    def extract_institutional_part(self, sentence, language):\n",
    "        \"\"\"Extract the institutional part from a sentence\"\"\"\n",
    "        # Use regex patterns to extract institutional names\n",
    "        if language == 'es':\n",
    "            patterns = [\n",
    "                r'Instituto\\s+de\\s+[A-Za-z√Å√°√â√©√ç√≠√ì√≥√ö√∫√ë√±\\s,]+?(?:,|\\.|\\s+UNAM)',\n",
    "                r'Universidad\\s+[A-Za-z√Å√°√â√©√ç√≠√ì√≥√ö√∫√ë√±\\s,]+?(?:,|\\.)',\n",
    "                r'Departamento\\s+de\\s+[A-Za-z√Å√°√â√©√ç√≠√ì√≥√ö√∫√ë√±\\s,]+?(?:,|\\.)'\n",
    "            ]\n",
    "        else:\n",
    "            patterns = [\n",
    "                r'Institute\\s+of\\s+[A-Za-z\\s,]+?(?:,|\\.|\\s+UNAM)',\n",
    "                r'University\\s+of\\s+[A-Za-z\\s,]+?(?:,|\\.)',\n",
    "                r'Department\\s+of\\s+[A-Za-z\\s,]+?(?:,|\\.)'\n",
    "            ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group().strip()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def analyze_affiliations_with_clustering(self, affiliations_list):\n",
    "        \"\"\"Advanced analysis with similarity clustering\"\"\"\n",
    "        from difflib import SequenceMatcher\n",
    "        \n",
    "        def similarity(a, b):\n",
    "            return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "        \n",
    "        # Group similar affiliations\n",
    "        clusters = []\n",
    "        processed = set()\n",
    "        \n",
    "        for affiliation in affiliations_list:\n",
    "            if affiliation in processed:\n",
    "                continue\n",
    "                \n",
    "            # Find similar affiliations\n",
    "            cluster = [affiliation]\n",
    "            processed.add(affiliation)\n",
    "            \n",
    "            for other in affiliations_list:\n",
    "                if other not in processed and similarity(affiliation, other) > 0.7:\n",
    "                    cluster.append(other)\n",
    "                    processed.add(other)\n",
    "            \n",
    "            if len(cluster) >= 1:\n",
    "                clusters.append(cluster)\n",
    "        \n",
    "        return clusters\n",
    "\n",
    "# Usage example and demo\n",
    "def demo_enhanced_mining():\n",
    "    \"\"\"Demonstrate enhanced affiliation mining\"\"\"\n",
    "    miner = EnhancedAffiliationMiner()\n",
    "    \n",
    "    sample_text = \"\"\"\n",
    "    Instituto de Fisiolog√≠a Celular, Universidad Nacional Aut√≥noma de M√©xico, \n",
    "    Ciudad Universitaria, M√©xico, D.F. 04510, M√©xico\n",
    "    \n",
    "    Department of Cellular Physiology, National Autonomous University of Mexico,\n",
    "    Mexico City, Mexico\n",
    "    \n",
    "    Departamento de Neurobiolog√≠a, Instituto de Fisiolog√≠a Celular, UNAM\n",
    "    Centro de Investigaci√≥n y de Estudios Avanzados del IPN\n",
    "    \n",
    "    IFC-UNAM, Circuito Exterior s/n, Ciudad Universitaria\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üß™ Testing enhanced affiliation extraction...\")\n",
    "    \n",
    "    # Advanced NLP extraction\n",
    "    affiliations = miner.extract_affiliations_advanced_nlp(sample_text)\n",
    "    \n",
    "    print(f\"\\nüß† Enhanced NLP extraction found {len(affiliations)} affiliations:\")\n",
    "    for affiliation in sorted(affiliations):\n",
    "        print(f\"   ‚Ä¢ {affiliation}\")\n",
    "    \n",
    "    # Clustering analysis\n",
    "    clusters = miner.analyze_affiliations_with_clustering(list(affiliations))\n",
    "    print(f\"\\nüîó Found {len(clusters)} similarity clusters:\")\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        print(f\"   Cluster {i+1}: {len(cluster)} variations\")\n",
    "        for variation in cluster:\n",
    "            print(f\"      - {variation}\")\n",
    "    \n",
    "    return affiliations\n",
    "\n",
    "# Run enhanced demo\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting Enhanced Affiliation Mining Demo...\")\n",
    "    demo_results = demo_enhanced_mining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pdfs_and_search_pubmed(pdf_dir, output_dir='../data/processed/affiliations', \n",
    "                                 limit_pdfs=None, max_results_per_query=20):\n",
    "    \"\"\"\n",
    "    Complete pipeline: Extract affiliations from PDFs and search PubMed\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir: Directory containing PDFs to process\n",
    "        output_dir: Directory for saving outputs\n",
    "        limit_pdfs: Maximum number of PDFs to process (None for all)\n",
    "        max_results_per_query: Maximum results per PubMed query\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Mine affiliations from PDFs\n",
    "    print(\"üîé Step 1: Mining affiliations from PDFs...\")\n",
    "    affiliations_output = os.path.join(output_dir, \"discovered_affiliations.json\")\n",
    "    affiliation_results = mine_affiliations_from_pdfs(\n",
    "        pdf_dir=pdf_dir,\n",
    "        output_json=affiliations_output,\n",
    "        limit=limit_pdfs\n",
    "    )\n",
    "    \n",
    "    # Extract PubMed search variations\n",
    "    pubmed_variations = affiliation_results.get('pubmed_search_variations', [])\n",
    "    if not pubmed_variations:\n",
    "        print(\"‚ö†Ô∏è No valid PubMed search variations found. Using default variations.\")\n",
    "    else:\n",
    "        print(f\"üîç Found {len(pubmed_variations)} PubMed search variations\")\n",
    "        print(\"\\nSample variations:\")\n",
    "        for i, var in enumerate(pubmed_variations[:5]):\n",
    "            print(f\"   {i+1}. {var}\")\n",
    "    \n",
    "    # Step 2: Search PubMed with discovered affiliations\n",
    "    print(\"\\nüîç Step 2: Searching PubMed with discovered affiliations...\")\n",
    "    searcher = PubmedSearcher()\n",
    "    articles = searcher.comprehensive_search(\n",
    "        affiliation_variations=pubmed_variations,\n",
    "        max_per_query=max_results_per_query\n",
    "    )\n",
    "    \n",
    "    # Step 3: Save PubMed results\n",
    "    print(f\"\\nüìä Found {len(articles)} articles from PubMed\")\n",
    "    pubmed_output = os.path.join(output_dir, \"pubmed_results.json\")\n",
    "    with open(pubmed_output, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ PubMed results saved to {pubmed_output}\")\n",
    "    \n",
    "    # Step 4: Summary\n",
    "    print(\"\\nüìã Pipeline Summary:\")\n",
    "    print(f\"   PDFs processed: {affiliation_results['total_pdfs_processed']}\")\n",
    "    print(f\"   Unique affiliations found: {affiliation_results['total_affiliations_found']}\")\n",
    "    print(f\"   Affiliation clusters: {len(affiliation_results['affiliation_clusters'])}\")\n",
    "    print(f\"   PubMed search variations: {len(pubmed_variations)}\")\n",
    "    print(f\"   PubMed articles found: {len(articles)}\")\n",
    "    \n",
    "    return {\n",
    "        'affiliation_results': affiliation_results,\n",
    "        'pubmed_articles': articles\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833401e",
   "metadata": {},
   "source": [
    "### Mining all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "636aeeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (4.67.1)\n",
      "Testing affiliation extraction on PDFs in ../papers/downloaded/zotero\n",
      "üîç Initializing affiliation miner...\n",
      "Testing affiliation extraction on PDFs in ../papers/downloaded/zotero\n",
      "üîç Initializing affiliation miner...\n",
      "‚úÖ Loaded en_core_web_sm\n",
      "‚úÖ Loaded en_core_web_sm\n",
      "‚úÖ Loaded es_core_news_sm\n",
      "\n",
      "üìÑ Extracting text from PDFs...\n",
      "Found 345 PDF files to process\n",
      "‚úÖ Loaded es_core_news_sm\n",
      "\n",
      "üìÑ Extracting text from PDFs...\n",
      "Found 345 PDF files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 345/345 [00:37<00:00,  9.13it/s]\n",
      "Processing PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 345/345 [00:37<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted text from 345 PDFs\n",
      "\n",
      "üè¢ Mining affiliations from extracted text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mining affiliations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 345/345 [03:59<00:00,  1.44it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß© Clustering 1102 discovered affiliations...\n",
      "\n",
      "üîé Generating PubMed search variations...\n",
      "\n",
      "‚úÖ Results saved to ../data/processed/all_affiliations.json\n",
      "\n",
      "üè¢ Discovered affiliations:\n",
      "\n",
      "‚Ä¢ Main variation: University of Washington,\n",
      "  Other variations:\n",
      "  - University of Washington School of Medicine\n",
      "  - University of Warwick\n",
      "  - University of Michigan,\n",
      "  - University of Concepcion,\n",
      "  - University of Coahuila,\n",
      "  - University of Manchester,\n",
      "  - University of Aarhus,\n",
      "  - University of Dhaka,\n",
      "  - University of Michigan\n",
      "  - University of Michigan.\n",
      "  - University of Toronto\n",
      "  - University of¬†Cape Town\n",
      "  - University of Salento\n",
      "  - University of Palermo,\n",
      "  - University\n",
      "of Toronto,\n",
      "  - University of \n",
      "Massachusetts,\n",
      "  - University of Minnesota\n",
      "  - University of Illinois,\n",
      "  - University of Rome,\n",
      "  - University of Padua,\n",
      "  - University of Bath\n",
      "  - University of Manitoba\n",
      "  - E.S.-B, University of Washington.y\n",
      "  - University of Warwick,\n",
      "  - University of Toronto,\n",
      "  - University of Akron\n",
      "  - University of Groningen\n",
      "  - University of Valencia,\n",
      "  - University of Warsaw,\n",
      "  - Leo Pallanck, University of Washington\n",
      "  - University of Guanajuato\n",
      "  - University of Cambridge,\n",
      "  - University of Zagreb\n",
      "  - University of Reading\n",
      "  - University of Tokyo,\n",
      "  - University of Urbino\n",
      "  - University of Lisbon\n",
      "  - University of Gothenburg,\n",
      "  - University of California,\n",
      "  - University of Cambridge\n",
      "  - University of Urbino,\n",
      "  - University of Washington.\n",
      "  - University of Padova\n",
      "  - University of Trieste,\n",
      "  - University of Malaga,\n",
      "  - University of Washington\n",
      "School of Medicine,\n",
      "  - University of Tor-\n",
      "  - University of Lisbon,\n",
      "  - Biophysics, University of Washington School\n",
      "  - University of Innsbruck\n",
      "  - University of Alberta,\n",
      "  - University of Wisconsin-Madison\n",
      "  - University of Washington\n",
      "  - University of Reading,\n",
      "  - University of Malaga\n",
      "  - University of Mexico\n",
      "  - University of Washington School\n",
      "of Medicine,\n",
      "  - University of Nottingham,\n",
      "  - University of G√∂ttingen\n",
      "  - University of Massachusetts,\n",
      "  - University of Colorado,\n",
      "  - University of Trento\n",
      "  - University of Birmingham,\n",
      "  - University of Mexico,\n",
      "  - University of Washington School of Medicine,\n",
      "\n",
      "‚Ä¢ Main variation: Universidad de Guadalajara\n",
      "  Other variations:\n",
      "  - Universidad de Sevilla\n",
      "  - Universidad de\n",
      "Guadalajara\n",
      "  - Universidad de\n",
      "Guadalajara,\n",
      "  - Universidad de Guadalajara,\n",
      "  - Universidad de Guanajuato,\n",
      "\n",
      "‚Ä¢ Main variation: Molecular & Clinical Sciences Research Institute\n",
      "\n",
      "‚Ä¢ Main variation: Drexel University College of Medicine\n",
      "  Other variations:\n",
      "  - Stanford\n",
      "University School of Medicine\n",
      "  - University College London\n",
      "  - Marianna University School of\n",
      "Medicine\n",
      "  - Reserve University School of Medicine\n",
      "  - Yale University School of Medicine\n",
      "  - Stanford University School of Medicine\n",
      "\n",
      "‚Ä¢ Main variation: Departamento de Neurociencia Cognitiva,\n",
      "  Other variations:\n",
      "  - Departamento de Neurociencia\n",
      "Cognitiva and Laboratorio Nacional\n",
      "  - Departamento de Neurociencia Cognitiva\n",
      "  - Departamento de Farmacogen√©tica\n",
      "\n",
      "üîç PubMed search variations:\n",
      "   1. University of Washington[Affiliation]\n",
      "   2. Universidad de Guadalajara[Affiliation]\n",
      "   3. Molecular & Clinical Sciences Research Institute[Affiliation]\n",
      "   4. Drexel University College of Medicine[Affiliation]\n",
      "   5. Departamento de Neurociencia Cognitiva[Affiliation]\n",
      "\n",
      "üîé Generating PubMed search variations...\n",
      "\n",
      "‚úÖ Results saved to ../data/processed/all_affiliations.json\n",
      "\n",
      "üè¢ Discovered affiliations:\n",
      "\n",
      "‚Ä¢ Main variation: University of Washington,\n",
      "  Other variations:\n",
      "  - University of Washington School of Medicine\n",
      "  - University of Warwick\n",
      "  - University of Michigan,\n",
      "  - University of Concepcion,\n",
      "  - University of Coahuila,\n",
      "  - University of Manchester,\n",
      "  - University of Aarhus,\n",
      "  - University of Dhaka,\n",
      "  - University of Michigan\n",
      "  - University of Michigan.\n",
      "  - University of Toronto\n",
      "  - University of¬†Cape Town\n",
      "  - University of Salento\n",
      "  - University of Palermo,\n",
      "  - University\n",
      "of Toronto,\n",
      "  - University of \n",
      "Massachusetts,\n",
      "  - University of Minnesota\n",
      "  - University of Illinois,\n",
      "  - University of Rome,\n",
      "  - University of Padua,\n",
      "  - University of Bath\n",
      "  - University of Manitoba\n",
      "  - E.S.-B, University of Washington.y\n",
      "  - University of Warwick,\n",
      "  - University of Toronto,\n",
      "  - University of Akron\n",
      "  - University of Groningen\n",
      "  - University of Valencia,\n",
      "  - University of Warsaw,\n",
      "  - Leo Pallanck, University of Washington\n",
      "  - University of Guanajuato\n",
      "  - University of Cambridge,\n",
      "  - University of Zagreb\n",
      "  - University of Reading\n",
      "  - University of Tokyo,\n",
      "  - University of Urbino\n",
      "  - University of Lisbon\n",
      "  - University of Gothenburg,\n",
      "  - University of California,\n",
      "  - University of Cambridge\n",
      "  - University of Urbino,\n",
      "  - University of Washington.\n",
      "  - University of Padova\n",
      "  - University of Trieste,\n",
      "  - University of Malaga,\n",
      "  - University of Washington\n",
      "School of Medicine,\n",
      "  - University of Tor-\n",
      "  - University of Lisbon,\n",
      "  - Biophysics, University of Washington School\n",
      "  - University of Innsbruck\n",
      "  - University of Alberta,\n",
      "  - University of Wisconsin-Madison\n",
      "  - University of Washington\n",
      "  - University of Reading,\n",
      "  - University of Malaga\n",
      "  - University of Mexico\n",
      "  - University of Washington School\n",
      "of Medicine,\n",
      "  - University of Nottingham,\n",
      "  - University of G√∂ttingen\n",
      "  - University of Massachusetts,\n",
      "  - University of Colorado,\n",
      "  - University of Trento\n",
      "  - University of Birmingham,\n",
      "  - University of Mexico,\n",
      "  - University of Washington School of Medicine,\n",
      "\n",
      "‚Ä¢ Main variation: Universidad de Guadalajara\n",
      "  Other variations:\n",
      "  - Universidad de Sevilla\n",
      "  - Universidad de\n",
      "Guadalajara\n",
      "  - Universidad de\n",
      "Guadalajara,\n",
      "  - Universidad de Guadalajara,\n",
      "  - Universidad de Guanajuato,\n",
      "\n",
      "‚Ä¢ Main variation: Molecular & Clinical Sciences Research Institute\n",
      "\n",
      "‚Ä¢ Main variation: Drexel University College of Medicine\n",
      "  Other variations:\n",
      "  - Stanford\n",
      "University School of Medicine\n",
      "  - University College London\n",
      "  - Marianna University School of\n",
      "Medicine\n",
      "  - Reserve University School of Medicine\n",
      "  - Yale University School of Medicine\n",
      "  - Stanford University School of Medicine\n",
      "\n",
      "‚Ä¢ Main variation: Departamento de Neurociencia Cognitiva,\n",
      "  Other variations:\n",
      "  - Departamento de Neurociencia\n",
      "Cognitiva and Laboratorio Nacional\n",
      "  - Departamento de Neurociencia Cognitiva\n",
      "  - Departamento de Farmacogen√©tica\n",
      "\n",
      "üîç PubMed search variations:\n",
      "   1. University of Washington[Affiliation]\n",
      "   2. Universidad de Guadalajara[Affiliation]\n",
      "   3. Molecular & Clinical Sciences Research Institute[Affiliation]\n",
      "   4. Drexel University College of Medicine[Affiliation]\n",
      "   5. Departamento de Neurociencia Cognitiva[Affiliation]\n"
     ]
    }
   ],
   "source": [
    "# Install PyMuPDF if not already installed\n",
    "!pip install pymupdf tqdm\n",
    "\n",
    "# Test the affiliation extraction on the downloaded PDFs\n",
    "import os\n",
    "\n",
    "# Get the path to the recently downloaded PDFs\n",
    "test_pdf_dir = '../papers/downloaded/zotero'  # Update with your actual path\n",
    "\n",
    "# Test with a small number of PDFs first\n",
    "print(f\"Testing affiliation extraction on PDFs in {test_pdf_dir}\")\n",
    "test_results = mine_affiliations_from_pdfs(\n",
    "    pdf_dir=test_pdf_dir,\n",
    "    output_json='../data/processed/all_affiliations.json',\n",
    "    limit=None  # Process up to 5 PDFs\n",
    ")\n",
    "\n",
    "# Display the discovered affiliations\n",
    "print(\"\\nüè¢ Discovered affiliations:\")\n",
    "for cluster in test_results['affiliation_clusters'][:5]:  # Show top 5 clusters\n",
    "    print(f\"\\n‚Ä¢ Main variation: {cluster['representative']}\")\n",
    "    if len(cluster['variations']) > 1:\n",
    "        print(\"  Other variations:\")\n",
    "        for var in cluster['variations'][1:]:\n",
    "            print(f\"  - {var}\")\n",
    "\n",
    "# Show how these could be used for PubMed search\n",
    "print(\"\\nüîç PubMed search variations:\")\n",
    "for i, var in enumerate(test_results['pubmed_search_variations'][:5]):\n",
    "    print(f\"   {i+1}. {var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90cf7d",
   "metadata": {},
   "source": [
    "Review affiliations before merging\n",
    "Manual review step before searching PubMed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373018dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã AFFILIATION REVIEW\n",
      "\n",
      "Review the following affiliation clusters discovered from PDFs:\n",
      "Select which clusters to include in PubMed search\n",
      "\n",
      "Found 357 potentially relevant clusters out of 357 total.\n",
      "----------------------------------------\n",
      "\n",
      "Cluster 1/357: Representative -> University of Washington,\n",
      "  Other variations found:\n",
      "    2. University of Washington School of Medicine\n",
      "    3. University of Warwick\n",
      "    4. University of Michigan,\n",
      "    5. University of Concepcion,\n",
      "    6. University of Coahuila,\n",
      "    7. University of Manchester,\n",
      "    8. University of Aarhus,\n",
      "    9. University of Dhaka,\n",
      "    10. University of Michigan\n",
      "    11. University of Michigan.\n",
      "    12. University of Toronto\n",
      "    13. University of¬†Cape Town\n",
      "    14. University of Salento\n",
      "    15. University of Palermo,\n",
      "    16. University\n",
      "of Toronto,\n",
      "    17. University of \n",
      "Massachusetts,\n",
      "    18. University of Minnesota\n",
      "    19. University of Illinois,\n",
      "    20. University of Rome,\n",
      "    21. University of Padua,\n",
      "    22. University of Bath\n",
      "    23. University of Manitoba\n",
      "    24. E.S.-B, University of Washington.y\n",
      "    25. University of Warwick,\n",
      "    26. University of Toronto,\n",
      "    27. University of Akron\n",
      "    28. University of Groningen\n",
      "    29. University of Valencia,\n",
      "    30. University of Warsaw,\n",
      "    31. Leo Pallanck, University of Washington\n",
      "    32. University of Guanajuato\n",
      "    33. University of Cambridge,\n",
      "    34. University of Zagreb\n",
      "    35. University of Reading\n",
      "    36. University of Tokyo,\n",
      "    37. University of Urbino\n",
      "    38. University of Lisbon\n",
      "    39. University of Gothenburg,\n",
      "    40. University of California,\n",
      "    41. University of Cambridge\n",
      "    42. University of Urbino,\n",
      "    43. University of Washington.\n",
      "    44. University of Padova\n",
      "    45. University of Trieste,\n",
      "    46. University of Malaga,\n",
      "    47. University of Washington\n",
      "School of Medicine,\n",
      "    48. University of Tor-\n",
      "    49. University of Lisbon,\n",
      "    50. Biophysics, University of Washington School\n",
      "    51. University of Innsbruck\n",
      "    52. University of Alberta,\n",
      "    53. University of Wisconsin-Madison\n",
      "    54. University of Washington\n",
      "    55. University of Reading,\n",
      "    56. University of Malaga\n",
      "    57. University of Mexico\n",
      "    58. University of Washington School\n",
      "of Medicine,\n",
      "    59. University of Nottingham,\n",
      "    60. University of G√∂ttingen\n",
      "    61. University of Massachusetts,\n",
      "    62. University of Colorado,\n",
      "    63. University of Trento\n",
      "    64. University of Birmingham,\n",
      "    65. University of Mexico,\n",
      "    66. University of Washington School of Medicine,\n",
      "Invalid input. Please enter 'y', 'n', or 's'.\n",
      "Invalid input. Please enter 'y', 'n', or 's'.\n",
      "‚ùå Cluster excluded.\n",
      "----------------------------------------\n",
      "\n",
      "Cluster 2/357: Representative -> Universidad de Guadalajara\n",
      "  Other variations found:\n",
      "    2. Universidad de Sevilla\n",
      "    3. Universidad de\n",
      "Guadalajara\n",
      "    4. Universidad de\n",
      "Guadalajara,\n",
      "    5. Universidad de Guadalajara,\n",
      "    6. Universidad de Guanajuato,\n",
      "‚ùå Cluster excluded.\n",
      "----------------------------------------\n",
      "\n",
      "Cluster 2/357: Representative -> Universidad de Guadalajara\n",
      "  Other variations found:\n",
      "    2. Universidad de Sevilla\n",
      "    3. Universidad de\n",
      "Guadalajara\n",
      "    4. Universidad de\n",
      "Guadalajara,\n",
      "    5. Universidad de Guadalajara,\n",
      "    6. Universidad de Guanajuato,\n",
      "‚úÖ Approved. Added 6 variations.\n",
      "----------------------------------------\n",
      "\n",
      "Cluster 3/357: Representative -> Molecular & Clinical Sciences Research Institute\n",
      "‚úÖ Approved. Added 6 variations.\n",
      "----------------------------------------\n",
      "\n",
      "Cluster 3/357: Representative -> Molecular & Clinical Sciences Research Institute\n",
      "‚úÖ Approved. Added 1 variations.\n",
      "----------------------------------------\n",
      "\n",
      "Cluster 4/357: Representative -> Drexel University College of Medicine\n",
      "  Other variations found:\n",
      "    2. Stanford\n",
      "University School of Medicine\n",
      "    3. University College London\n",
      "    4. Marianna University School of\n",
      "Medicine\n",
      "    5. Reserve University School of Medicine\n",
      "    6. Yale University School of Medicine\n",
      "    7. Stanford University School of Medicine\n",
      "‚úÖ Approved. Added 1 variations.\n",
      "----------------------------------------\n",
      "\n",
      "Cluster 4/357: Representative -> Drexel University College of Medicine\n",
      "  Other variations found:\n",
      "    2. Stanford\n",
      "University School of Medicine\n",
      "    3. University College London\n",
      "    4. Marianna University School of\n",
      "Medicine\n",
      "    5. Reserve University School of Medicine\n",
      "    6. Yale University School of Medicine\n",
      "    7. Stanford University School of Medicine\n"
     ]
    }
   ],
   "source": [
    "def review_and_select_affiliations(affiliation_clusters):\n",
    "    \"\"\"\n",
    "    Interactive review of discovered affiliation clusters before PubMed search\n",
    "    \n",
    "    Args:\n",
    "        affiliation_clusters: List of affiliation clusters discovered from PDFs\n",
    "        \n",
    "    Returns:\n",
    "        List of approved affiliation variations for PubMed search\n",
    "    \"\"\"\n",
    "    print(\"\\nüìã AFFILIATION REVIEW\\n\")\n",
    "    print(\"Review the following affiliation clusters discovered from PDFs:\")\n",
    "    print(\"Select which clusters to include in PubMed search\\n\")\n",
    "    \n",
    "    approved_variations = []\n",
    "    \n",
    "    # Filter clusters to only include relevant ones based on keywords\n",
    "    relevant_keywords = [\n",
    "        'instituto', 'institute', 'universidad', 'university',\n",
    "        'departamento', 'department', 'unam', 'ifc', 'mexico',\n",
    "        'fisiolog', 'physiolog', 'celular', 'cellular', 'neurobiolog'\n",
    "    ]\n",
    "    \n",
    "    relevant_clusters = []\n",
    "    for cluster in affiliation_clusters:\n",
    "        # Check if any variation in the cluster contains a relevant keyword\n",
    "        if any(\n",
    "            any(keyword in variation.lower() for keyword in relevant_keywords)\n",
    "            for variation in cluster['variations']\n",
    "        ):\n",
    "            relevant_clusters.append(cluster)\n",
    "\n",
    "    print(f\"Found {len(relevant_clusters)} potentially relevant clusters out of {len(affiliation_clusters)} total.\")\n",
    "\n",
    "    for i, cluster_data in enumerate(relevant_clusters):\n",
    "        cluster = cluster_data['variations']\n",
    "        representative = cluster_data['representative']\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "        print(f\"\\nCluster {i+1}/{len(relevant_clusters)}: Representative -> {representative}\")\n",
    "        \n",
    "        if len(cluster) > 1:\n",
    "            print(\"  Other variations found:\")\n",
    "            for j, variation in enumerate(cluster):\n",
    "                if variation != representative:\n",
    "                    print(f\"    {j+1}. {variation}\")\n",
    "        \n",
    "        # Ask for approval\n",
    "        while True:\n",
    "            try:\n",
    "                choice = input(f\"Include this cluster in PubMed search? (y/n/skip): \").lower().strip()\n",
    "                if choice in ('y', 'yes', 'n', 'no', 's', 'skip'):\n",
    "                    break\n",
    "                print(\"Invalid input. Please enter 'y', 'n', or 's'.\")\n",
    "            except (EOFError, KeyboardInterrupt):\n",
    "                print(\"\\nReview interrupted. Exiting.\")\n",
    "                return approved_variations\n",
    "\n",
    "        if choice in ('y', 'yes'):\n",
    "            approved_variations.extend(cluster)\n",
    "            print(f\"‚úÖ Approved. Added {len(cluster)} variations.\")\n",
    "        elif choice in ('n', 'no'):\n",
    "            print(f\"‚ùå Cluster excluded.\")\n",
    "        else: # skip\n",
    "            print(\"Skipping remaining clusters.\")\n",
    "            break\n",
    "            \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"\\nüìä Review Complete: Approved {len(approved_variations)} total affiliation variations.\")\n",
    "    \n",
    "    return approved_variations\n",
    "\n",
    "# Extract the affiliation clusters from the results of the previous cell\n",
    "affiliation_clusters = test_results.get('affiliation_clusters', [])\n",
    "\n",
    "# Run the interactive review process\n",
    "if affiliation_clusters:\n",
    "    approved_affiliations = review_and_select_affiliations(affiliation_clusters)\n",
    "    print(\"\\nFinal list of approved affiliations for PubMed search:\")\n",
    "    for aff in approved_affiliations:\n",
    "        print(f\"- {aff}\")\n",
    "else:\n",
    "    print(\"No affiliation clusters found in 'test_results'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a93212",
   "metadata": {},
   "source": [
    "we should updaate the PDF processing and PubMed search workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba1898ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pdfs_and_search_pubmed_with_review(pdf_dir, output_dir='../data/processed/affiliations', \n",
    "                                             limit_pdfs=None, max_results_per_query=20):\n",
    "    \"\"\"\n",
    "    Complete pipeline with manual review: Extract affiliations, review them, then search PubMed\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir: Directory containing PDFs to process\n",
    "        output_dir: Directory for saving outputs\n",
    "        limit_pdfs: Maximum number of PDFs to process (None for all)\n",
    "        max_results_per_query: Maximum results per PubMed query\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Mine affiliations from PDFs\n",
    "    print(\"üîé Step 1: Mining affiliations from PDFs...\")\n",
    "    affiliations_output = os.path.join(output_dir, \"discovered_affiliations.json\")\n",
    "    affiliation_results = mine_affiliations_from_pdfs(\n",
    "        pdf_dir=pdf_dir,\n",
    "        output_json=affiliations_output,\n",
    "        limit=limit_pdfs\n",
    "    )\n",
    "    \n",
    "    clusters = affiliation_results.get('affiliation_clusters', [])\n",
    "    \n",
    "    # Step 2: Manual review of affiliations (NEW)\n",
    "    print(\"\\nüîç Step 2: Reviewing discovered affiliations...\")\n",
    "    if not clusters:\n",
    "        print(\"‚ö†Ô∏è No affiliation clusters found.\")\n",
    "        approved_variations = []\n",
    "    else:\n",
    "        # Extract clusters from results\n",
    "        raw_clusters = [cluster['variations'] for cluster in clusters]\n",
    "        approved_variations = review_and_select_affiliations(raw_clusters)\n",
    "    \n",
    "    # Step 3: Format approved variations for PubMed search\n",
    "    if not approved_variations:\n",
    "        print(\"‚ö†Ô∏è No affiliations approved. Using default affiliations for PubMed search.\")\n",
    "        pubmed_variations = None  # Will use defaults in PubmedSearcher\n",
    "    else:\n",
    "        pubmed_variations = []\n",
    "        for variation in approved_variations:\n",
    "            # Clean and format for PubMed\n",
    "            clean_aff = re.sub(r'[,.:]', '', variation)\n",
    "            clean_aff = re.sub(r'\\s+', ' ', clean_aff).strip()\n",
    "            pubmed_variation = f\"{clean_aff}[Affiliation]\"\n",
    "            pubmed_variations.append(pubmed_variation)\n",
    "    \n",
    "        print(f\"üîç Generated {len(pubmed_variations)} PubMed search variations\")\n",
    "        print(\"\\nSample variations:\")\n",
    "        for i, var in enumerate(pubmed_variations[:5]):\n",
    "            print(f\"   {i+1}. {var}\")\n",
    "    \n",
    "    # Step 4: Search PubMed with approved affiliations\n",
    "    print(\"\\nüîç Step 4: Searching PubMed with approved affiliations...\")\n",
    "    searcher = PubmedSearcher()\n",
    "    articles = searcher.comprehensive_search(\n",
    "        affiliation_variations=pubmed_variations,\n",
    "        max_per_query=max_results_per_query\n",
    "    )\n",
    "    \n",
    "    # Step 5: Save PubMed results\n",
    "    print(f\"\\nüìä Found {len(articles)} articles from PubMed\")\n",
    "    pubmed_output = os.path.join(output_dir, \"pubmed_results.json\")\n",
    "    with open(pubmed_output, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ PubMed results saved to {pubmed_output}\")\n",
    "    \n",
    "    # Step 6: Summary\n",
    "    print(\"\\nüìã Pipeline Summary:\")\n",
    "    print(f\"   PDFs processed: {affiliation_results['total_pdfs_processed']}\")\n",
    "    print(f\"   Unique affiliations found: {affiliation_results['total_affiliations_found']}\")\n",
    "    print(f\"   Affiliation clusters reviewed: {len(clusters)}\")\n",
    "    print(f\"   Approved variations: {len(approved_variations)}\")\n",
    "    print(f\"   PubMed articles found: {len(articles)}\")\n",
    "    \n",
    "    return {\n",
    "        'affiliation_results': affiliation_results,\n",
    "        'approved_variations': approved_variations,\n",
    "        'pubmed_articles': articles\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b160a",
   "metadata": {},
   "source": [
    "üéØ Top Results Found\n",
    "\n",
    "The system correctly identified target clusters:\n",
    "\n",
    "\"Instituto de Fisiolog√≠a Celular\" (Score: 1767) - 39 variations ‚úÖ\n",
    "\"Universidad Nacional Aut√≥noma de M√©xico\" (Score: 708) - 53 variations ‚úÖ\n",
    "\"Institute for Cellular Physiology\" (Score: 544) - 10 variations ‚úÖ\n",
    "Department-level affiliations with IFC connections ‚úÖ\n",
    "\"Cellular Physiology\" standalone terms ‚úÖ\n",
    "\n",
    "Intelligent Scoring: Uses pattern matching, keywords, and similarity algorithms\n",
    "- added Customizable: Easily adjust thresholds and criteria\n",
    "- Formatted for use in PubMed searches and workflows\n",
    "\n",
    "üöÄ How to Use\n",
    "Option 1: Use the pre-filtered results (Recommended):\n",
    "\n",
    "```bash\n",
    "# The filtered results are ready to use in:\n",
    "# - data/processed/filtered_affiliations.json\n",
    "# - data/processed/manual_review_affiliations.txt\n",
    "```\n",
    "\n",
    "Option 2: Interactive filtering\n",
    "\n",
    "```bash\n",
    "python3 scripts/filter_affiliations.py\n",
    "```\n",
    "\n",
    "Option 3: Command line\n",
    "\n",
    "```bash\n",
    "# Conservative (highest quality)\n",
    "python3 scripts/filter_affiliations.py --score 10.0 --limit 20\n",
    "\n",
    "# Liberal (more inclusive)  \n",
    "python3 scripts/filter_affiliations.py --score 2.0 --limit 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9257b9",
   "metadata": {},
   "source": [
    "## 4. PubMed Search Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b7f64e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting comprehensive PubMed search...\n",
      "Found 2248 results for query: Instituto de Fisiologia Celular[Affiliation]...\n",
      "\n",
      "üìä Test search found 5 articles\n",
      "\n",
      "Sample result:\n",
      "Title: Multistable bimodal perceptual coding within the ventral premotor cortex....\n",
      "Authors: Andrade-Ortega, Bernardo; D√≠az, H√©ctor; Bayones, Lucas; Alvarez, Manuel; Zainos, Antonio; Rivera-Yos...\n",
      "PMID: 40971437\n",
      "\n",
      "üîç Running search 1/8\n",
      "Found 2248 results for query: Instituto de Fisiologia Celular[Affiliation]...\n",
      "Added 20 new articles (total: 20)\n",
      "\n",
      "üîç Running search 2/8\n",
      "Found 88 results for query: Institute of Cellular Physiology[Affiliation]...\n",
      "Added 20 new articles (total: 40)\n",
      "\n",
      "üîç Running search 3/8\n",
      "Found 522 results for query: IFC UNAM[Affiliation]...\n",
      "Added 12 new articles (total: 52)\n",
      "\n",
      "üîç Running search 4/8\n",
      "Found 9 results for query: Departamento de Neurobiologia UNAM[Affiliation]...\n",
      "Added 9 new articles (total: 61)\n",
      "\n",
      "üîç Running search 5/8\n",
      "Found 3968 results for query: Universidad Nacional Autonoma Mexico Fisiologia[Af...\n",
      "Added 10 new articles (total: 71)\n",
      "\n",
      "üîç Running search 6/8\n",
      "Found 59 results for query: National Autonomous University Mexico Cellular Phy...\n",
      "Added 7 new articles (total: 78)\n",
      "\n",
      "üîç Running search 7/8\n",
      "Found 619 results for query: (Instituto de Fisiologia Celular[Affiliation] OR I...\n",
      "Added 16 new articles (total: 94)\n",
      "\n",
      "üîç Running search 8/8\n",
      "Found 929 results for query: (Instituto de Fisiologia Celular[Affiliation] OR I...\n",
      "Added 20 new articles (total: 114)\n",
      "\n",
      "üéâ Found 114 total unique articles from PubMed\n"
     ]
    }
   ],
   "source": [
    "class PubmedSearcher:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "        \n",
    "    def build_search_queries(self, affiliation_variations=None):\n",
    "        \"\"\"Build comprehensive search queries for different affiliation variations\"\"\"\n",
    "        \n",
    "        if affiliation_variations is None:\n",
    "            # Default variations based on your institute\n",
    "            affiliation_variations = [\n",
    "                \"Instituto de Fisiologia Celular[Affiliation]\",\n",
    "                \"Institute of Cellular Physiology[Affiliation]\",\n",
    "                \"IFC UNAM[Affiliation]\",\n",
    "                \"Departamento de Neurobiologia UNAM[Affiliation]\",\n",
    "                \"Universidad Nacional Autonoma Mexico Fisiologia[Affiliation]\",\n",
    "                \"National Autonomous University Mexico Cellular Physiology[Affiliation]\"\n",
    "            ]\n",
    "        \n",
    "        queries = []\n",
    "        \n",
    "        # Individual affiliation searches\n",
    "        for aff in affiliation_variations:\n",
    "            queries.append(aff)\n",
    "            \n",
    "        # Combined searches with time ranges\n",
    "        recent_query = f\"({' OR '.join(affiliation_variations[:3])}) AND (2020:2024[pdat])\"\n",
    "        historical_query = f\"({' OR '.join(affiliation_variations[:3])}) AND (2010:2019[pdat])\"\n",
    "        \n",
    "        queries.extend([recent_query, historical_query])\n",
    "        \n",
    "        return queries\n",
    "    \n",
    "    def search_pubmed(self, query, max_results=100):\n",
    "        \"\"\"Search PubMed with a given query\"\"\"\n",
    "        \n",
    "        # Step 1: Search\n",
    "        search_url = f\"{self.base_url}esearch.fcgi\"\n",
    "        search_params = {\n",
    "            'db': 'pubmed',\n",
    "            'term': query,\n",
    "            'retmax': max_results,\n",
    "            'retmode': 'json'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(search_url, params=search_params)\n",
    "            search_data = response.json()\n",
    "            \n",
    "            pmids = search_data['esearchresult']['idlist']\n",
    "            total_count = int(search_data['esearchresult']['count'])\n",
    "            \n",
    "            print(f\"Found {total_count} results for query: {query[:50]}...\")\n",
    "            \n",
    "            if not pmids:\n",
    "                return []\n",
    "            \n",
    "            # Step 2: Fetch details\n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "            \n",
    "            fetch_url = f\"{self.base_url}efetch.fcgi\"\n",
    "            fetch_params = {\n",
    "                'db': 'pubmed',\n",
    "                'id': ','.join(pmids),\n",
    "                'retmode': 'xml'\n",
    "            }\n",
    "            \n",
    "            fetch_response = requests.get(fetch_url, params=fetch_params)\n",
    "            \n",
    "            # Parse XML (simplified - you might want to use xml.etree.ElementTree)\n",
    "            articles = self.parse_pubmed_xml(fetch_response.text)\n",
    "            \n",
    "            return articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching PubMed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def parse_pubmed_xml(self, xml_content):\n",
    "        \"\"\"Simple XML parsing for PubMed results (you might want to improve this)\"\"\"\n",
    "        import xml.etree.ElementTree as ET\n",
    "        \n",
    "        articles = []\n",
    "        \n",
    "        try:\n",
    "            root = ET.fromstring(xml_content)\n",
    "            \n",
    "            for article in root.findall('.//PubmedArticle'):\n",
    "                try:\n",
    "                    # Extract basic info\n",
    "                    pmid = article.find('.//PMID').text\n",
    "                    \n",
    "                    title_elem = article.find('.//ArticleTitle')\n",
    "                    title = title_elem.text if title_elem is not None else \"No title\"\n",
    "                    \n",
    "                    # Authors\n",
    "                    authors = []\n",
    "                    for author in article.findall('.//Author'):\n",
    "                        lastname = author.find('.//LastName')\n",
    "                        firstname = author.find('.//ForeName')\n",
    "                        if lastname is not None:\n",
    "                            author_name = lastname.text\n",
    "                            if firstname is not None:\n",
    "                                author_name += f\", {firstname.text}\"\n",
    "                            authors.append(author_name)\n",
    "                    \n",
    "                    # Journal and year\n",
    "                    journal_elem = article.find('.//Journal/Title')\n",
    "                    journal = journal_elem.text if journal_elem is not None else \"Unknown\"\n",
    "                    \n",
    "                    year_elem = article.find('.//PubDate/Year')\n",
    "                    year = int(year_elem.text) if year_elem is not None else None\n",
    "                    \n",
    "                    # Abstract\n",
    "                    abstract_elem = article.find('.//Abstract/AbstractText')\n",
    "                    abstract = abstract_elem.text if abstract_elem is not None else \"\"\n",
    "                    \n",
    "                    # DOI\n",
    "                    doi_elem = article.find('.//ELocationID[@EIdType=\"doi\"]')\n",
    "                    doi = doi_elem.text if doi_elem is not None else None\n",
    "                    \n",
    "                    article_data = {\n",
    "                        'pmid': pmid,\n",
    "                        'title': title,\n",
    "                        'authors': '; '.join(authors),\n",
    "                        'journal': journal,\n",
    "                        'year': year,\n",
    "                        'abstract': abstract,\n",
    "                        'doi': doi\n",
    "                    }\n",
    "                    \n",
    "                    articles.append(article_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing article: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing XML: {e}\")\n",
    "            \n",
    "        return articles\n",
    "    \n",
    "    def comprehensive_search(self, affiliation_variations=None, max_per_query=50):\n",
    "        \"\"\"\n",
    "        Run comprehensive search with all query variations\n",
    "        \n",
    "        Args:\n",
    "            affiliation_variations: Optional list of affiliation variations to use\n",
    "            max_per_query: Maximum results per query\n",
    "        \n",
    "        Returns:\n",
    "            List of articles found\n",
    "        \"\"\"\n",
    "        queries = self.build_search_queries(affiliation_variations)\n",
    "        all_articles = []\n",
    "        seen_pmids = set()\n",
    "        \n",
    "        for i, query in enumerate(queries):\n",
    "            print(f\"\\nüîç Running search {i+1}/{len(queries)}\")\n",
    "            articles = self.search_pubmed(query, max_per_query)\n",
    "            \n",
    "            # Deduplicate\n",
    "            new_articles = []\n",
    "            for article in articles:\n",
    "                if article['pmid'] not in seen_pmids:\n",
    "                    seen_pmids.add(article['pmid'])\n",
    "                    new_articles.append(article)\n",
    "            \n",
    "            all_articles.extend(new_articles)\n",
    "            print(f\"Added {len(new_articles)} new articles (total: {len(all_articles)})\")\n",
    "            \n",
    "            time.sleep(1)  # Be respectful to NCBI\n",
    "            \n",
    "        return all_articles\n",
    "# Run comprehensive PubMed search\n",
    "print(\"üîç Starting comprehensive PubMed search...\")\n",
    "searcher = PubmedSearcher()\n",
    "\n",
    "# Test with a single query first\n",
    "test_articles = searcher.search_pubmed(\"Instituto de Fisiologia Celular[Affiliation]\", max_results=5)\n",
    "print(f\"\\nüìä Test search found {len(test_articles)} articles\")\n",
    "\n",
    "if test_articles:\n",
    "    print(\"\\nSample result:\")\n",
    "    sample = test_articles[0]\n",
    "    print(f\"Title: {sample['title'][:100]}...\")\n",
    "    print(f\"Authors: {sample['authors'][:100]}...\")\n",
    "    print(f\"PMID: {sample['pmid']}\")\n",
    "\n",
    "new_articles = searcher.comprehensive_search(max_per_query=20)\n",
    "print(f\"\\nüéâ Found {len(new_articles)} total unique articles from PubMed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd3966",
   "metadata": {},
   "source": [
    "## Using Pre-Filtered Affiliations for PubMed Search\n",
    "\n",
    "Instead of manually specifying affiliation variations, we'll now use the automatically filtered and scored affiliations from our filtering system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-filtered affiliation results\n",
    "import json\n",
    "\n",
    "def load_filtered_affiliations(min_score=15.0):\n",
    "    \"\"\"\n",
    "    Load pre-filtered affiliation clusters for PubMed searches.\n",
    "    \n",
    "    Args:\n",
    "        min_score: Minimum relevance score to include (default: 15.0 for high quality)\n",
    "        \n",
    "    Returns:\n",
    "        List of affiliation terms optimized for PubMed searches\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_file = 'data/processed/filtered_affiliations.json'\n",
    "    \n",
    "    print(f\"üìÅ Loading filtered affiliations from {filtered_file}\")\n",
    "    \n",
    "    with open(filtered_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    clusters = data['relevant_affiliation_clusters']\n",
    "    \n",
    "    # Filter by score and extract search terms\n",
    "    affiliation_terms = []\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        if cluster['relevance_score'] >= min_score:\n",
    "            \n",
    "            # Add representative term\n",
    "            representative = clean_affiliation_for_search(cluster['representative'])\n",
    "            if representative:\n",
    "                affiliation_terms.append(f'\"{representative}\"[Affiliation]')\n",
    "            \n",
    "            # Add top variations (limit to avoid too many terms)\n",
    "            for variation in cluster['variations'][:3]:  # Top 3 variations per cluster\n",
    "                cleaned = clean_affiliation_for_search(variation)\n",
    "                if cleaned and len(cleaned) > 10:  # Only substantial terms\n",
    "                    search_term = f'\"{cleaned}\"[Affiliation]'\n",
    "                    if search_term not in affiliation_terms:  # Avoid duplicates\n",
    "                        affiliation_terms.append(search_term)\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(affiliation_terms)} affiliation search terms from {len([c for c in clusters if c['relevance_score'] >= min_score])} high-scoring clusters\")\n",
    "    \n",
    "    return affiliation_terms\n",
    "\n",
    "def clean_affiliation_for_search(term):\n",
    "    \"\"\"Clean an affiliation term for PubMed search.\"\"\"\n",
    "    import re\n",
    "    \n",
    "    if not term:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove common noise patterns\n",
    "    cleaned = re.sub(r'[‚Ä¢\\d]+\\s*', '', term)  # Remove bullets and leading numbers\n",
    "    cleaned = re.sub(r'[^\\w\\s\\-,.]', ' ', cleaned)  # Remove special chars except basic punctuation\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)  # Normalize whitespace\n",
    "    cleaned = cleaned.strip()\n",
    "    \n",
    "    # Remove very generic prefixes\n",
    "    prefixes_to_remove = ['the ', 'a ', 'an ', 'at the ', 'from the ']\n",
    "    for prefix in prefixes_to_remove:\n",
    "        if cleaned.lower().startswith(prefix):\n",
    "            cleaned = cleaned[len(prefix):]\n",
    "    \n",
    "    # Skip very short or generic terms\n",
    "    if len(cleaned) < 8:\n",
    "        return \"\"\n",
    "    \n",
    "    generic_terms = ['university', 'institute', 'department', 'school', 'college']\n",
    "    if cleaned.lower() in generic_terms:\n",
    "        return \"\"\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Load the filtered affiliations\n",
    "print(\"üîç LOADING PRE-FILTERED AFFILIATIONS FOR PUBMED SEARCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# You can adjust the min_score based on your needs:\n",
    "# - 20.0+: Only the highest quality, most relevant affiliations\n",
    "# - 15.0+: High quality affiliations (recommended)\n",
    "# - 10.0+: Moderate quality, more inclusive\n",
    "# - 5.0+: Liberal, includes more possibilities\n",
    "\n",
    "filtered_affiliations = load_filtered_affiliations(min_score=15.0)\n",
    "\n",
    "print(f\"\\nüìã TOP 10 AFFILIATION SEARCH TERMS:\")\n",
    "for i, term in enumerate(filtered_affiliations[:10], 1):\n",
    "    print(f\"{i:2d}. {term}\")\n",
    "\n",
    "if len(filtered_affiliations) > 10:\n",
    "    print(f\"    ... and {len(filtered_affiliations) - 10} more terms\")\n",
    "\n",
    "print(f\"\\nüí° You can adjust min_score to get more or fewer terms:\")\n",
    "print(f\"   - Current (15.0): {len(filtered_affiliations)} terms\")\n",
    "print(f\"   - If 10.0: {len(load_filtered_affiliations(10.0))} terms\") \n",
    "print(f\"   - If 20.0: {len(load_filtered_affiliations(20.0))} terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647707ae",
   "metadata": {},
   "source": [
    "### Enhanced PubMed Search with Filtered Affiliations\n",
    "\n",
    "Now we'll modify the PubMed searcher to use our filtered affiliations instead of hardcoded ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb80e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced PubMed Searcher using filtered affiliations\n",
    "class EnhancedPubmedSearcher(PubmedSearcher):\n",
    "    \"\"\"Enhanced PubMed searcher that uses pre-filtered affiliation results.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.filtered_affiliations = None\n",
    "    \n",
    "    def load_affiliations(self, min_score=15.0):\n",
    "        \"\"\"Load filtered affiliations for searches.\"\"\"\n",
    "        self.filtered_affiliations = load_filtered_affiliations(min_score)\n",
    "        return self.filtered_affiliations\n",
    "    \n",
    "    def build_smart_search_queries(self, max_terms_per_query=5):\n",
    "        \"\"\"\n",
    "        Build intelligent search queries using filtered affiliations.\n",
    "        \n",
    "        Args:\n",
    "            max_terms_per_query: Maximum affiliation terms per query to avoid overly complex searches\n",
    "            \n",
    "        Returns:\n",
    "            List of optimized search queries\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.filtered_affiliations:\n",
    "            print(\"‚ö†Ô∏è  No filtered affiliations loaded. Loading with default score...\")\n",
    "            self.load_affiliations()\n",
    "        \n",
    "        affiliations = self.filtered_affiliations\n",
    "        queries = []\n",
    "        \n",
    "        # Strategy 1: High-priority terms (top scoring affiliations)\n",
    "        high_priority = affiliations[:max_terms_per_query]\n",
    "        if high_priority:\n",
    "            priority_query = f\"({' OR '.join(high_priority)})\"\n",
    "            queries.append(priority_query)\n",
    "        \n",
    "        # Strategy 2: Batch remaining terms to avoid overwhelming single queries\n",
    "        remaining_terms = affiliations[max_terms_per_query:]\n",
    "        \n",
    "        for i in range(0, len(remaining_terms), max_terms_per_query):\n",
    "            batch = remaining_terms[i:i + max_terms_per_query]\n",
    "            if batch:\n",
    "                batch_query = f\"({' OR '.join(batch)})\"\n",
    "                queries.append(batch_query)\n",
    "        \n",
    "        # Strategy 3: Add time-filtered searches for high-priority terms\n",
    "        if high_priority:\n",
    "            recent_query = f\"({' OR '.join(high_priority[:3])}) AND (2020:2024[pdat])\"\n",
    "            historical_query = f\"({' OR '.join(high_priority[:3])}) AND (2010:2019[pdat])\"\n",
    "            queries.extend([recent_query, historical_query])\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(queries)} optimized search queries\")\n",
    "        return queries\n",
    "    \n",
    "    def comprehensive_filtered_search(self, min_score=15.0, max_per_query=50, max_terms_per_query=5):\n",
    "        \"\"\"\n",
    "        Run comprehensive search using filtered affiliations.\n",
    "        \n",
    "        Args:\n",
    "            min_score: Minimum relevance score for affiliations to include\n",
    "            max_per_query: Maximum results per individual query\n",
    "            max_terms_per_query: Maximum affiliation terms per query\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with search results and metadata\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üîç COMPREHENSIVE PUBMED SEARCH WITH FILTERED AFFILIATIONS\")\n",
    "        print(\"=\" * 65)\n",
    "        \n",
    "        # Load affiliations\n",
    "        affiliations = self.load_affiliations(min_score)\n",
    "        print(f\"üìä Using {len(affiliations)} filtered affiliation terms (score >= {min_score})\")\n",
    "        \n",
    "        # Build queries\n",
    "        queries = self.build_smart_search_queries(max_terms_per_query)\n",
    "        \n",
    "        # Execute searches\n",
    "        all_articles = []\n",
    "        seen_pmids = set()\n",
    "        query_results = []\n",
    "        \n",
    "        for i, query in enumerate(queries):\n",
    "            print(f\"\\nüîç Query {i+1}/{len(queries)}: {query[:100]}{'...' if len(query) > 100 else ''}\")\n",
    "            \n",
    "            try:\n",
    "                articles = self.search_pubmed(query, max_per_query)\n",
    "                \n",
    "                # Deduplicate\n",
    "                new_articles = []\n",
    "                for article in articles:\n",
    "                    if article['pmid'] not in seen_pmids:\n",
    "                        seen_pmids.add(article['pmid'])\n",
    "                        new_articles.append(article)\n",
    "                \n",
    "                all_articles.extend(new_articles)\n",
    "                \n",
    "                query_result = {\n",
    "                    'query': query,\n",
    "                    'total_found': len(articles),\n",
    "                    'new_articles': len(new_articles),\n",
    "                    'cumulative_total': len(all_articles)\n",
    "                }\n",
    "                query_results.append(query_result)\n",
    "                \n",
    "                print(f\"   Found: {len(articles)} | New: {len(new_articles)} | Total: {len(all_articles)}\")\n",
    "                \n",
    "                time.sleep(1)  # Rate limiting\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\nüéâ SEARCH COMPLETE!\")\n",
    "        print(f\"üìä Total unique articles found: {len(all_articles)}\")\n",
    "        print(f\"üîç Queries executed: {len([r for r in query_results if r['total_found'] > 0])}/{len(queries)}\")\n",
    "        \n",
    "        # Return comprehensive results\n",
    "        return {\n",
    "            'articles': all_articles,\n",
    "            'search_metadata': {\n",
    "                'total_articles': len(all_articles),\n",
    "                'affiliations_used': len(affiliations),\n",
    "                'min_score_threshold': min_score,\n",
    "                'queries_executed': len(queries),\n",
    "                'query_results': query_results\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize the enhanced searcher\n",
    "print(\"üöÄ INITIALIZING ENHANCED PUBMED SEARCHER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "enhanced_searcher = EnhancedPubmedSearcher()\n",
    "\n",
    "# Test with a small search first\n",
    "print(\"\\nüß™ TESTING WITH SMALL SEARCH...\")\n",
    "test_affiliations = enhanced_searcher.load_affiliations(min_score=20.0)  # Very high score for testing\n",
    "print(f\"Test will use {len(test_affiliations)} highest-scoring affiliations\")\n",
    "\n",
    "if test_affiliations:\n",
    "    print(f\"\\nTop 3 test affiliations:\")\n",
    "    for i, aff in enumerate(test_affiliations[:3], 1):\n",
    "        print(f\"  {i}. {aff}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No affiliations found with score >= 20.0, try lower score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09459342",
   "metadata": {},
   "source": [
    "### Run the Enhanced Search\n",
    "\n",
    "Choose your search strategy based on your needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf64204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE YOUR SEARCH STRATEGY\n",
    "# Uncomment the option you want to use:\n",
    "\n",
    "# Option 1: CONSERVATIVE - High precision, fewer results\n",
    "# Uses only the highest-scoring affiliations (20.0+)\n",
    "# Best for: High-quality, highly relevant results\n",
    "# search_results = enhanced_searcher.comprehensive_filtered_search(\n",
    "#     min_score=20.0,      # Only top-quality affiliations\n",
    "#     max_per_query=30,    # Fewer results per query  \n",
    "#     max_terms_per_query=3 # Simpler queries\n",
    "# )\n",
    "\n",
    "# Option 2: BALANCED - Good balance of precision and recall (RECOMMENDED)\n",
    "# Uses high-scoring affiliations (15.0+)  \n",
    "# Best for: Most use cases\n",
    "search_results = enhanced_searcher.comprehensive_filtered_search(\n",
    "    min_score=15.0,      # High-quality affiliations\n",
    "    max_per_query=50,    # Moderate results per query\n",
    "    max_terms_per_query=5 # Balanced query complexity\n",
    ")\n",
    "\n",
    "# Option 3: LIBERAL - Higher recall, more results\n",
    "# Uses moderately-scoring affiliations (10.0+)\n",
    "# Best for: Comprehensive literature review, exploratory research\n",
    "# search_results = enhanced_searcher.comprehensive_filtered_search(\n",
    "#     min_score=10.0,      # More inclusive\n",
    "#     max_per_query=75,    # More results per query\n",
    "#     max_terms_per_query=7 # More complex queries\n",
    "# )\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä SEARCH RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "articles = search_results['articles']\n",
    "metadata = search_results['search_metadata']\n",
    "\n",
    "print(f\"üéØ Total Articles Found: {len(articles)}\")\n",
    "print(f\"üîç Affiliations Used: {metadata['affiliations_used']} (score >= {metadata['min_score_threshold']})\")\n",
    "print(f\"‚öôÔ∏è  Queries Executed: {metadata['queries_executed']}\")\n",
    "\n",
    "if articles:\n",
    "    print(f\"\\nüìñ Sample Results:\")\n",
    "    for i, article in enumerate(articles[:3], 1):\n",
    "        print(f\"\\n{i}. {article['title'][:100]}{'...' if len(article['title']) > 100 else ''}\")\n",
    "        print(f\"   Authors: {article['authors'][:80]}{'...' if len(article['authors']) > 80 else ''}\")\n",
    "        print(f\"   Journal: {article['journal']} ({article['year']})\")\n",
    "        print(f\"   PMID: {article['pmid']}\")\n",
    "        \n",
    "    if len(articles) > 3:\n",
    "        print(f\"\\n   ... and {len(articles) - 3} more articles\")\n",
    "\n",
    "# Query performance breakdown\n",
    "print(f\"\\nüîç Query Performance:\")\n",
    "successful_queries = [q for q in metadata['query_results'] if q['total_found'] > 0]\n",
    "print(f\"Successful queries: {len(successful_queries)}/{len(metadata['query_results'])}\")\n",
    "\n",
    "for i, query_result in enumerate(successful_queries[:5], 1):  # Show top 5 performing queries\n",
    "    print(f\"{i}. Found {query_result['total_found']} articles ({query_result['new_articles']} new)\")\n",
    "    print(f\"   Query: {query_result['query'][:80]}{'...' if len(query_result['query']) > 80 else ''}\")\n",
    "\n",
    "print(f\"\\nüíæ Next Steps:\")\n",
    "print(f\"1. Review the {len(articles)} articles found\")\n",
    "print(f\"2. Save results to your database\")\n",
    "print(f\"3. Run text extraction and analysis on promising articles\")\n",
    "print(f\"4. Adjust search parameters if needed (min_score, max_per_query, etc.)\")\n",
    "\n",
    "# Save results for later use\n",
    "if articles:\n",
    "    print(f\"\\nüíæ Would you like to save these results?\")\n",
    "    # Uncomment to save:\n",
    "    # import json\n",
    "    # with open('data/processed/pubmed_filtered_search_results.json', 'w') as f:\n",
    "    #     json.dump(search_results, f, indent=2)\n",
    "    # print(\"‚úÖ Results saved to data/processed/pubmed_filtered_search_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df88f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c93f53bd",
   "metadata": {},
   "source": [
    "## 5. Database Integration & Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a62ae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Database expansion simulation\n",
      "Current database size: 2\n",
      "Processing 1 potential new publications...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 87\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Create some demo \"new\" publications\u001b[39;00m\n\u001b[1;32m     75\u001b[0m demo_new_pubs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     76\u001b[0m     {\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpmid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m99999999\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     }\n\u001b[1;32m     85\u001b[0m ]\n\u001b[0;32m---> 87\u001b[0m expanded_db \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_publication_databases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublications\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemo_new_pubs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 38\u001b[0m, in \u001b[0;36mmerge_publication_databases\u001b[0;34m(existing_pubs, new_pubs, output_file)\u001b[0m\n\u001b[1;32m     23\u001b[0m     is_duplicate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_duplicate:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Convert PubMed format to your format\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     converted_pub \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: pub\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors\u001b[39m\u001b[38;5;124m'\u001b[39m: pub\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjournal\u001b[39m\u001b[38;5;124m'\u001b[39m: pub\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjournal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m: pub\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoi\u001b[39m\u001b[38;5;124m'\u001b[39m: pub\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoi\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpubmed_id\u001b[39m\u001b[38;5;124m'\u001b[39m: pub\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpmid\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mifc_url\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Not available from PubMed\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m: pub\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_text\u001b[39m\u001b[38;5;124m'\u001b[39m: pub\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m pub\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),  \u001b[38;5;66;03m# Text for embeddings\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords_extracted\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mextract_keywords\u001b[49m(pub\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m pub\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPubMed_search\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas_full_text\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Set to True when full text is available\u001b[39;00m\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maffiliation_matched\u001b[39m\u001b[38;5;124m'\u001b[39m: pub\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maffiliation_matched\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Store which affiliation matched                \u001b[39;00m\n\u001b[1;32m     43\u001b[0m     }\n\u001b[1;32m     44\u001b[0m }\n\u001b[1;32m     46\u001b[0m     merged_pubs\u001b[38;5;241m.\u001b[39mappend(converted_pub)\n\u001b[1;32m     47\u001b[0m     new_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_keywords' is not defined"
     ]
    }
   ],
   "source": [
    "def merge_publication_databases(existing_pubs, new_pubs, output_file='../data/processed/expanded_ifc_publications.json'):\n",
    "    \"\"\"Merge existing publications with newly found ones, removing duplicates\"\"\"\n",
    "    \n",
    "    # Create lookup sets for deduplication\n",
    "    existing_dois = {pub.get('doi') for pub in existing_pubs if pub.get('doi')}\n",
    "    existing_pmids = {pub.get('pubmed_id') for pub in existing_pubs if pub.get('pubmed_id')}\n",
    "    existing_titles = {pub.get('title', '').lower().strip() for pub in existing_pubs if pub.get('title')}\n",
    "    \n",
    "    merged_pubs = existing_pubs.copy()\n",
    "    new_count = 0\n",
    "    \n",
    "    print(f\"Processing {len(new_pubs)} potential new publications...\")\n",
    "    \n",
    "    for pub in new_pubs:\n",
    "        is_duplicate = False\n",
    "        \n",
    "        # Check for duplicates\n",
    "        if pub.get('doi') and pub['doi'] in existing_dois:\n",
    "            is_duplicate = True\n",
    "        elif pub.get('pmid') and pub['pmid'] in existing_pmids:\n",
    "            is_duplicate = True\n",
    "        elif pub.get('title', '').lower().strip() in existing_titles:\n",
    "            is_duplicate = True\n",
    "            \n",
    "        if not is_duplicate:\n",
    "            # Convert PubMed format to your format\n",
    "            converted_pub = {\n",
    "                'title': pub.get('title', ''),\n",
    "                'authors': pub.get('authors', ''),\n",
    "                'journal': pub.get('journal', ''),\n",
    "                'year': pub.get('year'),\n",
    "                'doi': pub.get('doi'),\n",
    "                'pubmed_id': pub.get('pmid'),\n",
    "                'ifc_url': None,  # Not available from PubMed\n",
    "                'abstract': pub.get('abstract', ''),\n",
    "                'keywords': None,\n",
    "                'embedding_text': pub.get('abstract', '') + \" \" + pub.get('title', ''),  # Text for embeddings\n",
    "                'keywords_extracted': extract_keywords(pub.get('abstract', '') + \" \" + pub.get('title', '')),\n",
    "                'metadata': {\n",
    "                    'source': 'PubMed_search',\n",
    "                    'has_full_text': False,  # Set to True when full text is available\n",
    "                    'affiliation_matched': pub.get('affiliation_matched', 'Unknown')  # Store which affiliation matched                \n",
    "            }\n",
    "        }\n",
    "            \n",
    "            merged_pubs.append(converted_pub)\n",
    "            new_count += 1\n",
    "            \n",
    "            # Update tracking sets (FIXED - removed erroneous import)\n",
    "            if pub.get('doi'):\n",
    "                existing_dois.add(pub['doi'])\n",
    "            if pub.get('pmid'):\n",
    "                existing_pmids.add(pub['pmid'])\n",
    "            existing_titles.add(pub.get('title', '').lower().strip())\n",
    "    \n",
    "    \n",
    "    # Save expanded database\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_pubs, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nüìä Database expansion complete:\")\n",
    "    print(f\"   Original publications: {len(existing_pubs)}\")\n",
    "    print(f\"   New publications added: {new_count}\")\n",
    "    print(f\"   Total publications: {len(merged_pubs)}\")\n",
    "    print(f\"   Saved to: {output_file}\")\n",
    "    \n",
    "    return merged_pubs\n",
    "\n",
    "# Demo with existing data\n",
    "print(\"üìà Database expansion simulation\")\n",
    "print(f\"Current database size: {len(publications)}\")\n",
    "\n",
    "# Create some demo \"new\" publications\n",
    "demo_new_pubs = [\n",
    "    {\n",
    "        'pmid': '99999999',\n",
    "        'title': 'Demo paper: Synaptic plasticity in hippocampal circuits',\n",
    "        'authors': 'Demo Author, A.; Demo Author, B.',\n",
    "        'journal': 'Demo Journal of Neuroscience',\n",
    "        'year': 2023,\n",
    "        'abstract': 'This is a demo abstract about hippocampal synaptic plasticity...',\n",
    "        'doi': '10.1234/demo.2023.001'\n",
    "    }\n",
    "]\n",
    "\n",
    "expanded_db = merge_publication_databases(publications, demo_new_pubs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing merge with full IFC database to evaluate method effectiveness\n",
      "\n",
      "‚úÖ Loaded full IFC database: 404 publications\n",
      "üìä Comparison Analysis:\n",
      "   Full IFC database: 404 publications\n",
      "   PubMed search found: 114 publications\n",
      "\n",
      "   Starting merge effectiveness analysis...\n",
      "   Building lookup tables for faster matching...\n",
      "\n",
      "   Processing publications:\n",
      "   Progress: 114/114 (100.0%)\n",
      "   Processing complete!                           \n",
      "\n",
      "üéØ Method Effectiveness Analysis:\n",
      "   üìà Publications found by PubMed: 114\n",
      "   ‚ú® Truly new publications: 89\n",
      "   üîÑ Duplicates found: 25\n",
      "      - By DOI: 23\n",
      "      - By PMID: 2\n",
      "      - By Title: 0\n",
      "   üìä Method effectiveness: 78.1% new content\n",
      "\n",
      "üìã Sample of new publications found:\n",
      "   1. Multistable bimodal perceptual coding within the ventral premotor cortex.\n",
      "      Journal: Science advances, Year: 2025\n",
      "      DOI: 10.1126/sciadv.adw5500\n",
      "\n",
      "   2. Inhibition of the oncogenic channel Kv10.1 by the antipsychotic drug penfluridol.\n",
      "      Journal: Frontiers in pharmacology, Year: 2025\n",
      "      DOI: 10.3389/fphar.2025.1655406\n",
      "\n",
      "   3. Interleukin-1Œ≤ depresses neuronal activity in the rat olfactory bulb even during odor stimulation.\n",
      "      Journal: PloS one, Year: 2025\n",
      "      DOI: 10.1371/journal.pone.0332592\n",
      "\n",
      "   4. Transcriptomic profiling of \n",
      "      Journal: Applied and environmental microbiology, Year: 2025\n",
      "      DOI: 10.1128/aem.01557-25\n",
      "\n",
      "   5. Globally accessible platforms for the exchange of research findings and career development.\n",
      "      Journal: Nature neuroscience, Year: 2025\n",
      "      DOI: 10.1038/s41593-025-02063-5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test merge with the full IFC database to evaluate effectiveness\n",
    "print(\"üß™ Testing merge with full IFC database to evaluate method effectiveness\\n\")\n",
    "\n",
    "# Load the full IFC database\n",
    "try:\n",
    "    with open('../data/raw/all_ifc_publications.json', 'r', encoding='utf-8') as f:\n",
    "        full_ifc_db = json.load(f)\n",
    "    print(f\"‚úÖ Loaded full IFC database: {len(full_ifc_db)} publications\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Could not find '../data/raw/all_ifc_publications.json'\")\n",
    "    print(\"   Please check the file path\")\n",
    "    full_ifc_db = []\n",
    "\n",
    "if full_ifc_db and new_articles:\n",
    "    print(f\"üìä Comparison Analysis:\")\n",
    "    print(f\"   Full IFC database: {len(full_ifc_db)} publications\")\n",
    "    print(f\"   PubMed search found: {len(new_articles)} publications\")\n",
    "    \n",
    "    # Test merge (in memory only)\n",
    "    def test_merge_effectiveness(existing_pubs, new_pubs):\n",
    "        \"\"\"Test merge to evaluate method effectiveness without saving\"\"\"\n",
    "        \n",
    "        # Create lookup sets for deduplication (do this once, not in the loop)\n",
    "        print(\"   Building lookup tables for faster matching...\")\n",
    "        existing_dois = {pub.get('doi') for pub in existing_pubs if pub.get('doi')}\n",
    "        existing_pmids = {pub.get('pubmed_id') for pub in existing_pubs if pub.get('pubmed_id')}\n",
    "        \n",
    "        # Pre-process titles once for performance\n",
    "        existing_titles = set()\n",
    "        for pub in existing_pubs:\n",
    "            if pub.get('title'):\n",
    "                existing_titles.add(pub.get('title', '').lower().strip())\n",
    "        \n",
    "        # Analyze new publications\n",
    "        new_count = 0\n",
    "        duplicate_by_doi = 0\n",
    "        duplicate_by_pmid = 0\n",
    "        duplicate_by_title = 0\n",
    "        truly_new = []\n",
    "        \n",
    "        # Process with progress indicator\n",
    "        print(\"\\n   Processing publications:\")\n",
    "        total_new = len(new_pubs)\n",
    "        update_interval = max(1, min(100, total_new // 10))  # Show 10 updates or every item for small sets\n",
    "        \n",
    "        for i, pub in enumerate(new_pubs):\n",
    "            # Show progress periodically\n",
    "            if i % update_interval == 0 or i == total_new - 1:\n",
    "                progress = (i + 1) / total_new * 100\n",
    "                print(f\"   Progress: {i+1}/{total_new} ({progress:.1f}%)\", end=\"\\r\")\n",
    "            \n",
    "            is_duplicate = False\n",
    "            duplicate_reason = \"\"\n",
    "            \n",
    "            # Check for duplicates with detailed tracking - check fastest methods first\n",
    "            if pub.get('doi') and pub['doi'] in existing_dois:\n",
    "                is_duplicate = True\n",
    "                duplicate_reason = \"DOI match\"\n",
    "                duplicate_by_doi += 1\n",
    "            elif pub.get('pmid') and pub['pmid'] in existing_pmids:\n",
    "                is_duplicate = True\n",
    "                duplicate_reason = \"PMID match\"\n",
    "                duplicate_by_pmid += 1\n",
    "            elif pub.get('title'):\n",
    "                title_lower = pub.get('title', '').lower().strip()\n",
    "                if title_lower in existing_titles:\n",
    "                    is_duplicate = True\n",
    "                    duplicate_reason = \"Title match\"\n",
    "                    duplicate_by_title += 1\n",
    "                \n",
    "            if not is_duplicate:\n",
    "                # This is a new publication\n",
    "                truly_new.append({\n",
    "                    'title': pub.get('title', ''),\n",
    "                    'authors': pub.get('authors', ''),\n",
    "                    'journal': pub.get('journal', ''),\n",
    "                    'year': pub.get('year'),\n",
    "                    'doi': pub.get('doi'),\n",
    "                    'pubmed_id': pub.get('pmid'),\n",
    "                    'abstract': pub.get('abstract', ''),\n",
    "                    'source': 'PubMed_search'\n",
    "                })\n",
    "                new_count += 1\n",
    "        \n",
    "        print(\"\\n   Processing complete!                           \")  # Clear progress line\n",
    "        \n",
    "        return {\n",
    "            'total_found': len(new_pubs),\n",
    "            'truly_new': new_count,\n",
    "            'truly_new_articles': truly_new,\n",
    "            'duplicates': {\n",
    "                'by_doi': duplicate_by_doi,\n",
    "                'by_pmid': duplicate_by_pmid,\n",
    "                'by_title': duplicate_by_title,\n",
    "                'total': len(new_pubs) - new_count\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Perform test merge\n",
    "    print(\"\\n   Starting merge effectiveness analysis...\")\n",
    "    merge_results = test_merge_effectiveness(full_ifc_db, new_articles)\n",
    "    \n",
    "    print(f\"\\nüéØ Method Effectiveness Analysis:\")\n",
    "    print(f\"   üìà Publications found by PubMed: {merge_results['total_found']}\")\n",
    "    print(f\"   ‚ú® Truly new publications: {merge_results['truly_new']}\")\n",
    "    print(f\"   üîÑ Duplicates found: {merge_results['duplicates']['total']}\")\n",
    "    print(f\"      - By DOI: {merge_results['duplicates']['by_doi']}\")\n",
    "    print(f\"      - By PMID: {merge_results['duplicates']['by_pmid']}\")\n",
    "    print(f\"      - By Title: {merge_results['duplicates']['by_title']}\")\n",
    "    \n",
    "    effectiveness_rate = (merge_results['truly_new'] / merge_results['total_found']) * 100 if merge_results['total_found'] > 0 else 0\n",
    "    print(f\"   üìä Method effectiveness: {effectiveness_rate:.1f}% new content\")\n",
    "    \n",
    "    # Show sample of new publications\n",
    "    if merge_results['truly_new'] > 0:\n",
    "        print(f\"\\nüìã Sample of new publications found:\")\n",
    "        sample_size = min(5, merge_results['truly_new'])\n",
    "        for i, pub in enumerate(merge_results['truly_new_articles'][:sample_size]):\n",
    "            print(f\"   {i+1}. {pub['title']}\")\n",
    "            print(f\"      Journal: {pub['journal']}, Year: {pub['year']}\")\n",
    "            print(f\"      DOI: {pub['doi']}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot perform analysis - missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b1128",
   "metadata": {},
   "source": [
    "## 6. Automated Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe4697",
   "metadata": {},
   "source": [
    " Performance Optimizations for Large Datasets\n",
    "For handling larger databases,  adding indexes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_publication_indexes(publications):\n",
    "    \"\"\"Create index structures for faster lookup\"\"\"\n",
    "    indexes = {\n",
    "        'doi': {},\n",
    "        'pmid': {},\n",
    "        'title_lower': {},\n",
    "        'year': {},\n",
    "        'journal': {}\n",
    "    }\n",
    "    \n",
    "    for i, pub in enumerate(publications):\n",
    "        if pub.get('doi'):\n",
    "            indexes['doi'][pub['doi']] = i\n",
    "        if pub.get('pubmed_id'):\n",
    "            indexes['pmid'][pub['pubmed_id']] = i\n",
    "        if pub.get('title'):\n",
    "            indexes['title_lower'][pub['title'].lower().strip()] = i\n",
    "        if pub.get('year'):\n",
    "            year = pub['year']\n",
    "            if year not in indexes['year']:\n",
    "                indexes['year'][year] = []\n",
    "            indexes['year'][year].append(i)\n",
    "        if pub.get('journal'):\n",
    "            journal = pub['journal']\n",
    "            if journal not in indexes['journal']:\n",
    "                indexes['journal'][journal] = []\n",
    "            indexes['journal'][journal].append(i)\n",
    "    \n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53666925",
   "metadata": {},
   "source": [
    "adding keywords for better embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85697688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text, max_keywords=10):\n",
    "    \"\"\"Extract key terms from text for better embedding search\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "        \n",
    "    try:\n",
    "        # Simple frequency-based extraction\n",
    "        # In a real application, consider using more sophisticated methods\n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,15}\\b', text.lower())\n",
    "        word_freq = Counter(words)\n",
    "        \n",
    "        # Filter out common words\n",
    "        common_words = {'the', 'and', 'was', 'were', 'with', 'for', 'this', 'that'}\n",
    "        for word in common_words:\n",
    "            if word in word_freq:\n",
    "                del word_freq[word]\n",
    "                \n",
    "        # Return top keywords\n",
    "        return [word for word, _ in word_freq.most_common(max_keywords)]\n",
    "    except Exception:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d6c87",
   "metadata": {},
   "source": [
    "include text fields (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3096b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When preparing for ChromaDB\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "for i, pub in enumerate(final_db):\n",
    "    # Combine text fields for embedding\n",
    "    doc_text = pub.get('title', '') + \" \" + pub.get('abstract', '')\n",
    "    if pub.get('full_text'):\n",
    "        doc_text += \" \" + pub.get('full_text')\n",
    "    \n",
    "    # Add to lists\n",
    "    documents.append(doc_text)\n",
    "    metadatas.append({\n",
    "        'title': pub.get('title'),\n",
    "        'year': pub.get('year'),\n",
    "        'journal': pub.get('journal'),\n",
    "        'authors': pub.get('authors'),\n",
    "        'keywords': pub.get('keywords_extracted', [])\n",
    "    })\n",
    "    ids.append(f\"pub_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c77deb",
   "metadata": {},
   "source": [
    "data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(publications):\n",
    "    \"\"\"Check database quality before embedding\"\"\"\n",
    "    issues = {\n",
    "        'missing_title': 0,\n",
    "        'missing_abstract': 0,\n",
    "        'missing_year': 0,\n",
    "        'missing_authors': 0,\n",
    "        'short_text': 0\n",
    "    }\n",
    "    \n",
    "    for pub in publications:\n",
    "        if not pub.get('title'):\n",
    "            issues['missing_title'] += 1\n",
    "        if not pub.get('abstract'):\n",
    "            issues['missing_abstract'] += 1\n",
    "        if not pub.get('year'):\n",
    "            issues['missing_year'] += 1\n",
    "        if not pub.get('authors'):\n",
    "            issues['missing_authors'] += 1\n",
    "        \n",
    "        # Check if there's enough text to create meaningful embeddings\n",
    "        text_len = len((pub.get('title', '') + \" \" + pub.get('abstract', '')).split())\n",
    "        if text_len < 30:  # Arbitrary threshold\n",
    "            issues['short_text'] += 1\n",
    "    \n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "publication type classification to improve search capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_publication_type(title, abstract):\n",
    "    \"\"\"Simple rule-based classification of publication type\"\"\"\n",
    "    text = (title + \" \" + abstract).lower()\n",
    "    \n",
    "    if any(kw in text for kw in ['review', 'overview', 'survey']):\n",
    "        return 'Review'\n",
    "    elif any(kw in text for kw in ['trial', 'randomized', 'placebo']):\n",
    "        return 'Clinical Trial'\n",
    "    elif any(kw in text for kw in ['case report', 'patient case']):\n",
    "        return 'Case Report'\n",
    "    elif any(kw in text for kw in ['method', 'technique', 'protocol']):\n",
    "        return 'Methodology'\n",
    "    else:\n",
    "        return 'Research Article'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83401f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_pipeline_with_review(initial_json_path, pdf_dir, output_dir='../data/processed'):\n",
    "    \"\"\"Complete automated pipeline with affiliation review step\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting complete publication database expansion pipeline\\n\")\n",
    "    \n",
    "    # Step 1: Load existing data\n",
    "    print(\"üìÇ Step 1: Loading existing publications\")\n",
    "    with open(initial_json_path, 'r', encoding='utf-8') as f:\n",
    "        existing_pubs = json.load(f)\n",
    "    print(f\"   Loaded {len(existing_pubs)} existing publications\")\n",
    "    \n",
    "    # Step 2: Mine affiliations from PDFs and review\n",
    "    print(\"\\n\udd0d Step 2: Mining and reviewing affiliations from PDFs\")\n",
    "    review_results = analyze_pdfs_and_search_pubmed_with_review(\n",
    "        pdf_dir=pdf_dir,\n",
    "        output_dir=os.path.join(output_dir, 'affiliations'),\n",
    "        max_results_per_query=30\n",
    "    )\n",
    "    \n",
    "    new_articles = review_results.get('pubmed_articles', [])\n",
    "    print(f\"   Found {len(new_articles)} potential new articles\")\n",
    "    \n",
    "    # Step 3: Merge databases\n",
    "    print(\"\\nüîÑ Step 3: Merging and deduplicating databases\")\n",
    "    expanded_json_path = os.path.join(output_dir, 'expanded_ifc_publications.json')\n",
    "    final_db = merge_publication_databases(existing_pubs, new_articles, expanded_json_path)\n",
    "\n",
    "    # Step 5: Create final BibTeX\n",
    "    print(\"\\nüìö Step 5: Creating final BibTeX file\")\n",
    "    final_bibtex_path = os.path.join(output_dir, 'final_ifc_publications.bib')\n",
    "    create_bibtex_from_publications(final_db, final_bibtex_path)\n",
    "    \n",
    "    # Step 6: Generate summary report\n",
    "    print(\"\\nüìä Step 6: Generating summary report\")\n",
    "    report = {\n",
    "        'pipeline_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'original_count': len(existing_pubs),\n",
    "        'pubmed_found': len(new_articles),\n",
    "        'final_count': len(final_db),\n",
    "        'new_additions': len(final_db) - len(existing_pubs),\n",
    "        'files_created': {\n",
    "            'expanded_json': expanded_json_path,\n",
    "            # 'bibtex_original': bibtex_path, # optional\n",
    "            'bibtex_final': final_bibtex_path\n",
    "        },\n",
    "        'year_distribution': {},\n",
    "        'top_journals': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze year distribution\n",
    "    years = [pub.get('year') for pub in final_db if pub.get('year')]\n",
    "    year_counts = Counter(years)\n",
    "    report['year_distribution'] = dict(year_counts.most_common(10))\n",
    "    \n",
    "    # Analyze top journals\n",
    "    journals = [pub.get('journal') for pub in final_db if pub.get('journal')]\n",
    "    journal_counts = Counter(journals)\n",
    "    report['top_journals'] = dict(journal_counts.most_common(10))\n",
    "    \n",
    "    # Save report\n",
    "    report_path = os.path.join(output_dir, 'pipeline_report.json')\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pipeline complete! Summary:\")\n",
    "    print(f\"   üìä Original: {report['original_count']} publications\")\n",
    "    print(f\"   üÜï Added: {report['new_additions']} new publications\")\n",
    "    print(f\"   üìà Final: {report['final_count']} total publications\")\n",
    "    print(f\"   üìÑ Report saved: {report_path}\")\n",
    "    \n",
    "    return final_db, report\n",
    "\n",
    "def extract_and_store_full_text(publications_with_dois, pdf_dir, output_dir):\n",
    "    \"\"\"Extract full text from PDFs and store it with publications data\"\"\"   \n",
    "    # Track which publications have full text\n",
    "    has_full_text = set()\n",
    "    # Extract text from PDFs where available\n",
    "    pdf_texts = batch_process_pdfs(pdf_dir)\n",
    "    \n",
    "    # Match PDFs to publications by DOI\n",
    "    for pub in publications_with_dois:\n",
    "        if pub.get('doi'):\n",
    "            # Look for PDF with DOI in filename (PyPaperBot naming convention)\n",
    "            doi_filename = pub['doi'].replace('/', '_') + '.pdf'\n",
    "            if doi_filename in pdf_texts:\n",
    "                # Store text with publication\n",
    "                pub['full_text'] = pdf_texts[doi_filename]\n",
    "                pub['metadata']['has_full_text'] = True # Will fail if 'metadata' doesn't exist\n",
    "                has_full_text.add(pub['doi'])\n",
    "    \n",
    "    print(f\"Added full text to {len(has_full_text)} publications\")\n",
    "    return publications_with_dois\n",
    "\n",
    "# final_database, pipeline_report = run_complete_pipeline_with_review('../data/raw/test_ifc_publications.json')\n",
    "\n",
    "# print(\"\\nüéØ Pipeline ready! Uncomment the line above to run the complete workflow.\")\n",
    "# print(\"\\nNext steps:\")\n",
    "# print(\"1. Run this pipeline to expand your database\")\n",
    "# print(\"2. Import the BibTeX files into Zotero to download PDFs\")\n",
    "# print(\"3. Use the expanded JSON database for your ChromaDB embeddings\")\n",
    "# print(\"4. Run affiliation mining on downloaded PDFs to find more variations\")\n",
    "# print(\"5. Iterate to continuously expand your database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786356ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "initial_json_path = '../data/raw/all_ifc_publications.json'  # Your existing publications\n",
    "pdf_dir = '../papers/downloaded'  # Directory with PDFs for affiliation mining\n",
    "output_dir = '../data/processed'  # Where to save results\n",
    "\n",
    "# Run the complete pipeline\n",
    "final_database, pipeline_report = run_complete_pipeline_with_review(\n",
    "    initial_json_path=initial_json_path,\n",
    "    pdf_dir=pdf_dir,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# Print report summary\n",
    "print(f\"\\nExpanded database from {pipeline_report['original_count']} to {pipeline_report['final_count']} publications\")\n",
    "print(f\"Added {pipeline_report['new_additions']} new publications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436204cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract full text from PDFs and add to publications\n",
    "enriched_database = extract_and_store_full_text(\n",
    "    publications_with_dois=final_database,\n",
    "    pdf_dir=pdf_dir,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# Save the enriched database with full text\n",
    "enriched_output_path = os.path.join(output_dir, 'ifc_publications_with_fulltext.json')\n",
    "with open(enriched_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(enriched_database, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "print(f\"Saved enriched database with full text to: {enriched_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality\n",
    "quality_issues = check_data_quality(enriched_database)\n",
    "print(\"\\nData quality report:\")\n",
    "for issue, count in quality_issues.items():\n",
    "    percentage = (count / len(enriched_database)) * 100\n",
    "    print(f\"  ‚Ä¢ {issue}: {count} publications ({percentage:.1f}%)\")\n",
    "\n",
    "# Generate metadata about publication types for better search\n",
    "publication_types = Counter()\n",
    "for pub in enriched_database:\n",
    "    pub_type = classify_publication_type(\n",
    "        pub.get('title', ''), \n",
    "        pub.get('abstract', '')\n",
    "    )\n",
    "    publication_types[pub_type] += 1\n",
    "    \n",
    "    # Add the classification to the publication\n",
    "    if 'metadata' not in pub:\n",
    "        pub['metadata'] = {}\n",
    "    pub['metadata']['publication_type'] = pub_type\n",
    "\n",
    "print(\"\\nPublication type distribution:\")\n",
    "for pub_type, count in publication_types.most_common():\n",
    "    percentage = (count / len(enriched_database)) * 100\n",
    "    print(f\"  ‚Ä¢ {pub_type}: {count} publications ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6709a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
