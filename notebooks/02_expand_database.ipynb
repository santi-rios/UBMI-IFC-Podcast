{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11c74b3",
   "metadata": {},
   "source": [
    "# Building IFC Publications Database\n",
    "\n",
    "This notebook implements multiple strategies to build a comprehensive database of Instituto de Fisiolog√≠a Celular publications:\n",
    "\n",
    "1. **PDF Acquisition**: Sci-Hub integration + BibTeX export for Zotero\n",
    "2. **Affiliation Mining**: Extract all variations of institute names from existing PDFs\n",
    "3. **PubMed Search Strategy**: Use discovered affiliations to find more papers\n",
    "4. **Database Expansion**: Automated workflow to grow the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78786ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import bibtexparser\n",
    "from bibtexparser.bwriter import BibTexWriter\n",
    "from bibtexparser.bibdatabase import BibDatabase\n",
    "from pypdf import PdfReader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b7c3c",
   "metadata": {},
   "source": [
    "## 1. Load Existing Publications Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3866a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 publications\n",
      "Sample publication:\n",
      "{\n",
      "  \"title\": \"Neural mechanisms of memory formation in hippocampal circuits\",\n",
      "  \"authors\": \"Garc√≠a-L√≥pez, M., Rodr√≠guez-Silva, A., Mendoza-P√©rez, J.\",\n",
      "  \"journal\": \"Journal of Neuroscience\",\n",
      "  \"year\": 2024,\n",
      "  \"doi\": \"10.1523/JNEUROSCI.1234-24.2024\",\n",
      "  \"pubmed_id\": \"38123456\",\n",
      "  \"ifc_url\": \"https://www.ifc.unam.mx/publicacion.php?ut=000123456789\",\n",
      "  \"abstract\": \"We investigated the cellular and molecular mechanisms underlying memory formation in hippocampal circuits. Using electrophysiological recordings and optogenetic manipulations, we found that...\",\n",
      "  \"keywords\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load your existing publications\n",
    "with open('../data/raw/test_ifc_publications.json', 'r', encoding='utf-8') as f:\n",
    "    publications = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(publications)} publications\")\n",
    "print(\"Sample publication:\")\n",
    "print(json.dumps(publications[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e9272",
   "metadata": {},
   "source": [
    "## 2. PDF Acquisition Strategy\n",
    "\n",
    "### Option A: Sci-Hub Integration (own implementation)\n",
    "\n",
    "> ‚ö†Ô∏è USE OPTION C\n",
    "\n",
    "- Not tested for CAPTCHAs\n",
    "- Use method B or C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc8f9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1/2: Neural mechanisms of memory formation in hippocamp...\n",
      "‚ùå Failed to download 10.1523/JNEUROSCI.1234-24.2024\n",
      "Downloading 2/2: Cardiac physiology under metabolic stress conditio...\n",
      "‚ùå Failed to download 10.1093/cvr/cvz098\n"
     ]
    }
   ],
   "source": [
    "class PdfDownloader:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        \n",
    "    def download_from_scihub(self, doi, output_dir='../papers/downloaded'):\n",
    "        \"\"\"Download PDF from Sci-Hub using DOI\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Sci-Hub mirrors (these change frequently)\n",
    "        scihub_urls = [\n",
    "            'https://sci-hub.se/',\n",
    "            'https://sci-hub.st/',\n",
    "            'https://sci-hub.ru/',\n",
    "        ]\n",
    "        \n",
    "        for base_url in scihub_urls:\n",
    "            try:\n",
    "                url = f\"{base_url}{doi}\"\n",
    "                response = self.session.get(url, timeout=10)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Look for PDF download link\n",
    "                    if 'application/pdf' in response.headers.get('content-type', ''):\n",
    "                        # Direct PDF\n",
    "                        filename = f\"{doi.replace('/', '_')}.pdf\"\n",
    "                        filepath = os.path.join(output_dir, filename)\n",
    "                        \n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            f.write(response.content)\n",
    "                        \n",
    "                        return filepath\n",
    "                    \n",
    "                time.sleep(1)  # Be respectful\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed with {base_url}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return None\n",
    "    \n",
    "    def download_batch(self, publications, max_downloads=10):\n",
    "        \"\"\"Download multiple PDFs with rate limiting\"\"\"\n",
    "        downloaded = []\n",
    "        \n",
    "        for i, pub in enumerate(publications[:max_downloads]):\n",
    "            if 'doi' in pub and pub['doi']:\n",
    "                print(f\"Downloading {i+1}/{max_downloads}: {pub['title'][:50]}...\")\n",
    "                \n",
    "                filepath = self.download_from_scihub(pub['doi'])\n",
    "                if filepath:\n",
    "                    downloaded.append({\n",
    "                        'publication': pub,\n",
    "                        'pdf_path': filepath\n",
    "                    })\n",
    "                    print(f\"‚úÖ Downloaded to {filepath}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to download {pub['doi']}\")\n",
    "                    \n",
    "                time.sleep(2)  # Rate limiting\n",
    "                \n",
    "        return downloaded\n",
    "\n",
    "# Test with a small sample (2)\n",
    "downloader = PdfDownloader()\n",
    "downloaded_pdfs = downloader.download_batch(publications, max_downloads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4aaf7",
   "metadata": {},
   "source": [
    "### Option B: BibTeX Export for Zotero\n",
    "\n",
    "> ‚ö†Ô∏è USE OPTION C\n",
    "\n",
    "> Use one of the multiple zotero -> sci-hub plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a73c94b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing improved BibTeX creation...\n",
      "üìö Created BibTeX file with 2 entries: ../data/processed/ifc_publications.bib\n",
      "Import this file into Zotero to download PDFs automatically\n",
      "\n",
      "üîç Sample author formatting:\n",
      "1. Original: Garc√≠a-L√≥pez, M., Rodr√≠guez-Silva, A., Mendoza-P√©rez, J.\n",
      "   BibTeX:   Garc√≠a-L√≥pez, M. and Rodr√≠guez-Silva, A. and Mendoza-P√©rez, J.\n",
      "2. Original: Hern√°ndez-Campos, L., L√≥pez-Mart√≠n, R.\n",
      "   BibTeX:   Hern√°ndez-Campos, L. and L√≥pez-Mart√≠n, R.\n",
      "\n",
      "BibTeX file created at: ../data/processed/ifc_publications.bib\n",
      "\n",
      "üìÑ Sample BibTeX entries:\n",
      "@article{GarcaLpez2024_ifc_0,\n",
      " abstract = {We investigated the cellular and molecular mechanisms underlying memory formation in hippocampal circuits. Using electrophysiological recordings and optogenetic manipulations, we found that...},\n",
      " author = {Garc√≠a-L√≥pez, M. and Rodr√≠guez-Silva, A. and Mendoza-P√©rez, J.},\n",
      " doi = {10.1523/JNEUROSCI.1234-24.2024},\n",
      " journal = {Journal of Neuroscience},\n",
      " note = {Instituto de Fisiolog√≠a Celular, UNAM},\n",
      " pmid = {38123456},\n",
      " title = {Neural mechanisms of memory formation in hippocampal circuits},\n",
      " url = {https://www.ifc.unam.mx/publicacion.php?ut=000123456789},\n",
      " year = {2024}\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_bibtex_from_publications(publications, output_file='../data/processed/ifc_publications.bib'):\n",
    "    \"\"\"Convert JSON publications to BibTeX format for Zotero import\"\"\"\n",
    "    \n",
    "    db = BibDatabase()\n",
    "    entries = []\n",
    "    \n",
    "    def format_authors_for_bibtex(author_string):\n",
    "        \"\"\"Convert author string to proper BibTeX format\"\"\"\n",
    "        if not author_string:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Split by commas and clean each author\n",
    "        authors = [author.strip() for author in author_string.split(',')]\n",
    "        \n",
    "        # Group authors (assuming they come in pairs: LastName, FirstName)\n",
    "        formatted_authors = []\n",
    "        i = 0\n",
    "        while i < len(authors):\n",
    "            if i + 1 < len(authors):\n",
    "                # Check if next item looks like a first name (short, no hyphens typically)\n",
    "                next_item = authors[i + 1].strip()\n",
    "                if (len(next_item) <= 3 or \n",
    "                    (len(next_item.split()) == 1 and '.' in next_item) or\n",
    "                    re.match(r'^[A-Z]\\.?$', next_item)):\n",
    "                    # This is likely a first name/initial\n",
    "                    last_name = authors[i].strip()\n",
    "                    first_name = next_item\n",
    "                    formatted_authors.append(f\"{last_name}, {first_name}\")\n",
    "                    i += 2\n",
    "                else:\n",
    "                    # This is likely a full name or last name only\n",
    "                    formatted_authors.append(authors[i].strip())\n",
    "                    i += 1\n",
    "            else:\n",
    "                # Last author, no pair\n",
    "                formatted_authors.append(authors[i].strip())\n",
    "                i += 1\n",
    "        \n",
    "        # Join with \" and \" for BibTeX format\n",
    "        return \" and \".join(formatted_authors)\n",
    "    \n",
    "    for i, pub in enumerate(publications):\n",
    "        # Create a unique citation key\n",
    "        first_author = pub['authors'].split(',')[0].strip() if pub['authors'] else 'Unknown'\n",
    "        first_author_clean = re.sub(r'[^a-zA-Z]', '', first_author)\n",
    "        citation_key = f\"{first_author_clean}{pub['year']}_ifc_{i}\"\n",
    "        \n",
    "        # Format authors properly for BibTeX\n",
    "        formatted_authors = format_authors_for_bibtex(pub['authors'])\n",
    "        \n",
    "        entry = {\n",
    "            'ENTRYTYPE': 'article',\n",
    "            'ID': citation_key,\n",
    "            'title': pub['title'],\n",
    "            'author': formatted_authors,  # Now properly formatted\n",
    "            'journal': pub['journal'],\n",
    "            'year': str(pub['year']),\n",
    "            'abstract': pub.get('abstract', ''),\n",
    "            'url': pub.get('ifc_url', ''),\n",
    "            'note': 'Instituto de Fisiolog√≠a Celular, UNAM'\n",
    "        }\n",
    "        \n",
    "        if pub.get('doi'):\n",
    "            entry['doi'] = pub['doi']\n",
    "            \n",
    "        if pub.get('pubmed_id'):\n",
    "            entry['pmid'] = pub['pubmed_id']\n",
    "            \n",
    "        entries.append(entry)\n",
    "    \n",
    "    db.entries = entries\n",
    "    \n",
    "    # Write to file\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    writer = BibTexWriter()\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(writer.write(db))\n",
    "    \n",
    "    print(f\"üìö Created BibTeX file with {len(entries)} entries: {output_file}\")\n",
    "    print(\"Import this file into Zotero to download PDFs automatically\")\n",
    "    \n",
    "    # Show sample formatted authors for verification\n",
    "    print(\"\\nüîç Sample author formatting:\")\n",
    "    for i, entry in enumerate(entries[:3]):\n",
    "        print(f\"{i+1}. Original: {publications[i]['authors']}\")\n",
    "        print(f\"   BibTeX:   {entry['author']}\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "# Test the improved function\n",
    "print(\"üîß Testing improved BibTeX creation...\")\n",
    "bibtex_file = create_bibtex_from_publications(publications)\n",
    "print(f\"\\nBibTeX file created at: {bibtex_file}\")\n",
    "\n",
    "# Let's also check the actual BibTeX content\n",
    "print(\"\\nüìÑ Sample BibTeX entries:\")\n",
    "with open(bibtex_file, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    # Show first entry\n",
    "    first_entry_end = content.find('\\n}\\n') + 3\n",
    "    print(content[:first_entry_end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad48f81",
   "metadata": {},
   "source": [
    "### Option C: PyPaperBot\n",
    "\n",
    "- [Repo](https://github.com/ferru97/PyPaperBot)\n",
    "\n",
    "- Download papers given a query\n",
    "- Download papers given paper's DOIs\n",
    "- Generate Bibtex of the downloaded paper\n",
    "- Filter downloaded paper by year, journal and citations number\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "- Multiple Download Methods: DOI-based downloads (most reliable)\n",
    "- Google Scholar queries\n",
    "- BibTeX-only generation\n",
    "- Flexible Modes: Download PDFs only, BibTeX only, or both\n",
    "- IFC-Specific Queries: Pre-configured searches for the institute\n",
    "- Deduplication: Automatic removal of duplicate downloads\n",
    "- Rate Limiting: Respectful delays between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1800263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import PyPaperBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e43d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing PyPaperBot integration...\n",
      "\n",
      "üìö Test 1: Generate BibTeX only\n",
      "‚ùå PyPaperBot not found. Install with: pip install PyPaperBot\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'publications' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 239\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìö Test 1: Generate BibTeX only\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    237\u001b[0m downloader \u001b[38;5;241m=\u001b[39m PyPaperBotDownloader()\n\u001b[1;32m    238\u001b[0m bibtex_result \u001b[38;5;241m=\u001b[39m downloader\u001b[38;5;241m.\u001b[39mgenerate_bibtex_only(\n\u001b[0;32m--> 239\u001b[0m     \u001b[43mpublications\u001b[49m, \n\u001b[1;32m    240\u001b[0m     output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/processed/pypaper_ifc_publications.bib\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ PyPaperBot integration fixed and ready!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'publications' is not defined"
     ]
    }
   ],
   "source": [
    "class PyPaperBotDownloader:\n",
    "    def __init__(self, download_dir='../papers/pypaper_downloads'):\n",
    "        \"\"\"Initialize PyPaperBot downloader with configuration\"\"\"\n",
    "        self.download_dir = Path(download_dir)\n",
    "        self.download_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.python_cmd = 'python'  # Initialize with default\n",
    "        \n",
    "        # Check if PyPaperBot is installed\n",
    "        self._check_installation()\n",
    "        \n",
    "    def _check_installation(self):\n",
    "        \"\"\"Check if PyPaperBot is installed\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([self.python_cmd, '-m', 'PyPaperBot', '-h'], \n",
    "                                  capture_output=True, text=True, timeout=10)\n",
    "            if result.returncode == 0:\n",
    "                print(\"‚úÖ PyPaperBot installation verified\")\n",
    "                return True\n",
    "        except (subprocess.TimeoutExpired, FileNotFoundError):\n",
    "            pass\n",
    "            \n",
    "        # Try with 'py' command\n",
    "        try:\n",
    "            result = subprocess.run(['py', '-m', 'PyPaperBot', '-h'], \n",
    "                                  capture_output=True, text=True, timeout=10)\n",
    "            if result.returncode == 0:\n",
    "                self.python_cmd = 'py'\n",
    "                print(\"‚úÖ PyPaperBot installation verified (using 'py' command)\")\n",
    "                return True\n",
    "        except (subprocess.TimeoutExpired, FileNotFoundError):\n",
    "            pass\n",
    "            \n",
    "        print(\"‚ùå PyPaperBot not found. Install with: pip install PyPaperBot\")\n",
    "        return False\n",
    "    \n",
    "    def download_by_dois(self, publications, restrict_mode=None, max_downloads=None, \n",
    "                        scihub_mirror=None, use_doi_filename=True):\n",
    "        \"\"\"Download papers using their DOIs\"\"\"\n",
    "        \n",
    "        # Filter publications with DOIs\n",
    "        pubs_with_dois = [pub for pub in publications if pub.get('doi')]\n",
    "        \n",
    "        if not pubs_with_dois:\n",
    "            print(\"‚ùå No publications with DOIs found\")\n",
    "            return {'pdf_files': [], 'bibtex_files': [], 'publications': []}\n",
    "        \n",
    "        if max_downloads:\n",
    "            pubs_with_dois = pubs_with_dois[:max_downloads]\n",
    "        \n",
    "        print(f\"üì• Attempting to download {len(pubs_with_dois)} papers using DOIs...\")\n",
    "        \n",
    "        # Create temporary DOI file\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as doi_file:\n",
    "            for pub in pubs_with_dois:\n",
    "                doi_file.write(f\"{pub['doi']}\\n\")\n",
    "            doi_file_path = doi_file.name\n",
    "        \n",
    "        try:\n",
    "            # Build PyPaperBot command\n",
    "            cmd = [\n",
    "                self.python_cmd, '-m', 'PyPaperBot',\n",
    "                '--doi-file', doi_file_path,\n",
    "                '--dwn-dir', str(self.download_dir)\n",
    "            ]\n",
    "            \n",
    "            # Add optional parameters\n",
    "            if restrict_mode is not None:\n",
    "                cmd.extend(['--restrict', str(restrict_mode)])\n",
    "            \n",
    "            if scihub_mirror:\n",
    "                cmd.extend(['--scihub-mirror', scihub_mirror])\n",
    "                \n",
    "            if use_doi_filename:\n",
    "                cmd.append('--use-doi-as-filename')\n",
    "            \n",
    "            print(f\"üîß Running command: {' '.join(cmd)}\")\n",
    "            \n",
    "            # Execute PyPaperBot\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)\n",
    "            \n",
    "            print(\"üìã PyPaperBot output:\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            if result.stderr:\n",
    "                print(\"‚ö†Ô∏è PyPaperBot warnings/errors:\")\n",
    "                print(result.stderr)\n",
    "            \n",
    "            # Check downloaded files\n",
    "            downloaded_files = list(self.download_dir.glob('*.pdf'))\n",
    "            downloaded_bibtex = list(self.download_dir.glob('*.bib'))\n",
    "            \n",
    "            print(f\"‚úÖ Downloaded {len(downloaded_files)} PDF files\")\n",
    "            print(f\"‚úÖ Generated {len(downloaded_bibtex)} BibTeX files\")\n",
    "            \n",
    "            return {\n",
    "                'pdf_files': downloaded_files,\n",
    "                'bibtex_files': downloaded_bibtex,\n",
    "                'publications': pubs_with_dois\n",
    "            }\n",
    "            \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"‚è∞ Download timed out after 10 minutes\")\n",
    "            return {'pdf_files': [], 'bibtex_files': [], 'publications': []}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during download: {e}\")\n",
    "            return {'pdf_files': [], 'bibtex_files': [], 'publications': []}\n",
    "        finally:\n",
    "            # Clean up temporary file\n",
    "            Path(doi_file_path).unlink(missing_ok=True)\n",
    "    \n",
    "    def download_by_query(self, query, scholar_pages=3, min_year=None, \n",
    "                         max_downloads=None, restrict_mode=None, \n",
    "                         skip_words=None, scihub_mirror=None):\n",
    "        \"\"\"Download papers using Google Scholar query\"\"\"\n",
    "        \n",
    "        cmd = [\n",
    "            self.python_cmd, '-m', 'PyPaperBot',\n",
    "            '--query', query,\n",
    "            '--scholar-pages', str(scholar_pages),\n",
    "            '--dwn-dir', str(self.download_dir)\n",
    "        ]\n",
    "        \n",
    "        # Add optional parameters\n",
    "        if min_year:\n",
    "            cmd.extend(['--min-year', str(min_year)])\n",
    "            \n",
    "        if max_downloads:\n",
    "            if isinstance(max_downloads, str) and 'year' in max_downloads.lower():\n",
    "                cmd.extend(['--max-dwn-year', str(max_downloads)])\n",
    "            else:\n",
    "                cmd.extend(['--max-dwn-cites', str(max_downloads)])\n",
    "        \n",
    "        if restrict_mode is not None:\n",
    "            cmd.extend(['--restrict', str(restrict_mode)])\n",
    "            \n",
    "        if skip_words:\n",
    "            cmd.extend(['--skip-words', skip_words])\n",
    "            \n",
    "        if scihub_mirror:\n",
    "            cmd.extend(['--scihub-mirror', scihub_mirror])\n",
    "        \n",
    "        print(f\"üîç Searching and downloading papers for query: '{query}'\")\n",
    "        print(f\"üîß Running command: {' '.join(cmd)}\")\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=900)\n",
    "            \n",
    "            print(\"üìã PyPaperBot output:\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            if result.stderr:\n",
    "                print(\"‚ö†Ô∏è PyPaperBot warnings/errors:\")\n",
    "                print(result.stderr)\n",
    "            \n",
    "            # Check results\n",
    "            downloaded_files = list(self.download_dir.glob('*.pdf'))\n",
    "            downloaded_bibtex = list(self.download_dir.glob('*.bib'))\n",
    "            \n",
    "            print(f\"‚úÖ Downloaded {len(downloaded_files)} PDF files\")\n",
    "            print(f\"‚úÖ Generated {len(downloaded_bibtex)} BibTeX files\")\n",
    "            \n",
    "            return {\n",
    "                'pdf_files': downloaded_files,\n",
    "                'bibtex_files': downloaded_bibtex,\n",
    "                'query': query\n",
    "            }\n",
    "            \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"‚è∞ Download timed out after 15 minutes\")\n",
    "            return {'pdf_files': [], 'bibtex_files': [], 'query': query}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during download: {e}\")\n",
    "            return {'pdf_files': [], 'bibtex_files': [], 'query': query}\n",
    "    \n",
    "    def generate_bibtex_only(self, publications, output_file=None):\n",
    "        \"\"\"Generate BibTeX file without downloading PDFs\"\"\"\n",
    "        \n",
    "        pubs_with_dois = [pub for pub in publications if pub.get('doi')]\n",
    "        \n",
    "        if not pubs_with_dois:\n",
    "            print(\"‚ùå No publications with DOIs found for BibTeX generation\")\n",
    "            return None\n",
    "        \n",
    "        # Create temporary DOI file\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as doi_file:\n",
    "            for pub in pubs_with_dois:\n",
    "                doi_file.write(f\"{pub['doi']}\\n\")\n",
    "            doi_file_path = doi_file.name\n",
    "        \n",
    "        try:\n",
    "            cmd = [\n",
    "                self.python_cmd, '-m', 'PyPaperBot',\n",
    "                '--doi-file', doi_file_path,\n",
    "                '--dwn-dir', str(self.download_dir),\n",
    "                '--restrict', '0'  # Download only BibTeX\n",
    "            ]\n",
    "            \n",
    "            print(f\"üìö Generating BibTeX for {len(pubs_with_dois)} publications...\")\n",
    "            \n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            print(\"üìã PyPaperBot output:\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            if result.stderr:\n",
    "                print(\"‚ö†Ô∏è PyPaperBot warnings/errors:\")\n",
    "                print(result.stderr)\n",
    "            \n",
    "            # Find generated BibTeX file\n",
    "            bibtex_files = list(self.download_dir.glob('*.bib'))\n",
    "            \n",
    "            if bibtex_files and output_file:\n",
    "                # Move to specified location\n",
    "                output_path = Path(output_file)\n",
    "                output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.move(str(bibtex_files[0]), str(output_path))\n",
    "                print(f\"üìö BibTeX file saved to: {output_path}\")\n",
    "                return output_path\n",
    "            elif bibtex_files:\n",
    "                print(f\"üìö BibTeX file generated: {bibtex_files[0]}\")\n",
    "                return bibtex_files[0]\n",
    "            else:\n",
    "                print(\"‚ùå No BibTeX file was generated\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating BibTeX: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            Path(doi_file_path).unlink(missing_ok=True)\n",
    "\n",
    "# Test the fixed PyPaperBot integration\n",
    "print(\"üß™ Testing PyPaperBot integration...\")\n",
    "\n",
    "# Test 1: Generate BibTeX only for existing publications\n",
    "print(\"\\nüìö Test 1: Generate BibTeX only\")\n",
    "downloader = PyPaperBotDownloader()\n",
    "bibtex_result = downloader.generate_bibtex_only(\n",
    "    publications, \n",
    "    output_file='../data/processed/pypaper_ifc_publications.bib'\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ PyPaperBot integration fixed and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f4b4c",
   "metadata": {},
   "source": [
    "> affiliation mining system:\n",
    "\n",
    "- Extracts text from PDFs using PyMuPDF\n",
    "- Uses both regex and NLP for affiliation detection\n",
    "- Supports Spanish and English processing\n",
    "- Groups similar affiliations automatically\n",
    "- Generates PubMed search variations from discovered affiliations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816bf78",
   "metadata": {},
   "source": [
    "> NOTE‚ö†Ô∏è\n",
    "\n",
    "spaCy:\n",
    "\n",
    "- Tokenizes the text into words, punctuation, etc.\n",
    "- Part-of-speech tags each token\n",
    "- Dependency parses to understand grammatical relationships\n",
    "- Named Entity Recognition identifies spans as organizations, people, locations, etc.\n",
    "Classification assigns labels like \"ORG\" (organization), \"PERSON\", \"GPE\" (geopolitical entity)\n",
    "\n",
    "```python entity_recognition_process\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"ORG\":  # Organization entity\n",
    "        print(ent.text)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c45fcd",
   "metadata": {},
   "source": [
    "#### spacy installation\n",
    "\n",
    "```python\n",
    "# Install Python packages\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Download spaCy language models\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download es_core_news_sm\n",
    "\n",
    "# Optional: Download larger, more accurate models\n",
    "python -m spacy download en_core_web_md\n",
    "python -m spacy download es_core_news_md\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6adb1873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_133257/572880994.py\", line 1, in <module>\n",
      "    import spacy\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c9ceb2",
   "metadata": {},
   "source": [
    "#### Enhanced Affiliation Mining\n",
    "\n",
    "Test full scpaCy capabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa3adad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Enhanced Affiliation Mining Demo...\n",
      "‚úÖ Loaded en_core_web_sm\n",
      "‚úÖ Loaded es_core_news_sm\n",
      "üß™ Testing enhanced affiliation extraction...\n",
      "\n",
      "üß† Enhanced NLP extraction found 6 affiliations:\n",
      "   ‚Ä¢ Fisiolog√≠a Celular\n",
      "   ‚Ä¢ IFC-UNAM\n",
      "   ‚Ä¢ Instituto de Fisiolog√≠a Celular\n",
      "   ‚Ä¢ Instituto de Fisiolog√≠a Celular,\n",
      "   ‚Ä¢ National Autonomous University of Mexico\n",
      "   ‚Ä¢ UNAM\n",
      "    Centro de Investigaci√≥n\n",
      "\n",
      "üîó Found 4 similarity clusters:\n",
      "   Cluster 1: 3 variations\n",
      "      - Instituto de Fisiolog√≠a Celular\n",
      "      - Instituto de Fisiolog√≠a Celular,\n",
      "      - Fisiolog√≠a Celular\n",
      "   Cluster 2: 1 variations\n",
      "      - IFC-UNAM\n",
      "   Cluster 3: 1 variations\n",
      "      - National Autonomous University of Mexico\n",
      "   Cluster 4: 1 variations\n",
      "      - UNAM\n",
      "    Centro de Investigaci√≥n\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "class EnhancedAffiliationMiner:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with advanced spaCy features\"\"\"\n",
    "        self.nlp_models = {}\n",
    "        self.matchers = {}\n",
    "        self.load_nlp_models()\n",
    "        self.setup_custom_matchers()\n",
    "        \n",
    "    def load_nlp_models(self):\n",
    "        \"\"\"Load spaCy models with error handling\"\"\"\n",
    "        models_to_load = {\n",
    "            'en': 'en_core_web_sm',\n",
    "            'es': 'es_core_news_sm'\n",
    "        }\n",
    "        \n",
    "        for lang, model_name in models_to_load.items():\n",
    "            try:\n",
    "                nlp = spacy.load(model_name)\n",
    "                # Add custom pipeline components\n",
    "                if not nlp.has_pipe('merge_entities'):\n",
    "                    nlp.add_pipe('merge_entities')\n",
    "                \n",
    "                self.nlp_models[lang] = nlp\n",
    "                print(f\"‚úÖ Loaded {model_name}\")\n",
    "                \n",
    "                # Setup matcher for this language\n",
    "                self.matchers[lang] = Matcher(nlp.vocab)\n",
    "                \n",
    "            except OSError:\n",
    "                print(f\"‚ùå {model_name} not found. Install with:\")\n",
    "                print(f\"   python -m spacy download {model_name}\")\n",
    "    \n",
    "    def setup_custom_matchers(self):\n",
    "        \"\"\"Setup custom pattern matchers for institutional names\"\"\"\n",
    "        \n",
    "        # Patterns for Spanish institutions\n",
    "        if 'es' in self.matchers:\n",
    "            spanish_patterns = [\n",
    "                # Instituto de X patterns\n",
    "                [{\"LOWER\": \"instituto\"}, {\"LOWER\": \"de\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # Universidad patterns\n",
    "                [{\"LOWER\": \"universidad\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                [{\"LOWER\": \"universidad\"}, {\"LOWER\": \"nacional\"}, {\"LOWER\": \"aut√≥noma\"}, {\"LOWER\": \"de\"}, {\"LOWER\": \"m√©xico\"}],\n",
    "                \n",
    "                # Departamento patterns\n",
    "                [{\"LOWER\": \"departamento\"}, {\"LOWER\": \"de\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # IFC patterns\n",
    "                [{\"TEXT\": {\"REGEX\": r\"IFC-?UNAM\"}}],\n",
    "            ]\n",
    "            \n",
    "            for i, pattern in enumerate(spanish_patterns):\n",
    "                self.matchers['es'].add(f\"SPANISH_INSTITUTION_{i}\", [pattern])\n",
    "        \n",
    "        # Patterns for English institutions\n",
    "        if 'en' in self.matchers:\n",
    "            english_patterns = [\n",
    "                # University of X patterns\n",
    "                [{\"LOWER\": \"university\"}, {\"LOWER\": \"of\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # Institute of X patterns\n",
    "                [{\"LOWER\": \"institute\"}, {\"LOWER\": \"of\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # Department of X patterns\n",
    "                [{\"LOWER\": \"department\"}, {\"LOWER\": \"of\"}, {\"IS_TITLE\": True, \"OP\": \"+\"}],\n",
    "                \n",
    "                # National Autonomous University of Mexico\n",
    "                [{\"LOWER\": \"national\"}, {\"LOWER\": \"autonomous\"}, {\"LOWER\": \"university\"}, \n",
    "                 {\"LOWER\": \"of\"}, {\"LOWER\": \"mexico\"}],\n",
    "            ]\n",
    "            \n",
    "            for i, pattern in enumerate(english_patterns):\n",
    "                self.matchers['en'].add(f\"ENGLISH_INSTITUTION_{i}\", [pattern])\n",
    "    \n",
    "    def detect_language_advanced(self, text):\n",
    "        \"\"\"Advanced language detection\"\"\"\n",
    "        try:\n",
    "            # Use langdetect for primary detection\n",
    "            detected = detect(text[:1000])  # Use first 1000 chars for speed\n",
    "            \n",
    "            # Validate with keyword analysis\n",
    "            spanish_keywords = ['de', 'del', 'la', 'el', 'y', 'universidad', 'instituto']\n",
    "            english_keywords = ['of', 'the', 'and', 'university', 'institute', 'department']\n",
    "            \n",
    "            text_lower = text.lower()\n",
    "            spanish_count = sum(1 for kw in spanish_keywords if kw in text_lower)\n",
    "            english_count = sum(1 for kw in english_keywords if kw in text_lower)\n",
    "            \n",
    "            # Override detection if keyword analysis is strong\n",
    "            if spanish_count > english_count * 1.5:\n",
    "                return 'es'\n",
    "            elif english_count > spanish_count * 1.5:\n",
    "                return 'en'\n",
    "            else:\n",
    "                return detected if detected in ['es', 'en'] else 'en'\n",
    "                \n",
    "        except:\n",
    "            return 'en'  # Default to English\n",
    "    \n",
    "    def extract_affiliations_advanced_nlp(self, text):\n",
    "        \"\"\"Advanced NER + custom patterns for affiliation extraction\"\"\"\n",
    "        language = self.detect_language_advanced(text)\n",
    "        \n",
    "        if language not in self.nlp_models:\n",
    "            print(f\"‚ö†Ô∏è No model available for language: {language}\")\n",
    "            return set()\n",
    "        \n",
    "        nlp = self.nlp_models[language]\n",
    "        matcher = self.matchers[language]\n",
    "        \n",
    "        affiliations = set()\n",
    "        \n",
    "        # Process text in chunks to handle large documents\n",
    "        max_length = 1000000\n",
    "        text_chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "        \n",
    "        for chunk in text_chunks:\n",
    "            try:\n",
    "                doc = nlp(chunk)\n",
    "                \n",
    "                # Method 1: Standard NER for organizations\n",
    "                for ent in doc.ents:\n",
    "                    if ent.label_ == \"ORG\":\n",
    "                        org_text = ent.text.strip()\n",
    "                        if self.is_relevant_affiliation(org_text):\n",
    "                            affiliations.add(org_text)\n",
    "                \n",
    "                # Method 2: Custom pattern matching\n",
    "                matches = matcher(doc)\n",
    "                for match_id, start, end in matches:\n",
    "                    span = doc[start:end]\n",
    "                    affiliation_text = span.text.strip()\n",
    "                    if len(affiliation_text) > 5:\n",
    "                        affiliations.add(affiliation_text)\n",
    "                \n",
    "                # Method 3: Context-based extraction\n",
    "                # Look for sentences containing institutional indicators\n",
    "                for sent in doc.sents:\n",
    "                    sent_text = sent.text.strip()\n",
    "                    if self.contains_institutional_indicators(sent_text, language):\n",
    "                        # Extract the institutional part\n",
    "                        extracted = self.extract_institutional_part(sent_text, language)\n",
    "                        if extracted:\n",
    "                            affiliations.add(extracted)\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing chunk: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return affiliations\n",
    "    \n",
    "    def is_relevant_affiliation(self, org_text):\n",
    "        \"\"\"Check if organization text is relevant to our search\"\"\"\n",
    "        relevant_keywords = [\n",
    "            'instituto', 'institute', 'universidad', 'university',\n",
    "            'departamento', 'department', 'unam', 'ifc', 'mexico',\n",
    "            'fisiolog', 'physiolog', 'celular', 'cellular', 'neurobiolog'\n",
    "        ]\n",
    "        \n",
    "        org_lower = org_text.lower()\n",
    "        return (len(org_text) > 10 and \n",
    "                any(keyword in org_lower for keyword in relevant_keywords))\n",
    "    \n",
    "    def contains_institutional_indicators(self, text, language):\n",
    "        \"\"\"Check if text contains institutional indicators\"\"\"\n",
    "        if language == 'es':\n",
    "            indicators = [\n",
    "                'instituto de', 'universidad', 'departamento de', \n",
    "                'centro de', 'facultad de', 'unam'\n",
    "            ]\n",
    "        else:\n",
    "            indicators = [\n",
    "                'institute of', 'university of', 'department of',\n",
    "                'center of', 'faculty of', 'unam'\n",
    "            ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        return any(indicator in text_lower for indicator in indicators)\n",
    "    \n",
    "    def extract_institutional_part(self, sentence, language):\n",
    "        \"\"\"Extract the institutional part from a sentence\"\"\"\n",
    "        # Use regex patterns to extract institutional names\n",
    "        if language == 'es':\n",
    "            patterns = [\n",
    "                r'Instituto\\s+de\\s+[A-Za-z√Å√°√â√©√ç√≠√ì√≥√ö√∫√ë√±\\s,]+?(?:,|\\.|\\s+UNAM)',\n",
    "                r'Universidad\\s+[A-Za-z√Å√°√â√©√ç√≠√ì√≥√ö√∫√ë√±\\s,]+?(?:,|\\.)',\n",
    "                r'Departamento\\s+de\\s+[A-Za-z√Å√°√â√©√ç√≠√ì√≥√ö√∫√ë√±\\s,]+?(?:,|\\.)'\n",
    "            ]\n",
    "        else:\n",
    "            patterns = [\n",
    "                r'Institute\\s+of\\s+[A-Za-z\\s,]+?(?:,|\\.|\\s+UNAM)',\n",
    "                r'University\\s+of\\s+[A-Za-z\\s,]+?(?:,|\\.)',\n",
    "                r'Department\\s+of\\s+[A-Za-z\\s,]+?(?:,|\\.)'\n",
    "            ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, sentence, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group().strip()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def analyze_affiliations_with_clustering(self, affiliations_list):\n",
    "        \"\"\"Advanced analysis with similarity clustering\"\"\"\n",
    "        from difflib import SequenceMatcher\n",
    "        \n",
    "        def similarity(a, b):\n",
    "            return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "        \n",
    "        # Group similar affiliations\n",
    "        clusters = []\n",
    "        processed = set()\n",
    "        \n",
    "        for affiliation in affiliations_list:\n",
    "            if affiliation in processed:\n",
    "                continue\n",
    "                \n",
    "            # Find similar affiliations\n",
    "            cluster = [affiliation]\n",
    "            processed.add(affiliation)\n",
    "            \n",
    "            for other in affiliations_list:\n",
    "                if other not in processed and similarity(affiliation, other) > 0.7:\n",
    "                    cluster.append(other)\n",
    "                    processed.add(other)\n",
    "            \n",
    "            if len(cluster) >= 1:\n",
    "                clusters.append(cluster)\n",
    "        \n",
    "        return clusters\n",
    "\n",
    "# Usage example and demo\n",
    "def demo_enhanced_mining():\n",
    "    \"\"\"Demonstrate enhanced affiliation mining\"\"\"\n",
    "    miner = EnhancedAffiliationMiner()\n",
    "    \n",
    "    sample_text = \"\"\"\n",
    "    Instituto de Fisiolog√≠a Celular, Universidad Nacional Aut√≥noma de M√©xico, \n",
    "    Ciudad Universitaria, M√©xico, D.F. 04510, M√©xico\n",
    "    \n",
    "    Department of Cellular Physiology, National Autonomous University of Mexico,\n",
    "    Mexico City, Mexico\n",
    "    \n",
    "    Departamento de Neurobiolog√≠a, Instituto de Fisiolog√≠a Celular, UNAM\n",
    "    Centro de Investigaci√≥n y de Estudios Avanzados del IPN\n",
    "    \n",
    "    IFC-UNAM, Circuito Exterior s/n, Ciudad Universitaria\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üß™ Testing enhanced affiliation extraction...\")\n",
    "    \n",
    "    # Advanced NLP extraction\n",
    "    affiliations = miner.extract_affiliations_advanced_nlp(sample_text)\n",
    "    \n",
    "    print(f\"\\nüß† Enhanced NLP extraction found {len(affiliations)} affiliations:\")\n",
    "    for affiliation in sorted(affiliations):\n",
    "        print(f\"   ‚Ä¢ {affiliation}\")\n",
    "    \n",
    "    # Clustering analysis\n",
    "    clusters = miner.analyze_affiliations_with_clustering(list(affiliations))\n",
    "    print(f\"\\nüîó Found {len(clusters)} similarity clusters:\")\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        print(f\"   Cluster {i+1}: {len(cluster)} variations\")\n",
    "        for variation in cluster:\n",
    "            print(f\"      - {variation}\")\n",
    "    \n",
    "    return affiliations\n",
    "\n",
    "# Run enhanced demo\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting Enhanced Affiliation Mining Demo...\")\n",
    "    demo_results = demo_enhanced_mining()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79902897",
   "metadata": {},
   "source": [
    "## 3. Affiliation Mining from Existing PDFs\n",
    "\n",
    "This is the key step - we'll analyze existing papers to find all variations of how your institute is mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4308eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def extract_affiliations_with_nlp(self, text):\n",
    "    # Use a library like spaCy to identify organization entities\n",
    "    nlp = spacy.load(\"es_core_news_md\")  # Spanish model\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9257b9",
   "metadata": {},
   "source": [
    "## 4. PubMed Search Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b7f64e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting comprehensive PubMed search...\n",
      "Error searching PubMed: name 'requests' is not defined\n",
      "\n",
      "üìä Test search found 0 articles\n",
      "\n",
      "üîç Running search 1/8\n",
      "Error searching PubMed: name 'requests' is not defined\n",
      "Added 0 new articles (total: 0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 182\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPMID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpmid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Uncomment to run full search\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m new_articles \u001b[38;5;241m=\u001b[39m \u001b[43msearcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomprehensive_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_per_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müéâ Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_articles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m total unique articles from PubMed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 162\u001b[0m, in \u001b[0;36mPubmedSearcher.comprehensive_search\u001b[0;34m(self, max_per_query)\u001b[0m\n\u001b[1;32m    159\u001b[0m     all_articles\u001b[38;5;241m.\u001b[39mextend(new_articles)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_articles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new articles (total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_articles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Be respectful to NCBI\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_articles\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "class PubmedSearcher:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "        \n",
    "    def build_search_queries(self, affiliation_variations=None):\n",
    "        \"\"\"Build comprehensive search queries for different affiliation variations\"\"\"\n",
    "        \n",
    "        if affiliation_variations is None:\n",
    "            # Default variations based on your institute\n",
    "            affiliation_variations = [\n",
    "                \"Instituto de Fisiologia Celular[Affiliation]\",\n",
    "                \"Institute of Cellular Physiology[Affiliation]\",\n",
    "                \"IFC UNAM[Affiliation]\",\n",
    "                \"Departamento de Neurobiologia UNAM[Affiliation]\",\n",
    "                \"Universidad Nacional Autonoma Mexico Fisiologia[Affiliation]\",\n",
    "                \"National Autonomous University Mexico Cellular Physiology[Affiliation]\"\n",
    "            ]\n",
    "        \n",
    "        queries = []\n",
    "        \n",
    "        # Individual affiliation searches\n",
    "        for aff in affiliation_variations:\n",
    "            queries.append(aff)\n",
    "            \n",
    "        # Combined searches with time ranges\n",
    "        recent_query = f\"({' OR '.join(affiliation_variations[:3])}) AND (2020:2024[pdat])\"\n",
    "        historical_query = f\"({' OR '.join(affiliation_variations[:3])}) AND (2010:2019[pdat])\"\n",
    "        \n",
    "        queries.extend([recent_query, historical_query])\n",
    "        \n",
    "        return queries\n",
    "    \n",
    "    def search_pubmed(self, query, max_results=100):\n",
    "        \"\"\"Search PubMed with a given query\"\"\"\n",
    "        \n",
    "        # Step 1: Search\n",
    "        search_url = f\"{self.base_url}esearch.fcgi\"\n",
    "        search_params = {\n",
    "            'db': 'pubmed',\n",
    "            'term': query,\n",
    "            'retmax': max_results,\n",
    "            'retmode': 'json'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(search_url, params=search_params)\n",
    "            search_data = response.json()\n",
    "            \n",
    "            pmids = search_data['esearchresult']['idlist']\n",
    "            total_count = int(search_data['esearchresult']['count'])\n",
    "            \n",
    "            print(f\"Found {total_count} results for query: {query[:50]}...\")\n",
    "            \n",
    "            if not pmids:\n",
    "                return []\n",
    "            \n",
    "            # Step 2: Fetch details\n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "            \n",
    "            fetch_url = f\"{self.base_url}efetch.fcgi\"\n",
    "            fetch_params = {\n",
    "                'db': 'pubmed',\n",
    "                'id': ','.join(pmids),\n",
    "                'retmode': 'xml'\n",
    "            }\n",
    "            \n",
    "            fetch_response = requests.get(fetch_url, params=fetch_params)\n",
    "            \n",
    "            # Parse XML (simplified - you might want to use xml.etree.ElementTree)\n",
    "            articles = self.parse_pubmed_xml(fetch_response.text)\n",
    "            \n",
    "            return articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching PubMed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def parse_pubmed_xml(self, xml_content):\n",
    "        \"\"\"Simple XML parsing for PubMed results (you might want to improve this)\"\"\"\n",
    "        import xml.etree.ElementTree as ET\n",
    "        \n",
    "        articles = []\n",
    "        \n",
    "        try:\n",
    "            root = ET.fromstring(xml_content)\n",
    "            \n",
    "            for article in root.findall('.//PubmedArticle'):\n",
    "                try:\n",
    "                    # Extract basic info\n",
    "                    pmid = article.find('.//PMID').text\n",
    "                    \n",
    "                    title_elem = article.find('.//ArticleTitle')\n",
    "                    title = title_elem.text if title_elem is not None else \"No title\"\n",
    "                    \n",
    "                    # Authors\n",
    "                    authors = []\n",
    "                    for author in article.findall('.//Author'):\n",
    "                        lastname = author.find('.//LastName')\n",
    "                        firstname = author.find('.//ForeName')\n",
    "                        if lastname is not None:\n",
    "                            author_name = lastname.text\n",
    "                            if firstname is not None:\n",
    "                                author_name += f\", {firstname.text}\"\n",
    "                            authors.append(author_name)\n",
    "                    \n",
    "                    # Journal and year\n",
    "                    journal_elem = article.find('.//Journal/Title')\n",
    "                    journal = journal_elem.text if journal_elem is not None else \"Unknown\"\n",
    "                    \n",
    "                    year_elem = article.find('.//PubDate/Year')\n",
    "                    year = int(year_elem.text) if year_elem is not None else None\n",
    "                    \n",
    "                    # Abstract\n",
    "                    abstract_elem = article.find('.//Abstract/AbstractText')\n",
    "                    abstract = abstract_elem.text if abstract_elem is not None else \"\"\n",
    "                    \n",
    "                    # DOI\n",
    "                    doi_elem = article.find('.//ELocationID[@EIdType=\"doi\"]')\n",
    "                    doi = doi_elem.text if doi_elem is not None else None\n",
    "                    \n",
    "                    article_data = {\n",
    "                        'pmid': pmid,\n",
    "                        'title': title,\n",
    "                        'authors': '; '.join(authors),\n",
    "                        'journal': journal,\n",
    "                        'year': year,\n",
    "                        'abstract': abstract,\n",
    "                        'doi': doi\n",
    "                    }\n",
    "                    \n",
    "                    articles.append(article_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing article: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing XML: {e}\")\n",
    "            \n",
    "        return articles\n",
    "    \n",
    "    def comprehensive_search(self, max_per_query=50):\n",
    "        \"\"\"Run comprehensive search with all query variations\"\"\"\n",
    "        queries = self.build_search_queries()\n",
    "        all_articles = []\n",
    "        seen_pmids = set()\n",
    "        \n",
    "        for i, query in enumerate(queries):\n",
    "            print(f\"\\nüîç Running search {i+1}/{len(queries)}\")\n",
    "            articles = self.search_pubmed(query, max_per_query)\n",
    "            \n",
    "            # Deduplicate\n",
    "            new_articles = []\n",
    "            for article in articles:\n",
    "                if article['pmid'] not in seen_pmids:\n",
    "                    seen_pmids.add(article['pmid'])\n",
    "                    new_articles.append(article)\n",
    "            \n",
    "            all_articles.extend(new_articles)\n",
    "            print(f\"Added {len(new_articles)} new articles (total: {len(all_articles)})\")\n",
    "            \n",
    "            time.sleep(1)  # Be respectful to NCBI\n",
    "            \n",
    "        return all_articles\n",
    "\n",
    "# Run comprehensive PubMed search\n",
    "print(\"üîç Starting comprehensive PubMed search...\")\n",
    "searcher = PubmedSearcher()\n",
    "\n",
    "# Test with a single query first\n",
    "test_articles = searcher.search_pubmed(\"Instituto de Fisiologia Celular[Affiliation]\", max_results=5)\n",
    "print(f\"\\nüìä Test search found {len(test_articles)} articles\")\n",
    "\n",
    "if test_articles:\n",
    "    print(\"\\nSample result:\")\n",
    "    sample = test_articles[0]\n",
    "    print(f\"Title: {sample['title'][:100]}...\")\n",
    "    print(f\"Authors: {sample['authors'][:100]}...\")\n",
    "    print(f\"PMID: {sample['pmid']}\")\n",
    "\n",
    "# Uncomment to run full search\n",
    "new_articles = searcher.comprehensive_search(max_per_query=20)\n",
    "print(f\"\\nüéâ Found {len(new_articles)} total unique articles from PubMed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f53bd",
   "metadata": {},
   "source": [
    "## 5. Database Integration & Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a62ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_publication_databases(existing_pubs, new_pubs, output_file='../data/processed/expanded_ifc_publications.json'):\n",
    "    \"\"\"Merge existing publications with newly found ones, removing duplicates\"\"\"\n",
    "    \n",
    "    # Create lookup sets for deduplication\n",
    "    existing_dois = {pub.get('doi') for pub in existing_pubs if pub.get('doi')}\n",
    "    existing_pmids = {pub.get('pubmed_id') for pub in existing_pubs if pub.get('pubmed_id')}\n",
    "    existing_titles = {pub.get('title', '').lower().strip() for pub in existing_pubs}\n",
    "    \n",
    "    merged_pubs = existing_pubs.copy()\n",
    "    new_count = 0\n",
    "    \n",
    "    for pub in new_pubs:\n",
    "        is_duplicate = False\n",
    "        \n",
    "        # Check for duplicates\n",
    "        if pub.get('doi') and pub['doi'] in existing_dois:\n",
    "            is_duplicate = True\n",
    "        elif pub.get('pmid') and pub['pmid'] in existing_pmids:\n",
    "            is_duplicate = True\n",
    "        elif pub.get('title', '').lower().strip() in existing_titles:\n",
    "            is_duplicate = True\n",
    "            \n",
    "        if not is_duplicate:\n",
    "            # Convert PubMed format to your format\n",
    "            converted_pub = {\n",
    "                'title': pub.get('title', ''),\n",
    "                'authors': pub.get('authors', ''),\n",
    "                'journal': pub.get('journal', ''),\n",
    "                'year': pub.get('year'),\n",
    "                'doi': pub.get('doi'),\n",
    "                'pubmed_id': pub.get('pmid'),\n",
    "                'ifc_url': None,  # Not available from PubMed\n",
    "                'abstract': pub.get('abstract', ''),\n",
    "                'keywords': None\n",
    "            }\n",
    "            \n",
    "            merged_pubs.append(converted_pub)\n",
    "            new_count += 1\n",
    "            \n",
    "            # Update tracking sets\n",
    "            if pub.get('doi'):\n",
    "                existing_dois.add(pub['doi'])\n",
    "            if pub.get('pmid'):\n",
    "                existing_pmids.add(pub['pmid'])\n",
    "            existing_titles.add(pub.get('title', '').lower().strip())\n",
    "    \n",
    "    # Save expanded database\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_pubs, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nüìä Database expansion complete:\")\n",
    "    print(f\"   Original publications: {len(existing_pubs)}\")\n",
    "    print(f\"   New publications added: {new_count}\")\n",
    "    print(f\"   Total publications: {len(merged_pubs)}\")\n",
    "    print(f\"   Saved to: {output_file}\")\n",
    "    \n",
    "    return merged_pubs\n",
    "\n",
    "# Demo with existing data\n",
    "print(\"üìà Database expansion simulation\")\n",
    "print(f\"Current database size: {len(publications)}\")\n",
    "\n",
    "# Create some demo \"new\" publications\n",
    "demo_new_pubs = [\n",
    "    {\n",
    "        'pmid': '99999999',\n",
    "        'title': 'Demo paper: Synaptic plasticity in hippocampal circuits',\n",
    "        'authors': 'Demo Author, A.; Demo Author, B.',\n",
    "        'journal': 'Demo Journal of Neuroscience',\n",
    "        'year': 2023,\n",
    "        'abstract': 'This is a demo abstract about hippocampal synaptic plasticity...',\n",
    "        'doi': '10.1234/demo.2023.001'\n",
    "    }\n",
    "]\n",
    "\n",
    "expanded_db = merge_publication_databases(publications, demo_new_pubs)\n",
    "\n",
    "# Generate updated BibTeX file\n",
    "updated_bibtex = create_bibtex_from_publications(expanded_db, '../data/processed/expanded_ifc_publications.bib')\n",
    "print(f\"\\nüìö Updated BibTeX file: {updated_bibtex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b1128",
   "metadata": {},
   "source": [
    "## 6. Automated Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83401f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_pipeline(initial_json_path, output_dir='../data/processed'):\n",
    "    \"\"\"Complete automated pipeline to expand publication database\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting complete publication database expansion pipeline\\n\")\n",
    "    \n",
    "    # Step 1: Load existing data\n",
    "    print(\"üìÇ Step 1: Loading existing publications\")\n",
    "    with open(initial_json_path, 'r', encoding='utf-8') as f:\n",
    "        existing_pubs = json.load(f)\n",
    "    print(f\"   Loaded {len(existing_pubs)} existing publications\")\n",
    "    \n",
    "    # Step 2: Create BibTeX for manual download\n",
    "    print(\"\\nüìö Step 2: Creating BibTeX file for Zotero\")\n",
    "    bibtex_path = os.path.join(output_dir, 'ifc_publications_for_zotero.bib')\n",
    "    create_bibtex_from_publications(existing_pubs, bibtex_path)\n",
    "    \n",
    "    # Step 3: Search PubMed for additional papers\n",
    "    print(\"\\nüîç Step 3: Searching PubMed for additional publications\")\n",
    "    searcher = PubmedSearcher()\n",
    "    new_articles = searcher.comprehensive_search(max_per_query=30)\n",
    "    print(f\"   Found {len(new_articles)} potential new articles\")\n",
    "    \n",
    "    # Step 4: Merge databases\n",
    "    print(\"\\nüîÑ Step 4: Merging and deduplicating databases\")\n",
    "    expanded_json_path = os.path.join(output_dir, 'expanded_ifc_publications.json')\n",
    "    final_db = merge_publication_databases(existing_pubs, new_articles, expanded_json_path)\n",
    "    \n",
    "    # Step 5: Create final BibTeX\n",
    "    print(\"\\nüìö Step 5: Creating final BibTeX file\")\n",
    "    final_bibtex_path = os.path.join(output_dir, 'final_ifc_publications.bib')\n",
    "    create_bibtex_from_publications(final_db, final_bibtex_path)\n",
    "    \n",
    "    # Step 6: Generate summary report\n",
    "    print(\"\\nüìä Step 6: Generating summary report\")\n",
    "    report = {\n",
    "        'pipeline_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'original_count': len(existing_pubs),\n",
    "        'pubmed_found': len(new_articles),\n",
    "        'final_count': len(final_db),\n",
    "        'new_additions': len(final_db) - len(existing_pubs),\n",
    "        'files_created': {\n",
    "            'expanded_json': expanded_json_path,\n",
    "            'bibtex_original': bibtex_path,\n",
    "            'bibtex_final': final_bibtex_path\n",
    "        },\n",
    "        'year_distribution': {},\n",
    "        'top_journals': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze year distribution\n",
    "    years = [pub.get('year') for pub in final_db if pub.get('year')]\n",
    "    year_counts = Counter(years)\n",
    "    report['year_distribution'] = dict(year_counts.most_common(10))\n",
    "    \n",
    "    # Analyze top journals\n",
    "    journals = [pub.get('journal') for pub in final_db if pub.get('journal')]\n",
    "    journal_counts = Counter(journals)\n",
    "    report['top_journals'] = dict(journal_counts.most_common(10))\n",
    "    \n",
    "    # Save report\n",
    "    report_path = os.path.join(output_dir, 'pipeline_report.json')\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pipeline complete! Summary:\")\n",
    "    print(f\"   üìä Original: {report['original_count']} publications\")\n",
    "    print(f\"   üÜï Added: {report['new_additions']} new publications\")\n",
    "    print(f\"   üìà Final: {report['final_count']} total publications\")\n",
    "    print(f\"   üìÑ Report saved: {report_path}\")\n",
    "    \n",
    "    return final_db, report\n",
    "\n",
    "# Run the complete pipeline (uncomment to execute)\n",
    "# final_database, pipeline_report = run_complete_pipeline('../data/raw/test_ifc_publications.json')\n",
    "\n",
    "print(\"\\nüéØ Pipeline ready! Uncomment the line above to run the complete workflow.\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Run this pipeline to expand your database\")\n",
    "print(\"2. Import the BibTeX files into Zotero to download PDFs\")\n",
    "print(\"3. Use the expanded JSON database for your ChromaDB embeddings\")\n",
    "print(\"4. Run affiliation mining on downloaded PDFs to find more variations\")\n",
    "print(\"5. Iterate to continuously expand your database\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
