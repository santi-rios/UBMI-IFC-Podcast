{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393732c7",
   "metadata": {},
   "source": [
    "# Testing Advanced AI-Powered Scientific Podcast Generation\n",
    "\n",
    "This notebook tests advanced functionalities for scientific podcast generation using mock data and fallback implementations for paid APIs.\n",
    "\n",
    "We can replace our current keyword-based `_identify_research_field` function with a much more robust machine learning classifier. This will provide more accurate and nuanced categorization of research articles.\n",
    "\n",
    "## Features to Test:\n",
    "1. **AI-Powered Scientific Field Classifier** - Automatic categorization using embeddings\n",
    "2. **Structured Script Generation** - Pydantic-based consistent scientific narrative\n",
    "3. **Multi-Modal RAG Context** - Context-aware generation with related research\n",
    "4. **Complete Pipeline Integration** - All features working together\n",
    "5. **Error Handling** - Robust fallback mechanisms\n",
    "\n",
    "All tests use mock data so they can run without paid API access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fbaaeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added src to path: /home/santi/Projects/UBMI-IFC-Podcast/src\n",
      "Notebook dir: /home/santi/Projects/UBMI-IFC-Podcast/notebooks\n",
      "Src dir: /home/santi/Projects/UBMI-IFC-Podcast/src exists: True\n",
      "Test outputs directory: /home/santi/Projects/UBMI-IFC-Podcast/outputs/advanced_tests\n"
     ]
    }
   ],
   "source": [
    "# Setup: paths and imports\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Add project paths\n",
    "notebook_dir = Path().resolve()\n",
    "src_dir = notebook_dir.parent / 'src'\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "    print('Added src to path:', src_dir)\n",
    "    \n",
    "print('Notebook dir:', notebook_dir)\n",
    "print('Src dir:', src_dir, 'exists:', src_dir.exists())\n",
    "\n",
    "# Create output directories\n",
    "outputs_dir = notebook_dir.parent / 'outputs'\n",
    "test_outputs_dir = outputs_dir / 'advanced_tests'\n",
    "test_outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Test outputs directory: {test_outputs_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b18f699e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… pydantic already installed\n",
      "ðŸ“¦ Installing scikit-learn...\n",
      "Requirement already satisfied: scikit-learn in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/santi/Projects/UBMI-IFC-Podcast/venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "âœ… scikit-learn installed successfully\n",
      "âœ… numpy already installed\n",
      "âœ… pandas already installed\n",
      "\n",
      "âœ… All required packages imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages_to_install = ['pydantic', 'scikit-learn', 'numpy', 'pandas']\n",
    "\n",
    "for package in packages_to_install:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f'âœ… {package} already installed')\n",
    "    except ImportError:\n",
    "        print(f'ðŸ“¦ Installing {package}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        print(f'âœ… {package} installed successfully')\n",
    "\n",
    "# Now import all required packages\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "print('\\nâœ… All required packages imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d362d",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Configuration\n",
    "\n",
    "Create mock providers and configure test environment with fallback implementations for paid APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc288376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mock providers initialized successfully\n",
      "   ðŸ“Š Embedding Provider: Ready (768-dimensional vectors)\n",
      "   ðŸ¤– LLM Provider: Ready (structured + contextual responses)\n"
     ]
    }
   ],
   "source": [
    "# Mock providers for testing without paid APIs\n",
    "class MockEmbeddingProvider:\n",
    "    \"\"\"Mock embedding provider that generates realistic embeddings for testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedding_dim = 768  # Standard embedding dimension\n",
    "        self.logger = self._setup_logger()\n",
    "        \n",
    "    def _setup_logger(self):\n",
    "        import logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        return logging.getLogger(__name__)\n",
    "    \n",
    "    async def generate_embedding(self, text: str, task_type: str = \"CLASSIFICATION\") -> List[float]:\n",
    "        \"\"\"Generate mock embedding based on text content\"\"\"\n",
    "        # Use text hashing and feature extraction to create realistic embeddings\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        # Create seed based on text content for reproducible embeddings\n",
    "        seed = sum(ord(c) for c in text) % 10000\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Base embedding with random values\n",
    "        embedding = np.random.normal(0, 0.1, self.embedding_dim)\n",
    "        \n",
    "        # Add semantic features based on keywords\n",
    "        feature_weights = {\n",
    "            'neuroscience': np.random.normal(0.8, 0.1, 50),\n",
    "            'brain': np.random.normal(0.7, 0.1, 50),\n",
    "            'neural': np.random.normal(0.7, 0.1, 50),\n",
    "            'cancer': np.random.normal(0.9, 0.1, 50),\n",
    "            'tumor': np.random.normal(0.8, 0.1, 50),\n",
    "            'oncology': np.random.normal(0.8, 0.1, 50),\n",
    "            'immune': np.random.normal(0.7, 0.1, 50),\n",
    "            'antibody': np.random.normal(0.6, 0.1, 50),\n",
    "            'vaccine': np.random.normal(0.6, 0.1, 50),\n",
    "            'gene': np.random.normal(0.8, 0.1, 50),\n",
    "            'dna': np.random.normal(0.7, 0.1, 50),\n",
    "            'protein': np.random.normal(0.6, 0.1, 50),\n",
    "        }\n",
    "        \n",
    "        # Apply semantic weights\n",
    "        for word in words:\n",
    "            if word in feature_weights:\n",
    "                feature_vec = feature_weights[word][:self.embedding_dim]\n",
    "                embedding[:len(feature_vec)] += feature_vec\n",
    "        \n",
    "        # Normalize\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        \n",
    "        self.logger.info(f\"Generated embedding for text: '{text[:50]}...' (dim: {len(embedding)})\")\n",
    "        return embedding.tolist()\n",
    "\n",
    "class MockLLMProvider:\n",
    "    \"\"\"Mock LLM provider that generates realistic responses for testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = self._setup_logger()\n",
    "        \n",
    "    def _setup_logger(self):\n",
    "        import logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        return logging.getLogger(__name__)\n",
    "    \n",
    "    async def generate_response(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Generate mock response based on prompt analysis\"\"\"\n",
    "        prompt_lower = prompt.lower()\n",
    "        \n",
    "        # Detect request type and generate appropriate response\n",
    "        if 'json' in kwargs.get('response_format', '').lower() or 'json' in prompt_lower:\n",
    "            return self._generate_structured_response(prompt)\n",
    "        elif 'podcast script' in prompt_lower:\n",
    "            return self._generate_podcast_script(prompt)\n",
    "        elif 'context' in prompt_lower and 'previous work' in prompt_lower:\n",
    "            return self._generate_context_aware_script(prompt)\n",
    "        else:\n",
    "            return self._generate_general_response(prompt)\n",
    "    \n",
    "    def _generate_structured_response(self, prompt: str) -> str:\n",
    "        \"\"\"Generate structured JSON response for Pydantic validation\"\"\"\n",
    "        mock_response = {\n",
    "            \"podcast_title\": \"Breakthrough in Neural Network Plasticity: How Brain Cells Adapt to New Information\",\n",
    "            \"introduction\": \"Have you ever wondered how your brain learns and adapts throughout your life? Today we're diving into fascinating new research that reveals the incredible flexibility of neural networks in the adult brain.\",\n",
    "            \"methods_summary\": \"Researchers used advanced brain imaging techniques combined with machine learning algorithms to track how individual neurons change their connections over time during learning tasks.\",\n",
    "            \"key_findings\": [\n",
    "                \"Adult neurons can form new connections 40% faster than previously thought\",\n",
    "                \"Learning triggers specific protein cascades that strengthen synaptic bonds\",\n",
    "                \"Brain plasticity varies significantly across different regions and age groups\"\n",
    "            ],\n",
    "            \"implications_and_significance\": \"These findings could revolutionize how we approach neurological rehabilitation and age-related cognitive decline. Understanding neural plasticity mechanisms opens new doors for treating stroke patients and developing brain-training programs.\",\n",
    "            \"conclusion\": \"This research fundamentally changes our understanding of the adult brain's capacity for adaptation, proving that we're never too old to learn new tricks - literally at the cellular level.\"\n",
    "        }\n",
    "        return json.dumps(mock_response, indent=2)\n",
    "    \n",
    "    def _generate_podcast_script(self, prompt: str) -> str:\n",
    "        \"\"\"Generate mock podcast script\"\"\"\n",
    "        return \"\"\"Welcome to Science Decoded, the podcast where we break down the latest breakthroughs in research.\n",
    "\n",
    "I'm your host, and today we're exploring groundbreaking work in molecular biology that could change how we understand cellular processes.\n",
    "\n",
    "**The Research**\n",
    "Scientists at leading research institutions have discovered a new mechanism that controls how cells respond to environmental stress. Using cutting-edge techniques, they've identified key proteins that act like molecular switches.\n",
    "\n",
    "**Why It Matters**\n",
    "This discovery has implications for everything from cancer treatment to understanding aging processes. The research opens new avenues for therapeutic intervention.\n",
    "\n",
    "**Looking Forward**\n",
    "As we continue to unravel the mysteries of cellular biology, studies like this remind us how much more we have to learn about the incredible machinery of life.\n",
    "\n",
    "Thanks for joining us today on Science Decoded.\"\"\"\n",
    "    \n",
    "    def _generate_context_aware_script(self, prompt: str) -> str:\n",
    "        \"\"\"Generate context-aware script that references previous work\"\"\"\n",
    "        return \"\"\"Welcome to Research Frontiers, where we explore how today's discoveries build on yesterday's breakthroughs.\n",
    "\n",
    "Today's study represents a significant advancement over previous work in this field. While earlier research established the baseline mechanisms, this new study identifies the specific molecular players involved.\n",
    "\n",
    "**Building on Previous Work**\n",
    "The authors explicitly reference how their findings extend beyond the initial observations from previous studies. Where the earlier work noted general patterns, this research pinpoints exact molecular pathways.\n",
    "\n",
    "**Novel Contributions**\n",
    "What makes this work particularly exciting is how it fills gaps left by previous research. The team has identified the missing pieces that complete our understanding of this biological system.\n",
    "\n",
    "**Scientific Impact**\n",
    "By building upon the solid foundation of previous studies, this research demonstrates the collaborative nature of scientific progress. Each discovery adds another piece to the puzzle.\n",
    "\n",
    "This is exactly how science should work - standing on the shoulders of giants to see even further.\"\"\"\n",
    "\n",
    "    def _generate_general_response(self, prompt: str) -> str:\n",
    "        \"\"\"Generate general response\"\"\"\n",
    "        return \"This is a mock response generated for testing purposes. In a production environment, this would be replaced with actual LLM output.\"\n",
    "\n",
    "# Initialize mock providers\n",
    "mock_embedding_provider = MockEmbeddingProvider()\n",
    "mock_llm_provider = MockLLMProvider()\n",
    "\n",
    "print(\"âœ… Mock providers initialized successfully\")\n",
    "print(\"   ðŸ“Š Embedding Provider: Ready (768-dimensional vectors)\")\n",
    "print(\"   ðŸ¤– LLM Provider: Ready (structured + contextual responses)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7c503",
   "metadata": {},
   "source": [
    "## Section 2: Mock Data Generation\n",
    "\n",
    "Create realistic mock scientific articles, embeddings, and training data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e02e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 50 mock scientific articles\n",
      "\n",
      "ðŸ“Š Data Distribution:\n",
      "   Biochemistry: 12 articles\n",
      "   Neuroscience: 10 articles\n",
      "   Genetics: 10 articles\n",
      "   Cancer Research: 10 articles\n",
      "   Immunology: 8 articles\n",
      "\n",
      "ðŸ“„ Sample Article:\n",
      "   Title: Systematic regulation of protein during development\n",
      "   Field: Biochemistry\n",
      "   Journal: Science\n",
      "   Abstract: This study investigates molecular using mass spectrometry. We analyzed samples and found protein structure revealed. Statistical analysis revealed sig...\n",
      "\n",
      "ðŸ’¾ Mock data saved to: /home/santi/Projects/UBMI-IFC-Podcast/outputs/advanced_tests/mock_articles.json\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive mock scientific articles for testing\n",
    "class MockDataGenerator:\n",
    "    \"\"\"Generate realistic mock data for testing scientific podcast features\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fields = [\n",
    "            'Neuroscience', 'Cancer Research', 'Immunology', 'Genetics', \n",
    "            'Biochemistry', 'Cardiology', 'Infectious Disease', 'Oncology'\n",
    "        ]\n",
    "        self.journals = [\n",
    "            'Nature', 'Science', 'Cell', 'Nature Medicine', 'PNAS',\n",
    "            'Nature Biotechnology', 'Journal of Clinical Investigation',\n",
    "            'Nature Neuroscience', 'Cancer Cell', 'Immunity'\n",
    "        ]\n",
    "    \n",
    "    def generate_mock_articles(self, count: int = 50) -> List[Dict]:\n",
    "        \"\"\"Generate a dataset of mock scientific articles\"\"\"\n",
    "        articles = []\n",
    "        \n",
    "        # Field-specific templates\n",
    "        field_templates = {\n",
    "            'Neuroscience': {\n",
    "                'keywords': ['brain', 'neural', 'neuron', 'synaptic', 'cortex', 'hippocampus', 'plasticity'],\n",
    "                'methods': ['fMRI', 'electrophysiology', 'optogenetics', 'behavioral testing'],\n",
    "                'findings': ['increased activity', 'enhanced connectivity', 'improved memory', 'altered signaling']\n",
    "            },\n",
    "            'Cancer Research': {\n",
    "                'keywords': ['tumor', 'cancer', 'oncology', 'metastasis', 'chemotherapy', 'malignant'],\n",
    "                'methods': ['immunohistochemistry', 'RNA sequencing', 'cell culture', 'xenograft models'],\n",
    "                'findings': ['reduced tumor growth', 'inhibited metastasis', 'enhanced survival', 'drug resistance']\n",
    "            },\n",
    "            'Immunology': {\n",
    "                'keywords': ['immune', 'antibody', 'T-cell', 'vaccine', 'inflammation', 'cytokine'],\n",
    "                'methods': ['flow cytometry', 'ELISA', 'immunofluorescence', 'cell sorting'],\n",
    "                'findings': ['enhanced immune response', 'reduced inflammation', 'improved vaccine efficacy']\n",
    "            },\n",
    "            'Genetics': {\n",
    "                'keywords': ['gene', 'DNA', 'mutation', 'genome', 'CRISPR', 'hereditary'],\n",
    "                'methods': ['genome sequencing', 'PCR', 'gene editing', 'linkage analysis'],\n",
    "                'findings': ['identified mutations', 'gene function revealed', 'inheritance patterns']\n",
    "            },\n",
    "            'Biochemistry': {\n",
    "                'keywords': ['protein', 'enzyme', 'molecular', 'pathway', 'metabolism', 'catalysis'],\n",
    "                'methods': ['protein purification', 'crystallography', 'mass spectrometry'],\n",
    "                'findings': ['protein structure revealed', 'enzyme activity measured', 'pathway identified']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for i in range(count):\n",
    "            # Select random field and template\n",
    "            field = random.choice(list(field_templates.keys()))\n",
    "            template = field_templates[field]\n",
    "            \n",
    "            # Generate realistic article\n",
    "            title_parts = [\n",
    "                random.choice(['Novel', 'Advanced', 'Comprehensive', 'Systematic', 'Innovative']),\n",
    "                random.choice(['mechanisms of', 'role of', 'regulation of', 'analysis of']),\n",
    "                random.choice(template['keywords']),\n",
    "                random.choice(['in', 'during', 'following']),\n",
    "                random.choice(['disease progression', 'cellular response', 'development', 'treatment'])\n",
    "            ]\n",
    "            \n",
    "            title = f\"{title_parts[0]} {title_parts[1]} {title_parts[2]} {title_parts[3]} {title_parts[4]}\"\n",
    "            \n",
    "            # Generate abstract\n",
    "            abstract_parts = [\n",
    "                f\"This study investigates {random.choice(template['keywords'])} using {random.choice(template['methods'])}.\",\n",
    "                f\"We analyzed samples and found {random.choice(template['findings'])}.\",\n",
    "                \"Statistical analysis revealed significant differences between experimental groups.\",\n",
    "                f\"These findings have important implications for {field.lower()} research.\",\n",
    "                \"Our results contribute to the understanding of underlying mechanisms.\"\n",
    "            ]\n",
    "            \n",
    "            abstract = \" \".join(abstract_parts)\n",
    "            \n",
    "            # Generate other metadata\n",
    "            authors = [f\"{chr(65 + random.randint(0, 25))}. {random.choice(['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'])}\" \n",
    "                      for _ in range(random.randint(3, 8))]\n",
    "            \n",
    "            article = {\n",
    "                'title': title,\n",
    "                'abstract': abstract,\n",
    "                'authors': authors,\n",
    "                'journal': random.choice(self.journals),\n",
    "                'publication_date': f\"2024-{random.randint(1,12):02d}-{random.randint(1,28):02d}\",\n",
    "                'doi': f\"10.{random.randint(1000,9999)}/{random.randint(100000,999999)}\",\n",
    "                'pmid': str(random.randint(30000000, 39999999)),\n",
    "                'field': field,  # Ground truth for classification\n",
    "                'score': random.uniform(0.7, 1.0)\n",
    "            }\n",
    "            \n",
    "            articles.append(article)\n",
    "        \n",
    "        return articles\n",
    "\n",
    "# Generate mock data\n",
    "data_generator = MockDataGenerator()\n",
    "mock_articles = data_generator.generate_mock_articles(50)\n",
    "\n",
    "print(f\"âœ… Generated {len(mock_articles)} mock scientific articles\")\n",
    "print(\"\\nðŸ“Š Data Distribution:\")\n",
    "field_counts = pd.Series([article['field'] for article in mock_articles]).value_counts()\n",
    "for field, count in field_counts.items():\n",
    "    print(f\"   {field}: {count} articles\")\n",
    "\n",
    "print(f\"\\nðŸ“„ Sample Article:\")\n",
    "sample = mock_articles[0]\n",
    "print(f\"   Title: {sample['title']}\")\n",
    "print(f\"   Field: {sample['field']}\")\n",
    "print(f\"   Journal: {sample['journal']}\")\n",
    "print(f\"   Abstract: {sample['abstract'][:150]}...\")\n",
    "\n",
    "# Save mock data for reference\n",
    "mock_data_path = test_outputs_dir / 'mock_articles.json'\n",
    "with open(mock_data_path, 'w') as f:\n",
    "    json.dump(mock_articles, f, indent=2)\n",
    "print(f\"\\nðŸ’¾ Mock data saved to: {mock_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e63802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generated embedding for text: 'This study investigates molecular using mass spect...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates T-cell using flow cytometr...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates pathway using mass spectro...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates neural using electrophysio...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates genome using linkage analy...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates malignant using immunohist...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates hereditary using linkage a...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates plasticity using behaviora...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates neuron using optogenetics....' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates metastasis using immunohis...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates immune using cell sorting....' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates gene using PCR. We analyze...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates brain using behavioral tes...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates CRISPR using PCR. We analy...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates CRISPR using PCR. We analy...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates cortex using fMRI. We anal...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates chemotherapy using xenogra...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates molecular using mass spect...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates hippocampus using behavior...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates hereditary using genome se...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates enzyme using protein purif...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates inflammation using immunof...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates catalysis using crystallog...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates chemotherapy using cell cu...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates metabolism using crystallo...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates metastasis using cell cult...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates cancer using cell culture....' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates metastasis using xenograft...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates immune using flow cytometr...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates enzyme using mass spectrom...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates chemotherapy using xenogra...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates T-cell using flow cytometr...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates pathway using protein puri...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates synaptic using electrophys...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates CRISPR using genome sequen...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates gene using PCR. We analyze...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates catalysis using mass spect...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates cytokine using ELISA. We a...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates mutation using genome sequ...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates metastasis using RNA seque...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates tumor using xenograft mode...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates synaptic using optogenetic...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates brain using optogenetics. ...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates brain using fMRI. We analy...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates gene using PCR. We analyze...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates immune using ELISA. We ana...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates enzyme using mass spectrom...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates molecular using mass spect...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates pathway using mass spectro...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study investigates cytokine using immunofluor...' (dim: 768)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§® Generating mock embeddings for classification training...\n",
      "   Processing article 1/50\n",
      "   Processing article 11/50\n",
      "   Processing article 21/50\n",
      "   Processing article 31/50\n",
      "   Processing article 41/50\n",
      "âœ… Generated embeddings for 50 articles\n",
      "   Embedding dimension: 768\n",
      "ðŸ’¾ Embeddings saved to: /home/santi/Projects/UBMI-IFC-Podcast/outputs/advanced_tests/mock_embeddings.json\n",
      "ðŸ“Š Dataset size: 50 articles with 768-dim embeddings\n"
     ]
    }
   ],
   "source": [
    "# Generate mock embeddings for the articles\n",
    "async def generate_mock_embeddings_dataset():\n",
    "    \"\"\"Generate embeddings for all mock articles\"\"\"\n",
    "    print(\"ðŸ§® Generating mock embeddings for classification training...\")\n",
    "    \n",
    "    embeddings_data = []\n",
    "    \n",
    "    for i, article in enumerate(mock_articles):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"   Processing article {i+1}/{len(mock_articles)}\")\n",
    "        \n",
    "        # Generate embedding for abstract\n",
    "        embedding = await mock_embedding_provider.generate_embedding(\n",
    "            article['abstract'], \n",
    "            task_type=\"CLASSIFICATION\"\n",
    "        )\n",
    "        \n",
    "        embeddings_data.append({\n",
    "            'pmid': article['pmid'],\n",
    "            'field': article['field'],\n",
    "            'embedding': embedding,\n",
    "            'title': article['title'][:100]  # Truncated for storage\n",
    "        })\n",
    "    \n",
    "    print(f\"âœ… Generated embeddings for {len(embeddings_data)} articles\")\n",
    "    print(f\"   Embedding dimension: {len(embeddings_data[0]['embedding'])}\")\n",
    "    \n",
    "    return embeddings_data\n",
    "\n",
    "# Generate embeddings dataset\n",
    "embeddings_dataset = await generate_mock_embeddings_dataset()\n",
    "\n",
    "# Save embeddings dataset\n",
    "embeddings_path = test_outputs_dir / 'mock_embeddings.json'\n",
    "with open(embeddings_path, 'w') as f:\n",
    "    json.dump(embeddings_dataset, f, indent=2)\n",
    "\n",
    "print(f\"ðŸ’¾ Embeddings saved to: {embeddings_path}\")\n",
    "print(f\"ðŸ“Š Dataset size: {len(embeddings_dataset)} articles with {len(embeddings_dataset[0]['embedding'])}-dim embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d2d359",
   "metadata": {},
   "source": [
    "## Section 3: Scientific Field Classifier Testing\n",
    "\n",
    "Test the AI-powered scientific field classifier using mock embeddings and pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b600c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MockScientificFieldClassifier initialized\n",
      "ðŸ§  Training scientific field classifier...\n",
      "ðŸ“Š Preparing training data from 50 samples...\n",
      "   Features shape: (50, 768)\n",
      "   Found 5 unique fields: ['Biochemistry', 'Cancer Research', 'Genetics', 'Immunology', 'Neuroscience']\n",
      "   Training accuracy: 1.000\n",
      "   Test accuracy: 0.400\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "   Biochemistry       0.00      0.00      0.00         2\n",
      "Cancer Research       0.50      1.00      0.67         2\n",
      "       Genetics       1.00      0.50      0.67         2\n",
      "     Immunology       0.00      0.00      0.00         2\n",
      "   Neuroscience       0.25      0.50      0.33         2\n",
      "\n",
      "       accuracy                           0.40        10\n",
      "      macro avg       0.35      0.40      0.33        10\n",
      "   weighted avg       0.35      0.40      0.33        10\n",
      "\n",
      "\n",
      "ðŸŽ¯ Classifier Performance Summary:\n",
      "   Training Accuracy: 100.0%\n",
      "   Test Accuracy: 40.0%\n",
      "   Status: âš ï¸ Needs improvement\n"
     ]
    }
   ],
   "source": [
    "# Scientific Field Classifier Implementation\n",
    "class MockScientificFieldClassifier:\n",
    "    \"\"\"\n",
    "    Scientific field classifier using embeddings for automatic categorization.\n",
    "    Uses mock embeddings and scikit-learn for testing without paid APIs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_provider):\n",
    "        self.model = LogisticRegression(\n",
    "            class_weight='balanced', \n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "        self.embedder = embedding_provider\n",
    "        self.is_trained = False\n",
    "        self.class_names = []\n",
    "        self.label_to_class = {}\n",
    "        self.class_to_label = {}\n",
    "        print(\"âœ… MockScientificFieldClassifier initialized\")\n",
    "\n",
    "    def prepare_training_data(self, embeddings_data: List[Dict]) -> tuple:\n",
    "        \"\"\"Prepare training data from embeddings dataset\"\"\"\n",
    "        print(f\"ðŸ“Š Preparing training data from {len(embeddings_data)} samples...\")\n",
    "        \n",
    "        # Extract features and labels\n",
    "        X = np.array([item['embedding'] for item in embeddings_data])\n",
    "        y_raw = [item['field'] for item in embeddings_data]\n",
    "        \n",
    "        # Create label mappings\n",
    "        unique_fields = sorted(list(set(y_raw)))\n",
    "        self.class_names = unique_fields\n",
    "        self.label_to_class = {i: field for i, field in enumerate(unique_fields)}\n",
    "        self.class_to_label = {field: i for i, field in enumerate(unique_fields)}\n",
    "        \n",
    "        # Convert to numeric labels\n",
    "        y = np.array([self.class_to_label[field] for field in y_raw])\n",
    "        \n",
    "        print(f\"   Features shape: {X.shape}\")\n",
    "        print(f\"   Found {len(unique_fields)} unique fields: {unique_fields}\")\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def train(self, embeddings_data: List[Dict]):\n",
    "        \"\"\"Train the classifier on embeddings data\"\"\"\n",
    "        print(\"ðŸ§  Training scientific field classifier...\")\n",
    "        \n",
    "        X, y = self.prepare_training_data(embeddings_data)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Train model\n",
    "        self.model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_score = self.model.score(X_train_scaled, y_train)\n",
    "        test_score = self.model.score(X_test_scaled, y_test)\n",
    "        \n",
    "        print(f\"   Training accuracy: {train_score:.3f}\")\n",
    "        print(f\"   Test accuracy: {test_score:.3f}\")\n",
    "        \n",
    "        # Detailed classification report\n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        class_names_for_report = [self.label_to_class[i] for i in range(len(self.class_names))]\n",
    "        \n",
    "        print(\"\\nðŸ“Š Classification Report:\")\n",
    "        report = classification_report(\n",
    "            y_test, y_pred, \n",
    "            target_names=class_names_for_report,\n",
    "            zero_division=0\n",
    "        )\n",
    "        print(report)\n",
    "        \n",
    "        self.is_trained = True\n",
    "        return train_score, test_score\n",
    "\n",
    "    async def predict(self, article: Dict) -> tuple:\n",
    "        \"\"\"Predict the scientific field for a new article\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise RuntimeError(\"Classifier must be trained before making predictions\")\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = await self.embedder.generate_embedding(\n",
    "            article['abstract'], \n",
    "            task_type=\"CLASSIFICATION\"\n",
    "        )\n",
    "        \n",
    "        # Scale and predict\n",
    "        X = np.array([embedding])\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        prediction_label = self.model.predict(X_scaled)[0]\n",
    "        prediction_proba = self.model.predict_proba(X_scaled)[0]\n",
    "        \n",
    "        predicted_field = self.label_to_class[prediction_label]\n",
    "        confidence = float(prediction_proba[prediction_label])\n",
    "        \n",
    "        # Get top 3 predictions with probabilities\n",
    "        top_indices = np.argsort(prediction_proba)[::-1][:3]\n",
    "        top_predictions = [\n",
    "            (self.label_to_class[idx], float(prediction_proba[idx]))\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        return predicted_field, confidence, top_predictions\n",
    "\n",
    "# Initialize and test the classifier\n",
    "classifier = MockScientificFieldClassifier(mock_embedding_provider)\n",
    "\n",
    "# Train the classifier\n",
    "train_acc, test_acc = classifier.train(embeddings_dataset)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Classifier Performance Summary:\")\n",
    "print(f\"   Training Accuracy: {train_acc:.1%}\")\n",
    "print(f\"   Test Accuracy: {test_acc:.1%}\")\n",
    "print(f\"   Status: {'âœ… Ready for production' if test_acc > 0.7 else 'âš ï¸ Needs improvement'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39da1b",
   "metadata": {},
   "source": [
    "> ðŸ‘ Success\n",
    ">\n",
    "> This entire process is a simulation of a machine learning workflow.\n",
    "> The goal is to teach a simple AI model to associate the \"meaning\" of an article's abstract with a specific scientific field.\n",
    "\n",
    "\n",
    "*From Words to Recognizable Patterns*\n",
    "\n",
    "An AI model like LogisticRegression cannot understand words directly. It only understands numbers. The entire process is about converting the text of an article's abstract into a numerical fingerprint (an \"embedding\") and then teaching the model to recognize the fingerprints of different scientific fields.\n",
    "\n",
    "### Step 1: Generating Realistic (But Fake) Scientific Data\n",
    "\n",
    "- *What it is*: The MockDataGenerator class creates a dataset of 50 fake scientific articles.\n",
    "- *How it works*: It uses templates with field-specific keywords (e.g., 'Neuroscience' gets 'brain', 'neural', 'synaptic'). It then randomly combines these to generate realistic-sounding titles and abstracts.\n",
    "- Each fake article is given a \"ground truth\" label. For example:\n",
    "\n",
    "```\n",
    "# from MockDataGenerator\n",
    "article = {\n",
    "    'title': 'Novel mechanisms of neural plasticity...',\n",
    "    'abstract': 'This study investigates brain activity...',\n",
    "    'field': 'Neuroscience',  # <-- This is the correct answer key\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "### Step 2: Mock Embeddings (AI input)\n",
    "\n",
    "- What it is: It converts an article's abstract into a 768-dimension numerical vector (the \"embedding\").\n",
    "- How it simulates meaning:\n",
    "  - Base Vector: It starts by creating a base vector of 768 random numbers.\n",
    "  - Keyword Detection: It scans the abstract for the same keywords used in the data generator (e.g., 'brain', 'cancer', 'immune')\n",
    "  - Injecting a \"Semantic Signature\": If it finds a keyword, it adds a specific, pre-defined numerical pattern to the base vector.\n",
    "    - For example, all \"neuroscience\" articles will have a similar numerical pattern added to their embeddings because they contain words like 'brain' and 'neural'\n",
    "  - Result: The final embedding is a numerical fingerprint. Abstracts with similar keywords will have mathematically similar fingerprints. This mimics how a real embedding model works, where semantically similar texts result in vectors that are \"close\" to each other in multi-dimensional space.\n",
    "\n",
    "### Step 3: The AI Model - LogisticRegression\n",
    "\n",
    "- What it is: This is the \"AI brain\" you are training. It's a classic, efficient, and powerful machine learning model for classification tasks.\n",
    "- What it does: Its job is to find a mathematical formula that separates the different groups of numerical fingerprints. Think of it as learning to draw boundaries in a high-dimensional space to cordon off the \"Neuroscience\" embeddings from the \"Cancer Research\" embeddings.\n",
    "- Why it's used here: It's fast to train and works very well when the classes are reasonably separable, which our mock embeddings are designed to be.\n",
    "\n",
    "### Step 4: The Training Process (classifier.train)\n",
    "\n",
    "This is where the learning happens.\n",
    "\n",
    "- Prepare Data: The code takes the dataset of 50 articles and their corresponding mock embeddings. It now has the input (X = the 768-dimension embeddings) and the correct answer for each (y = the 'field' label, converted to a number like 0 for Neuroscience, 1 for Cancer Research, etc.).\n",
    "- Split the Data: It splits the data into a training set (80%) and a testing set (20%) using train_test_split.\n",
    "- Training Set: The model gets to see these examples and their correct answers to learn the patterns.\n",
    "- Testing Set: This data is held back. The model has never seen it. We use it at the end to see how well the model performs on new, unseen data.\n",
    "- Scaling: The StandardScaler normalizes the numerical features. This is a standard best practice that helps the LogisticRegression model learn more effectively and quickly.\n",
    "- Learning (`model.fit`): This is the core training command. The model analyzes the training embeddings (X_train_scaled) and their labels (y_train) and adjusts its internal formula to best separate the different fields.\n",
    "- Evaluation: After training, the code immediately uses the unseen testing set to evaluate performance. It shows the model the test embeddings and asks it to predict the field. It then compares the model's predictions to the correct answers (y_test) and calculates the accuracy. The classification_report gives you a detailed breakdown of its performance for each scientific field.\n",
    "\n",
    "## Step 5: Making a Prediction on a New Article (classifier.predict)\n",
    "\n",
    "Once trained, the classifier is ready to be used.\n",
    "\n",
    "- A new, unseen article abstract is provided.\n",
    "- It's passed to the MockEmbeddingProvider to get its unique 768-dimension numerical fingerprint.\n",
    "- This new fingerprint is scaled using the same scaler from the training step.\n",
    "- The scaled fingerprint is fed to the trained model.\n",
    "- The model applies its learned formula and outputs the most likely scientific field (e.g., \"Immunology\") along with a confidence score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b719f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generated embedding for text: 'We investigated the effects of deep brain stimulat...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'This study employed CRISPR-Cas9 gene editing techn...' (dim: 768)\n",
      "INFO:__main__:Generated embedding for text: 'We developed new vaccine adjuvants to improve immu...' (dim: 768)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing classifier predictions on new articles...\n",
      "\n",
      "ðŸ§ª Test Case 1: Deep Brain Stimulation Effects on Parkinson Diseas...\n",
      "   Expected: Neuroscience\n",
      "   Predicted: Cancer Research (confidence: 42.99%)\n",
      "   Result: âŒ Incorrect\n",
      "   Top 3 predictions:\n",
      "      1. Cancer Research: 42.99%\n",
      "      2. Neuroscience: 33.72%\n",
      "      3. Genetics: 11.55%\n",
      "\n",
      "ðŸ§ª Test Case 2: CRISPR-Cas9 Mediated Gene Editing in Cancer Cell L...\n",
      "   Expected: Cancer Research\n",
      "   Predicted: Cancer Research (confidence: 45.09%)\n",
      "   Result: âœ… Correct\n",
      "   Top 3 predictions:\n",
      "      1. Cancer Research: 45.09%\n",
      "      2. Neuroscience: 38.46%\n",
      "      3. Genetics: 6.82%\n",
      "\n",
      "ðŸ§ª Test Case 3: Novel Vaccine Adjuvants Enhance Antibody Response...\n",
      "   Expected: Immunology\n",
      "   Predicted: Cancer Research (confidence: 42.77%)\n",
      "   Result: âŒ Incorrect\n",
      "   Top 3 predictions:\n",
      "      1. Cancer Research: 42.77%\n",
      "      2. Neuroscience: 38.97%\n",
      "      3. Genetics: 7.88%\n",
      "\n",
      "ðŸ“Š Classification Test Results:\n",
      "   Accuracy: 33.3% (1/3)\n",
      "   Average Confidence: 43.6%\n",
      "   Status: âš ï¸ Needs improvement\n",
      "\n",
      "ðŸ’¾ Classification results saved to: /home/santi/Projects/UBMI-IFC-Podcast/outputs/advanced_tests/classification_results.json\n"
     ]
    }
   ],
   "source": [
    "# Test classifier with new articles\n",
    "async def test_classifier_predictions():\n",
    "    \"\"\"Test the trained classifier on new articles\"\"\"\n",
    "    print(\"ðŸ” Testing classifier predictions on new articles...\")\n",
    "    \n",
    "    # Create test articles from different fields\n",
    "    test_cases = [\n",
    "        {\n",
    "            'title': 'Deep Brain Stimulation Effects on Parkinson Disease Motor Symptoms',\n",
    "            'abstract': 'We investigated the effects of deep brain stimulation on motor symptoms in Parkinson disease patients. Using neuroimaging and clinical assessments, we found significant improvements in motor function following DBS treatment. Neural activity patterns showed increased coherence in motor circuits.',\n",
    "            'expected_field': 'Neuroscience'\n",
    "        },\n",
    "        {\n",
    "            'title': 'CRISPR-Cas9 Mediated Gene Editing in Cancer Cell Lines',\n",
    "            'abstract': 'This study employed CRISPR-Cas9 gene editing technology to target oncogenes in multiple cancer cell lines. We observed significant tumor growth inhibition and reduced metastatic potential following gene knockout. Molecular analysis revealed disrupted signaling pathways.',\n",
    "            'expected_field': 'Cancer Research'\n",
    "        },\n",
    "        {\n",
    "            'title': 'Novel Vaccine Adjuvants Enhance Antibody Response',\n",
    "            'abstract': 'We developed new vaccine adjuvants to improve immune response to viral antigens. Flow cytometry analysis showed enhanced T-cell activation and increased antibody production. The adjuvanted vaccines provided superior protection in animal models.',\n",
    "            'expected_field': 'Immunology'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\nðŸ§ª Test Case {i+1}: {test_case['title'][:50]}...\")\n",
    "        \n",
    "        predicted_field, confidence, top_predictions = await classifier.predict(test_case)\n",
    "        \n",
    "        is_correct = predicted_field == test_case['expected_field']\n",
    "        \n",
    "        print(f\"   Expected: {test_case['expected_field']}\")\n",
    "        print(f\"   Predicted: {predicted_field} (confidence: {confidence:.2%})\")\n",
    "        print(f\"   Result: {'âœ… Correct' if is_correct else 'âŒ Incorrect'}\")\n",
    "        \n",
    "        print(f\"   Top 3 predictions:\")\n",
    "        for rank, (field, prob) in enumerate(top_predictions, 1):\n",
    "            print(f\"      {rank}. {field}: {prob:.2%}\")\n",
    "        \n",
    "        results.append({\n",
    "            'test_case': i+1,\n",
    "            'title': test_case['title'],\n",
    "            'expected': test_case['expected_field'],\n",
    "            'predicted': predicted_field,\n",
    "            'confidence': confidence,\n",
    "            'correct': is_correct,\n",
    "            'top_predictions': top_predictions\n",
    "        })\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(r['correct'] for r in results) / len(results)\n",
    "    avg_confidence = sum(r['confidence'] for r in results) / len(results)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Classification Test Results:\")\n",
    "    print(f\"   Accuracy: {accuracy:.1%} ({sum(r['correct'] for r in results)}/{len(results)})\")\n",
    "    print(f\"   Average Confidence: {avg_confidence:.1%}\")\n",
    "    print(f\"   Status: {'âœ… Excellent' if accuracy >= 0.8 else 'ðŸ‘ Good' if accuracy >= 0.6 else 'âš ï¸ Needs improvement'}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run classifier tests\n",
    "classification_results = await test_classifier_predictions()\n",
    "\n",
    "# Save results\n",
    "results_path = test_outputs_dir / 'classification_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(classification_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Classification results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662602df",
   "metadata": {},
   "source": [
    "## Section 4: Structured Script Generation Testing\n",
    "\n",
    "Test Pydantic-based structured script generation using mock API responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a65949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Testing Structured Script Generation...\n",
      "============================================================\n",
      "\n",
      "ðŸ§ª Test 1: Generating script for Biochemistry research...\n",
      "   Article: Systematic regulation of protein during development...\n",
      "âœ… Structured script generated successfully!\n",
      "   Title: Breakthrough in Neural Network Plasticity: How Brain Cells Adapt to New Information\n",
      "   Introduction: Have you ever wondered how your brain learns and adapts throughout your life? Today we're diving int...\n",
      "   Key findings: 3 items\n",
      "   Validation: Title(83 chars), Findings(3 items) âœ…\n",
      "   ðŸ’¾ Saved to: structured_script_1_biochemistry.md\n",
      "\n",
      "ðŸ§ª Test 2: Generating script for Immunology research...\n",
      "   Article: Advanced mechanisms of immune during disease progression...\n",
      "âœ… Structured script generated successfully!\n",
      "   Title: Breakthrough in Neural Network Plasticity: How Brain Cells Adapt to New Information\n",
      "   Introduction: Have you ever wondered how your brain learns and adapts throughout your life? Today we're diving int...\n",
      "   Key findings: 3 items\n",
      "   Validation: Title(83 chars), Findings(3 items) âœ…\n",
      "   ðŸ’¾ Saved to: structured_script_2_immunology.md\n",
      "\n",
      "ðŸ§ª Test 3: Generating script for Biochemistry research...\n",
      "   Article: Comprehensive analysis of protein in development...\n",
      "âœ… Structured script generated successfully!\n",
      "   Title: Breakthrough in Neural Network Plasticity: How Brain Cells Adapt to New Information\n",
      "   Introduction: Have you ever wondered how your brain learns and adapts throughout your life? Today we're diving int...\n",
      "   Key findings: 3 items\n",
      "   Validation: Title(83 chars), Findings(3 items) âœ…\n",
      "   ðŸ’¾ Saved to: structured_script_3_biochemistry.md\n",
      "\n",
      "ðŸ“Š Structured Script Generation Summary:\n",
      "   Success Rate: 100.0% (3/3)\n",
      "   Average Script Length: 1235 characters\n",
      "   Status: âœ… Excellent\n",
      "\n",
      "ðŸ’¾ Structured script results saved to: /home/santi/Projects/UBMI-IFC-Podcast/outputs/advanced_tests/structured_script_results.json\n"
     ]
    }
   ],
   "source": [
    "# Pydantic models for structured script generation\n",
    "class PodcastScriptStructure(BaseModel):\n",
    "    \"\"\"Structured output schema for scientific podcast scripts\"\"\"\n",
    "    model_config = ConfigDict(\n",
    "        json_schema_extra={\n",
    "            \"example\": {\n",
    "                \"podcast_title\": \"Revolutionary Cancer Treatment Shows Promise in Clinical Trials\",\n",
    "                \"introduction\": \"Today we explore breakthrough research that could change cancer treatment...\",\n",
    "                \"methods_summary\": \"Researchers used advanced immunotherapy techniques...\",\n",
    "                \"key_findings\": [\"Finding 1\", \"Finding 2\", \"Finding 3\"],\n",
    "                \"implications_and_significance\": \"These results suggest new therapeutic approaches...\",\n",
    "                \"conclusion\": \"This research opens new doors for cancer patients worldwide.\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    podcast_title: str = Field(\n",
    "        description=\"Engaging, accessible title for the podcast episode\",\n",
    "        min_length=10,\n",
    "        max_length=100\n",
    "    )\n",
    "    introduction: str = Field(\n",
    "        description=\"Hook to grab listener attention, introducing the research topic and importance\",\n",
    "        min_length=50,\n",
    "        max_length=500\n",
    "    )\n",
    "    methods_summary: str = Field(\n",
    "        description=\"Simplified explanation of key research methods, avoiding jargon\",\n",
    "        min_length=30,\n",
    "        max_length=300\n",
    "    )\n",
    "    key_findings: List[str] = Field(\n",
    "        description=\"List of 2-4 main results or discoveries, explained clearly\",\n",
    "        min_items=2,\n",
    "        max_items=4\n",
    "    )\n",
    "    implications_and_significance: str = Field(\n",
    "        description=\"Why these findings matter for science and the public\",\n",
    "        min_length=50,\n",
    "        max_length=400\n",
    "    )\n",
    "    conclusion: str = Field(\n",
    "        description=\"Summary and concluding thought to leave listeners with\",\n",
    "        min_length=30,\n",
    "        max_length=200\n",
    "    )\n",
    "\n",
    "class StructuredScriptGenerator:\n",
    "    \"\"\"Generator for structured podcast scripts using Pydantic validation\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_provider):\n",
    "        self.llm_provider = llm_provider\n",
    "        \n",
    "    async def generate_structured_script(self, article: Dict) -> PodcastScriptStructure:\n",
    "        \"\"\"Generate a structured podcast script from an article\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Generate a structured podcast script for the following scientific article.\n",
    "        Make it accessible to a general audience while maintaining scientific accuracy.\n",
    "        \n",
    "        Article Title: {article.get('title', 'Unknown')}\n",
    "        Journal: {article.get('journal', 'Unknown')}\n",
    "        Field: {article.get('field', 'Unknown')}\n",
    "        \n",
    "        Abstract: {article.get('abstract', 'No abstract available')}\n",
    "        \n",
    "        Return the response as JSON matching the PodcastScriptStructure schema.\n",
    "        Focus on clear explanations and engaging presentation suitable for audio format.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get JSON response from mock LLM\n",
    "        response = await self.llm_provider.generate_response(\n",
    "            prompt, \n",
    "            response_format=\"json\"\n",
    "        )\n",
    "        \n",
    "        # Parse and validate with Pydantic\n",
    "        script_data = json.loads(response)\n",
    "        structured_script = PodcastScriptStructure.model_validate(script_data)\n",
    "        \n",
    "        return structured_script\n",
    "    \n",
    "    def script_to_text(self, structured_script: PodcastScriptStructure) -> str:\n",
    "        \"\"\"Convert structured script to readable text format\"\"\"\n",
    "        text_parts = [\n",
    "            f\"# {structured_script.podcast_title}\\n\",\n",
    "            \"## Introduction\",\n",
    "            structured_script.introduction,\n",
    "            \"\\n## Methods\",\n",
    "            structured_script.methods_summary,\n",
    "            \"\\n## Key Findings\",\n",
    "        ]\n",
    "        \n",
    "        for i, finding in enumerate(structured_script.key_findings, 1):\n",
    "            text_parts.append(f\"{i}. {finding}\")\n",
    "        \n",
    "        text_parts.extend([\n",
    "            \"\\n## Implications and Significance\",\n",
    "            structured_script.implications_and_significance,\n",
    "            \"\\n## Conclusion\",\n",
    "            structured_script.conclusion\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(text_parts)\n",
    "\n",
    "# Test structured script generation\n",
    "script_generator = StructuredScriptGenerator(mock_llm_provider)\n",
    "\n",
    "print(\"ðŸ“ Testing Structured Script Generation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with multiple articles\n",
    "test_articles = mock_articles[:3]  # Use first 3 articles for testing\n",
    "\n",
    "structured_results = []\n",
    "\n",
    "for i, article in enumerate(test_articles):\n",
    "    print(f\"\\nðŸ§ª Test {i+1}: Generating script for {article['field']} research...\")\n",
    "    print(f\"   Article: {article['title'][:60]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Generate structured script\n",
    "        structured_script = await script_generator.generate_structured_script(article)\n",
    "        \n",
    "        print(\"âœ… Structured script generated successfully!\")\n",
    "        print(f\"   Title: {structured_script.podcast_title}\")\n",
    "        print(f\"   Introduction: {structured_script.introduction[:100]}...\")\n",
    "        print(f\"   Key findings: {len(structured_script.key_findings)} items\")\n",
    "        \n",
    "        # Validate structure\n",
    "        validation_results = {\n",
    "            'title_length': len(structured_script.podcast_title),\n",
    "            'intro_length': len(structured_script.introduction),\n",
    "            'findings_count': len(structured_script.key_findings),\n",
    "            'has_conclusion': len(structured_script.conclusion) > 0\n",
    "        }\n",
    "        \n",
    "        print(f\"   Validation: Title({validation_results['title_length']} chars), \"\n",
    "              f\"Findings({validation_results['findings_count']} items) âœ…\")\n",
    "        \n",
    "        # Convert to readable format\n",
    "        full_text = script_generator.script_to_text(structured_script)\n",
    "        \n",
    "        # Save individual script\n",
    "        script_filename = f\"structured_script_{i+1}_{article['field'].lower().replace(' ', '_')}.md\"\n",
    "        script_path = test_outputs_dir / script_filename\n",
    "        \n",
    "        with open(script_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_text)\n",
    "        \n",
    "        print(f\"   ðŸ’¾ Saved to: {script_filename}\")\n",
    "        \n",
    "        structured_results.append({\n",
    "            'article_id': i+1,\n",
    "            'field': article['field'],\n",
    "            'title': article['title'],\n",
    "            'structured_script': structured_script.model_dump(),\n",
    "            'validation': validation_results,\n",
    "            'full_text_length': len(full_text),\n",
    "            'status': 'success'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Script generation failed: {e}\")\n",
    "        structured_results.append({\n",
    "            'article_id': i+1,\n",
    "            'field': article['field'],\n",
    "            'title': article['title'],\n",
    "            'error': str(e),\n",
    "            'status': 'error'\n",
    "        })\n",
    "\n",
    "# Summary of structured generation results\n",
    "successful_generations = sum(1 for r in structured_results if r['status'] == 'success')\n",
    "success_rate = successful_generations / len(structured_results)\n",
    "\n",
    "print(f\"\\nðŸ“Š Structured Script Generation Summary:\")\n",
    "print(f\"   Success Rate: {success_rate:.1%} ({successful_generations}/{len(structured_results)})\")\n",
    "print(f\"   Average Script Length: {np.mean([r.get('full_text_length', 0) for r in structured_results if r['status'] == 'success']):.0f} characters\")\n",
    "print(f\"   Status: {'âœ… Excellent' if success_rate >= 0.8 else 'ðŸ‘ Good' if success_rate >= 0.6 else 'âš ï¸ Needs work'}\")\n",
    "\n",
    "# Save structured results\n",
    "structured_results_path = test_outputs_dir / 'structured_script_results.json'\n",
    "with open(structured_results_path, 'w') as f:\n",
    "    json.dump(structured_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Structured script results saved to: {structured_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cbe4a8",
   "metadata": {},
   "source": [
    "## Section 5: Multi-Modal RAG Context Testing\n",
    "\n",
    "> ðŸ‘ **Goal**: To generate a podcast script that is not just a summary of one article, but a narrative that places the new research into the broader context of its scientific field. This is achieved by first retrieving relevant background documents and then using them to inform the script generation, a process known as Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "This section tests the system's ability to create richer, more insightful content by understanding how a new piece of research connects to previous work.\n",
    "\n",
    "### What is Retrieval-Augmented Generation (RAG)?\n",
    "\n",
    "RAG is an AI technique that enhances a Large Language Model's (LLM) ability to generate accurate and context-aware responses. Instead of relying solely on its pre-trained knowledge, the model first **retrieves** relevant information from an external, up-to-date knowledge base. This retrieved information is then provided to the model as context along with the user's prompt to **generate** a more informed output.\n",
    "\n",
    "**In our case:**\n",
    "1.  **Retrieval**: Find existing research papers, background articles, or methodology documents that are related to the new scientific article we want to summarize.\n",
    "2.  **Augmentation**: Feed both the new article and the retrieved documents into the LLM.\n",
    "3.  **Generation**: Ask the LLM to create a podcast script that explains the new article *in the context of* the retrieved documents.\n",
    "\n",
    "### The \"Multi-Modal\" Aspect in This Test\n",
    "\n",
    "While \"multi-modal\" often refers to different data types (text, image, audio), here it refers to retrieving different **modalities of information** from a textual knowledge base. The system doesn't just find similar articles; it looks for different *types* of context, such as:\n",
    "\n",
    "-   **Previous Research**: Foundational studies that the new article builds upon.\n",
    "-   **Background Information**: General knowledge that helps explain the importance of the field.\n",
    "-   **Methodology Documents**: Papers that explain the techniques used in the new research.\n",
    "\n",
    "By combining these different informational modes, the AI can construct a more complete and compelling narrative.\n",
    "\n",
    "### How the Test Works\n",
    "\n",
    "1.  **Mock Context Database (`MockContextRetriever`)**: We simulate a knowledge base with a few pre-written documents representing \"previous research,\" \"background,\" and \"methodology.\"\n",
    "\n",
    "2.  **Context Retrieval (`find_relevant_context`)**: When a new article is processed, this retriever scans the mock database to find the most relevant documents based on keywords and field. This simulates a real-world vector database search.\n",
    "\n",
    "3.  **Context-Aware Generation (`ContextAwareScriptGenerator`)**:\n",
    "    -   It takes the new article and the list of retrieved context documents.\n",
    "    -   It constructs a detailed prompt for the LLM, explicitly instructing it to use the provided context to highlight how the new research extends, confirms, or challenges previous findings.\n",
    "    -   The mock LLM then generates a script that (in theory) weaves these elements together, demonstrating a deeper understanding of the scientific progression.\n",
    "\n",
    "The ultimate test is to see if the final script successfully tells a story about science in motion, rather than just describing a single, isolated discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9f803ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Testing Multi-Modal RAG Context-Aware Generation...\n",
      "============================================================\n",
      "\n",
      "ðŸ§ª RAG Test 1: Immunology Research\n",
      "   Article: Advanced mechanisms of immune during disease progression...\n",
      "âœ… Context-aware script generated!\n",
      "   Contexts found: 0\n",
      "   Script length: 898 characters\n",
      "   Context integration indicators: 0\n",
      "   Context usage: âš ï¸ Limited\n",
      "   ðŸ’¾ Saved to: rag_script_1_immunology.md\n",
      "\n",
      "ðŸ§ª RAG Test 2: Neuroscience Research\n",
      "   Article: Comprehensive regulation of neural in development...\n",
      "âœ… Context-aware script generated!\n",
      "   Contexts found: 0\n",
      "   Script length: 898 characters\n",
      "   Context integration indicators: 0\n",
      "   Context usage: âš ï¸ Limited\n",
      "   ðŸ’¾ Saved to: rag_script_2_neuroscience.md\n",
      "\n",
      "ðŸ§ª RAG Test 3: Cancer Research Research\n",
      "   Article: Systematic mechanisms of oncology during disease progression...\n",
      "âœ… Context-aware script generated!\n",
      "   Contexts found: 0\n",
      "   Script length: 898 characters\n",
      "   Context integration indicators: 0\n",
      "   Context usage: âš ï¸ Limited\n",
      "   ðŸ’¾ Saved to: rag_script_3_cancer_research.md\n",
      "\n",
      "ðŸ“Š RAG Context-Aware Generation Summary:\n",
      "   Success Rate: 100.0% (3/3)\n",
      "   Average Contexts Found: 0.0\n",
      "   Average Context Indicators: 0.0\n",
      "   Context Integration: âš ï¸ Needs improvement\n",
      "\n",
      "ðŸ’¾ RAG results saved to: /home/santi/Projects/UBMI-IFC-Podcast/outputs/advanced_tests/rag_context_results.json\n"
     ]
    }
   ],
   "source": [
    "# Multi-Modal RAG Context System\n",
    "@dataclass\n",
    "class ContextDocument:\n",
    "    \"\"\"Represents a context document for RAG\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    doc_type: str  # 'previous_research', 'background', 'methodology'\n",
    "    relevance_score: float\n",
    "    metadata: Dict\n",
    "\n",
    "class MockContextRetriever:\n",
    "    \"\"\"Mock context retrieval system for RAG testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.context_db = self._build_mock_context_database()\n",
    "    \n",
    "    def _build_mock_context_database(self) -> List[ContextDocument]:\n",
    "        \"\"\"Build a database of mock context documents\"\"\"\n",
    "        contexts = [\n",
    "            ContextDocument(\n",
    "                title=\"Foundational Study on Neural Plasticity Mechanisms\",\n",
    "                content=\"Previous research in our laboratory established that neural plasticity involves complex molecular cascades. We identified key proteins including CREB and BDNF that regulate synaptic strength. However, the temporal dynamics and regional specificity remained unclear.\",\n",
    "                doc_type=\"previous_research\",\n",
    "                relevance_score=0.9,\n",
    "                metadata={'year': 2022, 'citations': 156}\n",
    "            ),\n",
    "            ContextDocument(\n",
    "                title=\"Methodological Advances in Protein Analysis\",\n",
    "                content=\"Recent developments in proteomics have enabled precise quantification of synaptic proteins. Mass spectrometry coupled with fluorescence microscopy provides unprecedented resolution for studying protein-protein interactions in live cells.\",\n",
    "                doc_type=\"methodology\",\n",
    "                relevance_score=0.8,\n",
    "                metadata={'year': 2023, 'citations': 89}\n",
    "            ),\n",
    "            ContextDocument(\n",
    "                title=\"Clinical Implications of Synaptic Dysfunction\",\n",
    "                content=\"Synaptic dysfunction underlies numerous neurological disorders including Alzheimer's disease and schizophrenia. Understanding molecular mechanisms of synaptic plasticity is crucial for developing targeted therapeutics.\",\n",
    "                doc_type=\"background\",\n",
    "                relevance_score=0.85,\n",
    "                metadata={'year': 2023, 'citations': 203}\n",
    "            ),\n",
    "            ContextDocument(\n",
    "                title=\"Cancer Cell Metabolism and Treatment Resistance\",\n",
    "                content=\"Our previous work demonstrated that cancer cells alter their metabolic pathways to survive chemotherapy. We identified key enzymes that could serve as therapeutic targets, but the mechanisms of resistance adaptation remained unexplored.\",\n",
    "                doc_type=\"previous_research\",\n",
    "                relevance_score=0.88,\n",
    "                metadata={'year': 2022, 'citations': 178}\n",
    "            ),\n",
    "            ContextDocument(\n",
    "                title=\"Immunotherapy Breakthrough: T-cell Engineering\",\n",
    "                content=\"Earlier studies from our group showed that engineered T-cells can effectively target cancer cells. However, off-target effects and T-cell exhaustion limited clinical applications. New approaches are needed to overcome these challenges.\",\n",
    "                doc_type=\"previous_research\",\n",
    "                relevance_score=0.91,\n",
    "                metadata={'year': 2021, 'citations': 245}\n",
    "            )\n",
    "        ]\n",
    "        return contexts\n",
    "    \n",
    "    async def find_relevant_context(self, article: Dict, max_contexts: int = 3) -> List[ContextDocument]:\n",
    "        \"\"\"Find relevant context documents for an article\"\"\"\n",
    "        article_field = article.get('field', '').lower()\n",
    "        article_text = (article.get('title', '') + ' ' + article.get('abstract', '')).lower()\n",
    "        \n",
    "        # Simple keyword-based matching for demo\n",
    "        field_keywords = {\n",
    "            'neuroscience': ['neural', 'brain', 'synap', 'neuron'],\n",
    "            'cancer research': ['cancer', 'tumor', 'oncology', 'chemotherapy'],\n",
    "            'immunology': ['immune', 'antibody', 'vaccine', 't-cell']\n",
    "        }\n",
    "        \n",
    "        relevant_contexts = []\n",
    "        \n",
    "        for context in self.context_db:\n",
    "            relevance = 0.0\n",
    "            \n",
    "            # Field matching\n",
    "            if article_field in context.content.lower():\n",
    "                relevance += 0.4\n",
    "            \n",
    "            # Keyword matching\n",
    "            if article_field in field_keywords:\n",
    "                keywords = field_keywords[article_field]\n",
    "                matching_keywords = sum(1 for kw in keywords if kw in context.content.lower())\n",
    "                relevance += 0.3 * (matching_keywords / len(keywords))\n",
    "            \n",
    "            # Content similarity (simplified)\n",
    "            common_words = set(article_text.split()) & set(context.content.lower().split())\n",
    "            relevance += 0.3 * (len(common_words) / 100)  # Normalize\n",
    "            \n",
    "            if relevance > 0.2:  # Threshold for relevance\n",
    "                context.relevance_score = relevance\n",
    "                relevant_contexts.append(context)\n",
    "        \n",
    "        # Sort by relevance and return top results\n",
    "        relevant_contexts.sort(key=lambda x: x.relevance_score, reverse=True)\n",
    "        return relevant_contexts[:max_contexts]\n",
    "\n",
    "class ContextAwareScriptGenerator:\n",
    "    \"\"\"Generate scripts that incorporate relevant context\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_provider, context_retriever):\n",
    "        self.llm_provider = llm_provider\n",
    "        self.context_retriever = context_retriever\n",
    "    \n",
    "    async def generate_context_aware_script(self, article: Dict) -> str:\n",
    "        \"\"\"Generate script with relevant context\"\"\"\n",
    "        \n",
    "        # Retrieve relevant context\n",
    "        contexts = await self.context_retriever.find_relevant_context(article)\n",
    "        \n",
    "        # Build context section for prompt\n",
    "        context_text = \"\"\n",
    "        if contexts:\n",
    "            context_text = \"\\nRELAVANT CONTEXT:\\n\"\n",
    "            for i, ctx in enumerate(contexts, 1):\n",
    "                context_text += f\"\\n{i}. {ctx.title} ({ctx.doc_type})\\n{ctx.content}\\n\"\n",
    "        \n",
    "        # Create enhanced prompt\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert science communicator. Generate a podcast script for the CURRENT ARTICLE.\n",
    "        Use the RELEVANT CONTEXT to provide background and highlight what makes this research significant.\n",
    "        \n",
    "        Explicitly reference how this work builds upon or extends previous findings.\n",
    "        Show the progression of scientific understanding in this field.\n",
    "        \n",
    "        CURRENT ARTICLE:\n",
    "        Title: {article.get('title', 'Unknown')}\n",
    "        Field: {article.get('field', 'Unknown')}\n",
    "        Abstract: {article.get('abstract', 'No abstract available')}\n",
    "        \n",
    "        {context_text}\n",
    "        \n",
    "        Create an engaging podcast script that:\n",
    "        1. Contextualizes the research within the broader scientific landscape\n",
    "        2. Explains how this work builds on previous studies\n",
    "        3. Highlights novel contributions and breakthroughs\n",
    "        4. Makes the science accessible to a general audience\n",
    "        \"\"\"\n",
    "        \n",
    "        script = await self.llm_provider.generate_response(prompt)\n",
    "        \n",
    "        return script, contexts\n",
    "\n",
    "# Test context-aware script generation\n",
    "context_retriever = MockContextRetriever()\n",
    "context_generator = ContextAwareScriptGenerator(mock_llm_provider, context_retriever)\n",
    "\n",
    "print(\"ðŸ“š Testing Multi-Modal RAG Context-Aware Generation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with articles from different fields\n",
    "rag_test_articles = [\n",
    "    article for article in mock_articles \n",
    "    if article['field'] in ['Neuroscience', 'Cancer Research', 'Immunology']\n",
    "][:3]\n",
    "\n",
    "rag_results = []\n",
    "\n",
    "for i, article in enumerate(rag_test_articles):\n",
    "    print(f\"\\nðŸ§ª RAG Test {i+1}: {article['field']} Research\")\n",
    "    print(f\"   Article: {article['title'][:60]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Generate context-aware script\n",
    "        script, used_contexts = await context_generator.generate_context_aware_script(article)\n",
    "        \n",
    "        print(f\"âœ… Context-aware script generated!\")\n",
    "        print(f\"   Contexts found: {len(used_contexts)}\")\n",
    "        \n",
    "        for j, ctx in enumerate(used_contexts, 1):\n",
    "            print(f\"      {j}. {ctx.title[:50]}... (relevance: {ctx.relevance_score:.2f})\")\n",
    "        \n",
    "        # Analyze context usage\n",
    "        context_references = sum(1 for ctx in used_contexts if ctx.title.lower()[:20] in script.lower())\n",
    "        script_length = len(script)\n",
    "        \n",
    "        # Check for context integration indicators\n",
    "        context_indicators = [\n",
    "            'previous work', 'earlier studies', 'builds upon', 'extends',\n",
    "            'foundation', 'background research', 'prior findings'\n",
    "        ]\n",
    "        indicator_count = sum(1 for indicator in context_indicators if indicator in script.lower())\n",
    "        \n",
    "        print(f\"   Script length: {script_length:,} characters\")\n",
    "        print(f\"   Context integration indicators: {indicator_count}\")\n",
    "        print(f\"   Context usage: {'âœ… Good' if indicator_count >= 2 else 'âš ï¸ Limited'}\")\n",
    "        \n",
    "        # Save context-aware script\n",
    "        script_filename = f\"rag_script_{i+1}_{article['field'].lower().replace(' ', '_')}.md\"\n",
    "        script_path = test_outputs_dir / script_filename\n",
    "        \n",
    "        with open(script_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# Context-Aware Script: {article['title']}\\n\\n\")\n",
    "            f.write(f\"**Field:** {article['field']}\\n\")\n",
    "            f.write(f\"**Contexts Used:** {len(used_contexts)}\\n\\n\")\n",
    "            \n",
    "            if used_contexts:\n",
    "                f.write(\"## Relevant Context Sources:\\n\")\n",
    "                for ctx in used_contexts:\n",
    "                    f.write(f\"- {ctx.title} (relevance: {ctx.relevance_score:.2f})\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## Generated Script:\\n\\n\")\n",
    "            f.write(script)\n",
    "        \n",
    "        print(f\"   ðŸ’¾ Saved to: {script_filename}\")\n",
    "        \n",
    "        rag_results.append({\n",
    "            'article_id': i+1,\n",
    "            'field': article['field'],\n",
    "            'title': article['title'],\n",
    "            'contexts_found': len(used_contexts),\n",
    "            'context_usage_indicators': indicator_count,\n",
    "            'script_length': script_length,\n",
    "            'contexts_used': [ctx.title for ctx in used_contexts],\n",
    "            'status': 'success'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Context-aware generation failed: {e}\")\n",
    "        rag_results.append({\n",
    "            'article_id': i+1,\n",
    "            'field': article['field'],\n",
    "            'title': article['title'],\n",
    "            'error': str(e),\n",
    "            'status': 'error'\n",
    "        })\n",
    "\n",
    "# RAG Summary\n",
    "successful_rag = sum(1 for r in rag_results if r['status'] == 'success')\n",
    "rag_success_rate = successful_rag / len(rag_results)\n",
    "avg_contexts = np.mean([r.get('contexts_found', 0) for r in rag_results if r['status'] == 'success'])\n",
    "avg_indicators = np.mean([r.get('context_usage_indicators', 0) for r in rag_results if r['status'] == 'success'])\n",
    "\n",
    "print(f\"\\nðŸ“Š RAG Context-Aware Generation Summary:\")\n",
    "print(f\"   Success Rate: {rag_success_rate:.1%} ({successful_rag}/{len(rag_results)})\")\n",
    "print(f\"   Average Contexts Found: {avg_contexts:.1f}\")\n",
    "print(f\"   Average Context Indicators: {avg_indicators:.1f}\")\n",
    "print(f\"   Context Integration: {'âœ… Excellent' if avg_indicators >= 3 else 'ðŸ‘ Good' if avg_indicators >= 2 else 'âš ï¸ Needs improvement'}\")\n",
    "\n",
    "# Save RAG results\n",
    "rag_results_path = test_outputs_dir / 'rag_context_results.json'\n",
    "with open(rag_results_path, 'w') as f:\n",
    "    json.dump(rag_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nðŸ’¾ RAG results saved to: {rag_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c515bb3",
   "metadata": {},
   "source": [
    "## Section 6: Integration Testing\n",
    "\n",
    "Test the complete pipeline integration with all new functionalities working together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70606d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Advanced Podcast Pipeline...\n",
      "============================================================\n",
      "âœ… MockScientificFieldClassifier initialized\n",
      "ðŸ§  Training scientific field classifier...\n",
      "ðŸ“Š Preparing training data from 50 samples...\n",
      "   Features shape: (50, 768)\n",
      "   Found 5 unique fields: ['Biochemistry', 'Cancer Research', 'Genetics', 'Immunology', 'Neuroscience']\n",
      "   Training accuracy: 1.000\n",
      "   Test accuracy: 0.400\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "   Biochemistry       0.00      0.00      0.00         2\n",
      "Cancer Research       0.50      1.00      0.67         2\n",
      "       Genetics       1.00      0.50      0.67         2\n",
      "     Immunology       0.00      0.00      0.00         2\n",
      "   Neuroscience       0.25      0.50      0.33         2\n",
      "\n",
      "       accuracy                           0.40        10\n",
      "      macro avg       0.35      0.40      0.33        10\n",
      "   weighted avg       0.35      0.40      0.33        10\n",
      "\n",
      "âœ… Classifier trained and ready\n",
      "âœ… Pipeline initialized successfully!\n",
      "   ðŸ“Š Classifier: Trained and ready\n",
      "   ðŸ“š Context retriever: Mock database loaded\n",
      "   ðŸ“ Script generators: Structured + Context-aware\n"
     ]
    }
   ],
   "source": [
    "# Integrated Advanced Pipeline\n",
    "class AdvancedPodcastPipeline:\n",
    "    \"\"\"Complete advanced pipeline integrating all new functionalities\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_provider, llm_provider):\n",
    "        # Initialize components\n",
    "        self.classifier = MockScientificFieldClassifier(embedding_provider)\n",
    "        self.context_retriever = MockContextRetriever()\n",
    "        self.structured_generator = StructuredScriptGenerator(llm_provider)\n",
    "        self.context_generator = ContextAwareScriptGenerator(llm_provider, self.context_retriever)\n",
    "        \n",
    "        # Train classifier if embeddings data available\n",
    "        if 'embeddings_dataset' in globals():\n",
    "            self.classifier.train(embeddings_dataset)\n",
    "            print(\"âœ… Classifier trained and ready\")\n",
    "        \n",
    "        self.processing_stats = {\n",
    "            'articles_processed': 0,\n",
    "            'classifications_made': 0,\n",
    "            'contexts_retrieved': 0,\n",
    "            'scripts_generated': 0,\n",
    "            'errors': 0\n",
    "        }\n",
    "    \n",
    "    async def process_article_complete(self, article: Dict) -> Dict:\n",
    "        \"\"\"Process an article through the complete advanced pipeline\"\"\"\n",
    "        result = {\n",
    "            'article_id': article.get('pmid', 'unknown'),\n",
    "            'title': article.get('title', 'Unknown'),\n",
    "            'original_field': article.get('field', 'Unknown'),\n",
    "            'processing_steps': {},\n",
    "            'outputs': {},\n",
    "            'errors': [],\n",
    "            'status': 'processing'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Automatic Field Classification\n",
    "            print(f\"ðŸ” Step 1: Classifying article field...\")\n",
    "            if self.classifier.is_trained:\n",
    "                predicted_field, confidence, top_predictions = await self.classifier.predict(article)\n",
    "                result['processing_steps']['classification'] = {\n",
    "                    'predicted_field': predicted_field,\n",
    "                    'confidence': confidence,\n",
    "                    'top_predictions': top_predictions,\n",
    "                    'matches_original': predicted_field == article.get('field', ''),\n",
    "                    'status': 'success'\n",
    "                }\n",
    "                self.processing_stats['classifications_made'] += 1\n",
    "                print(f\"   Predicted field: {predicted_field} (confidence: {confidence:.2%})\")\n",
    "            else:\n",
    "                result['processing_steps']['classification'] = {\n",
    "                    'status': 'skipped',\n",
    "                    'reason': 'classifier_not_trained'\n",
    "                }\n",
    "                predicted_field = article.get('field', 'Unknown')\n",
    "            \n",
    "            # Step 2: Context Retrieval (RAG)\n",
    "            print(f\"ðŸ“š Step 2: Retrieving relevant context...\")\n",
    "            relevant_contexts = await self.context_retriever.find_relevant_context(article, max_contexts=3)\n",
    "            result['processing_steps']['context_retrieval'] = {\n",
    "                'contexts_found': len(relevant_contexts),\n",
    "                'context_titles': [ctx.title for ctx in relevant_contexts],\n",
    "                'avg_relevance': np.mean([ctx.relevance_score for ctx in relevant_contexts]) if relevant_contexts else 0,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            self.processing_stats['contexts_retrieved'] += len(relevant_contexts)\n",
    "            print(f\"   Found {len(relevant_contexts)} relevant contexts\")\n",
    "            \n",
    "            # Step 3: Generate Structured Script\n",
    "            print(f\"ðŸ“ Step 3: Generating structured script...\")\n",
    "            structured_script = await self.structured_generator.generate_structured_script(article)\n",
    "            result['processing_steps']['structured_generation'] = {\n",
    "                'title': structured_script.podcast_title,\n",
    "                'sections_generated': 5,  # intro, methods, findings, implications, conclusion\n",
    "                'findings_count': len(structured_script.key_findings),\n",
    "                'word_count': len(structured_script.model_dump_json().split()),\n",
    "                'status': 'success'\n",
    "            }\n",
    "            result['outputs']['structured_script'] = structured_script.model_dump()\n",
    "            print(f\"   Generated structured script: '{structured_script.podcast_title}'\")\n",
    "            \n",
    "            # Step 4: Generate Context-Aware Script\n",
    "            print(f\"ðŸ§  Step 4: Generating context-aware script...\")\n",
    "            context_script, used_contexts = await self.context_generator.generate_context_aware_script(article)\n",
    "            result['processing_steps']['context_aware_generation'] = {\n",
    "                'contexts_used': len(used_contexts),\n",
    "                'context_indicators': sum(1 for indicator in ['previous work', 'builds upon', 'earlier studies'] \n",
    "                                        if indicator in context_script.lower()),\n",
    "                'script_length': len(context_script),\n",
    "                'status': 'success'\n",
    "            }\n",
    "            result['outputs']['context_aware_script'] = context_script\n",
    "            self.processing_stats['scripts_generated'] += 2  # structured + context-aware\n",
    "            print(f\"   Generated context-aware script ({len(context_script):,} chars)\")\n",
    "            \n",
    "            # Step 5: Pipeline Integration Analysis\n",
    "            print(f\"ðŸ”— Step 5: Pipeline integration analysis...\")\n",
    "            \n",
    "            # Check consistency between classification and context\n",
    "            classification_context_match = any(\n",
    "                predicted_field.lower() in ctx.content.lower() \n",
    "                for ctx in relevant_contexts\n",
    "            ) if relevant_contexts else False\n",
    "            \n",
    "            # Analyze script quality metrics\n",
    "            structured_quality = {\n",
    "                'has_all_sections': all([\n",
    "                    structured_script.introduction,\n",
    "                    structured_script.methods_summary,\n",
    "                    structured_script.key_findings,\n",
    "                    structured_script.implications_and_significance,\n",
    "                    structured_script.conclusion\n",
    "                ]),\n",
    "                'findings_adequate': 2 <= len(structured_script.key_findings) <= 4,\n",
    "                'length_appropriate': 500 <= len(structured_script.model_dump_json()) <= 2000\n",
    "            }\n",
    "            \n",
    "            context_quality = {\n",
    "                'has_context_integration': sum(1 for indicator in ['previous', 'builds', 'foundation'] \n",
    "                                             if indicator in context_script.lower()) >= 2,\n",
    "                'appropriate_length': len(context_script) >= 800,\n",
    "                'uses_retrieved_contexts': len(used_contexts) > 0\n",
    "            }\n",
    "            \n",
    "            result['processing_steps']['integration_analysis'] = {\n",
    "                'classification_context_consistency': classification_context_match,\n",
    "                'structured_quality_score': sum(structured_quality.values()) / len(structured_quality),\n",
    "                'context_quality_score': sum(context_quality.values()) / len(context_quality),\n",
    "                'overall_pipeline_score': 0.0,  # Will calculate below\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "            # Calculate overall pipeline score\n",
    "            pipeline_scores = [\n",
    "                result['processing_steps']['classification'].get('confidence', 0.5),\n",
    "                result['processing_steps']['context_retrieval']['avg_relevance'],\n",
    "                result['processing_steps']['integration_analysis']['structured_quality_score'],\n",
    "                result['processing_steps']['integration_analysis']['context_quality_score']\n",
    "            ]\n",
    "            overall_score = np.mean(pipeline_scores)\n",
    "            result['processing_steps']['integration_analysis']['overall_pipeline_score'] = overall_score\n",
    "            \n",
    "            result['status'] = 'completed'\n",
    "            self.processing_stats['articles_processed'] += 1\n",
    "            \n",
    "            print(f\"âœ… Pipeline completed! Overall score: {overall_score:.2%}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Pipeline error: {str(e)}\"\n",
    "            result['errors'].append(error_msg)\n",
    "            result['status'] = 'error'\n",
    "            self.processing_stats['errors'] += 1\n",
    "            print(f\"âŒ {error_msg}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_pipeline_stats(self) -> Dict:\n",
    "        \"\"\"Get pipeline processing statistics\"\"\"\n",
    "        return {\n",
    "            **self.processing_stats,\n",
    "            'success_rate': (self.processing_stats['articles_processed'] - self.processing_stats['errors']) / \n",
    "                          max(1, self.processing_stats['articles_processed']),\n",
    "            'avg_contexts_per_article': self.processing_stats['contexts_retrieved'] / \n",
    "                                      max(1, self.processing_stats['articles_processed']),\n",
    "            'avg_scripts_per_article': self.processing_stats['scripts_generated'] / \n",
    "                                     max(1, self.processing_stats['articles_processed'])\n",
    "        }\n",
    "\n",
    "# Initialize integrated pipeline\n",
    "print(\"ðŸš€ Initializing Advanced Podcast Pipeline...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "advanced_pipeline = AdvancedPodcastPipeline(mock_embedding_provider, mock_llm_provider)\n",
    "\n",
    "print(\"âœ… Pipeline initialized successfully!\")\n",
    "print(\"   ðŸ“Š Classifier: Trained and ready\")\n",
    "print(\"   ðŸ“š Context retriever: Mock database loaded\")\n",
    "print(\"   ðŸ“ Script generators: Structured + Context-aware\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
