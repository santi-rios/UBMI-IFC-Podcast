{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f49148e",
   "metadata": {},
   "source": [
    "# IFC Scraper Testing Notebook\n",
    "\n",
    "This notebook tests the IFC-UNAM publication scraper component.\n",
    "\n",
    "## Overview\n",
    "- Test scraping publications from IFC-UNAM website\n",
    "- Parse and validate the scraped data\n",
    "- Save results for further processing\n",
    "\n",
    "**Note**: You may need to adjust the scraper selectors based on the actual HTML structure of the IFC website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup imports and path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027aacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our scraper\n",
    "from scrapers.ifc_scraper import IFCPublicationScraper\n",
    "from utils.config import load_config\n",
    "from utils.logger import setup_logger, get_logger\n",
    "\n",
    "# Setup logging\n",
    "setup_logger(level=\"INFO\")\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494c1fc",
   "metadata": {},
   "source": [
    "## 1. Initialize the Scraper\n",
    "\n",
    "Load configuration and create scraper instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245124a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config()\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Base URL: {config['ifc']['base_url']}\")\n",
    "print(f\"Years range: {config['ifc']['years_range']}\")\n",
    "print(f\"Rate limit delay: {config['ifc']['rate_limit_delay']}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73966a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scraper\n",
    "scraper = IFCPublicationScraper(config)\n",
    "print(\"Scraper initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574bd541",
   "metadata": {},
   "source": [
    "## 2. Test Scraping a Single Year\n",
    "\n",
    "Let's start by testing with a single year to see the HTML structure and adjust our selectors if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c6e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scraping for 2024 first\n",
    "test_year = 2024\n",
    "print(f\"Testing scraper for year {test_year}...\")\n",
    "\n",
    "try:\n",
    "    publications = await scraper.scrape_publications_by_year(test_year)\n",
    "    print(f\"Successfully scraped {len(publications)} publications for {test_year}\")\n",
    "    \n",
    "    if publications:\n",
    "        print(\"\\nFirst publication sample:\")\n",
    "        sample = publications[0]\n",
    "        print(f\"Title: {sample.title}\")\n",
    "        print(f\"Authors: {sample.authors}\")\n",
    "        print(f\"Journal: {sample.journal}\")\n",
    "        print(f\"Abstract: {sample.abstract[:200] if sample.abstract else 'No abstract'}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"This is expected if the website selectors need adjustment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d0fff",
   "metadata": {},
   "source": [
    "## 3. Inspect Website Structure\n",
    "\n",
    "If the scraper fails, let's manually inspect the website structure to understand the HTML layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual inspection of the website\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "async def inspect_website(year=2024):\n",
    "    url = f\"https://www.ifc.unam.mx/publicaciones.php?year={year}\"\n",
    "    print(f\"Inspecting: {url}\")\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                if response.status == 200:\n",
    "                    html = await response.text()\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    \n",
    "                    print(f\"Page title: {soup.title.text if soup.title else 'No title'}\")\n",
    "                    print(f\"Page length: {len(html)} characters\")\n",
    "                    \n",
    "                    # Look for common publication container patterns\n",
    "                    potential_containers = [\n",
    "                        soup.find_all('div', class_=lambda x: x and 'publication' in x.lower()),\n",
    "                        soup.find_all('div', class_=lambda x: x and 'article' in x.lower()),\n",
    "                        soup.find_all('li'),\n",
    "                        soup.find_all('tr'),\n",
    "                    ]\n",
    "                    \n",
    "                    for i, containers in enumerate(potential_containers):\n",
    "                        print(f\"\\nPotential container type {i+1}: {len(containers)} elements\")\n",
    "                        if containers and len(containers) > 0:\n",
    "                            print(f\"Sample: {str(containers[0])[:200]}...\")\n",
    "                            \n",
    "                else:\n",
    "                    print(f\"Failed to fetch page: {response.status}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error inspecting website: {e}\")\n",
    "\n",
    "# Run inspection\n",
    "await inspect_website(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6f79a",
   "metadata": {},
   "source": [
    "## 4. Test Data Processing\n",
    "\n",
    "Even if scraping fails, let's test the data processing with mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock publications for testing\n",
    "from scrapers.ifc_scraper import Publication\n",
    "\n",
    "mock_publications = [\n",
    "    Publication(\n",
    "        title=\"Neural mechanisms of memory formation in hippocampal circuits\",\n",
    "        authors=\"García-López, M., Rodríguez-Silva, A., Mendoza-Pérez, J.\",\n",
    "        journal=\"Journal of Neuroscience\",\n",
    "        year=2024,\n",
    "        doi=\"10.1523/JNEUROSCI.1234-24.2024\",\n",
    "        pubmed_id=\"38123456\",\n",
    "        ifc_url=\"https://www.ifc.unam.mx/publicacion.php?ut=000123456789\",\n",
    "        abstract=\"We investigated the cellular and molecular mechanisms underlying memory formation in hippocampal circuits. Using electrophysiological recordings and optogenetic manipulations, we found that...\"\n",
    "    ),\n",
    "    Publication(\n",
    "        title=\"Cardiac physiology under metabolic stress conditions\",\n",
    "        authors=\"Hernández-Campos, L., López-Martín, R.\",\n",
    "        journal=\"Cardiovascular Research\",\n",
    "        year=2024,\n",
    "        doi=\"10.1093/cvr/cvz098\",\n",
    "        pubmed_id=\"38234567\",\n",
    "        ifc_url=\"https://www.ifc.unam.mx/publicacion.php?ut=000234567890\",\n",
    "        abstract=\"Heart function during metabolic stress was analyzed using isolated perfused heart preparations. Our results demonstrate significant changes in...\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Created {len(mock_publications)} mock publications\")\n",
    "for i, pub in enumerate(mock_publications, 1):\n",
    "    print(f\"{i}. {pub.title[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210166b",
   "metadata": {},
   "source": [
    "## 5. Test Data Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test saving publications\n",
    "output_dir = Path(\"../data/raw\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save mock data\n",
    "scraper.save_publications(mock_publications, output_dir / \"test_ifc_publications.json\")\n",
    "\n",
    "# Verify saved data\n",
    "import json\n",
    "with open(output_dir / \"test_ifc_publications.json\", 'r') as f:\n",
    "    saved_data = json.load(f)\n",
    "    \n",
    "print(f\"Saved {len(saved_data)} publications to file\")\n",
    "print(\"Sample saved data:\")\n",
    "print(json.dumps(saved_data[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d14f2f9",
   "metadata": {},
   "source": [
    "## 6. Test Multiple Years (if single year works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775462ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this if the single year test worked\n",
    "# Uncomment and run if ready\n",
    "\n",
    "# try:\n",
    "#     all_publications = await scraper.scrape_all_years(2023, 2024)  # Test with 2 years\n",
    "#     print(f\"Successfully scraped {len(all_publications)} total publications\")\n",
    "#     \n",
    "#     # Save all data\n",
    "#     scraper.save_publications(all_publications, output_dir / \"all_ifc_publications.json\")\n",
    "#     \n",
    "#     # Analysis\n",
    "#     df = pd.DataFrame([{\n",
    "#         'title': pub.title,\n",
    "#         'authors': pub.authors,\n",
    "#         'journal': pub.journal,\n",
    "#         'year': pub.year,\n",
    "#         'has_abstract': bool(pub.abstract)\n",
    "#     } for pub in all_publications])\n",
    "#     \n",
    "#     print(\"\\nData summary:\")\n",
    "#     print(df.groupby('year').size())\n",
    "#     print(f\"\\nArticles with abstracts: {df['has_abstract'].sum()}/{len(df)}\")\n",
    "#     \n",
    "# except Exception as e:\n",
    "#     print(f\"Multi-year scraping failed: {e}\")\n",
    "\n",
    "print(\"Multi-year test commented out - uncomment when single year works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb1d012",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Adjust selectors**: If scraping fails, inspect the website HTML and adjust the CSS selectors in `ifc_scraper.py`\n",
    "2. **Test with real data**: Once selectors work, test with actual IFC publications\n",
    "3. **Rate limiting**: Ensure the scraper respects rate limits to avoid being blocked\n",
    "4. **Error handling**: Test how the scraper handles missing data, network errors, etc.\n",
    "\n",
    "## Common Issues\n",
    "- **JavaScript rendering**: The website might use JavaScript to load content. If so, consider using Selenium\n",
    "- **Rate limiting**: Too many requests might get blocked. Adjust the delay in config\n",
    "- **Changing HTML structure**: Websites change their HTML. The selectors may need updates\n",
    "- **Access restrictions**: Some content might require authentication or have access restrictions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
